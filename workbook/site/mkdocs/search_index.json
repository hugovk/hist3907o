{
    "docs": [
        {
            "location": "/", 
            "text": "This workbook supports \n#HIST3907o\n in the Winter 2016 term at \nCarleton University\n. The main course materials are in the \nGithub repo\n; where the information in this book and on the repo diverge, the repo will be taken as the 'correct' version.\n\n\nThe original \nWinter 2015 version is here.\n\n\nFor more advanced tutorials and help, please see \n\n\n\n\n\n\nThe Programming Historian\n\n\n\n\n\n\nDigital History Methods in R\n\n\n\n\n\n\nShawn Graham, shawn dot graham at carleton dot ca, \n@electricarchaeo\n\n\n\n\n\n\nOriginal content by Shawn Graham is \n\n\nlicensed under a \nCreative Commons Attribution-NonCommercial 4.0 International License\n.", 
            "title": "Home"
        }, 
        {
            "location": "/introduction/crafting-digital-history/", 
            "text": "Getting yourself ready\n\n\n\n\nA Video Introduction to the Course\n\n\n\n\nWelcome!\n\n\nWelcome! This workbook is made by converting several plain-text files into a fully-operational website using the 'mkdocs' \nstatic website generator\n. That means, if you want to keep a copy of all these files for your own records, you may. Simply click on the 'edit on github' link at the top right. This will bring you to the repository that houses this workbook. Then, when you're signed into 'github', you can 'fork' (that is, make a copy) the repo into your own account. Why 'forking'? It seems an odd phrase. Think of it like this:\n\n\nWriting, crafting, coding: these are like a small river, flowing in one direction into the future. You get new ideas, new hunches: the river branches. There's a fork in its path. Sometimes new ideas, new hunches fold back into that original stream: they merge. \n\n\nGithub is a way of mapping that stream, and a guide to revisiting the interesting parts of it.\n\n\nIt's not the best metaphor, but it'll do. No doubt you've had that experience where, after working on an essay for days, you make a change that you regret. You wish you could go back to fix it, but ctrl+z only goes so far. You realize that everything you've done for the last week needs to be thrown out, and you start over.\n\n\nWell, with 'versioning control', you can travel back upriver, back to where things were good. There are two tools that we will use to make this happen: \ngit\n and \ngithub\n. You'll learn more about making those things work in Module 1. You'll see why you'd want to do that, and how to keep an open notebook of your work on Github, writing things in a plain-text format called \n'markdown'\n. \n\n\nIn module 2, we'll start exploring the wide variety of historical data out there. We'll talk about some of the ethical dilemmas posed by having so much data out there. 'Data' are not neutral 'things given'; rather, they are 'capta': things \ntaken\n.\n\n\nIn module 3, we'll see that data/capta are \nmessy\n, and that they make a kind of illusory order that as historians, we are not normally in the habit of thinking about. The big secret of digital history is that the majority of your time on \nany\n digital history project won't be finding the capta, won't be analyzing the capta, won't be thinking about the historical meaning of the capta. We'll be \ncleaning it up\n. The exercises in this module will help you do that more efficiently (and be aware that 'efficiency' in computing terms is not a theoretically neutral idea!)\n\n\nWith module 4, we talk about doing the analysis. I present finding, cleaning, and analyzing as if they were sequential steps, but in reality they are circular. Movement in one aspect often requires revisting another one! This module explores how we do this, and what it means for us as historians.\n\n\nIn module 5, we begin at last to think about how we communicate all of this to our audiences. Look at how \none university lays out the expectations for digital history work\n (and, do you see how this ties back to ideas about \nparadata\n?). We will think about things like typography and layout. We'll look at ways of making our visualizations more compelling, more effective.\n\n\nFinally, while there is no formal requirement for you to do this, it would be a great way of launching yourself as a digital historian to think about how you would formally reveal your project work in this class to the wider world: and then do it. There's a lot of noise on the internet, especially when it comes to history. How do you, as a digital historian, make the strongest possible signal?\n\n\nSome Readings To Kick Things Off\n\n\nTo get things underway, you should read and reflect on the following three pieces. We can discuss these in our initial face-to-face virtual meeting; you should also feel free to reflect upon these in your narrative blog.\n\n\n\n\nWilliam G Thomas III. 'What is digital scholarship? A typology'. \nhttp://railroads.unl.edu/blog/?p=1159\n\n\nJefferson Bailey, Lily Pregill. 'Speak to the Eyes: The History and Practice of Information Visualization' \nArt Documentation: Journal of the Art Libraries Society of North America\n, vol. 33 (fall 2014) 0730-7187/2014/3302-0002 \ntext\n\n\nShawn Graham, Ian Milligan, Scott Weingart. 'Principles of Information Visualization' \nThe Historian's Macroscope\n - working title. Under contract with Imperial College Press. Open Draft Version, Autumn 2013, \nsite\n\n\n--- On Diversity in Digital History \nThe Macroscope\n\n\n\n\nAcknowledgements\n\n\nThe writing of this workbook took place alongside the writing of my more formal book on \ndigital methods\n co-authored with the exceptional \nIan Milligan\n and \nScott Weingart\n. I learned far more about doing digital history from them than they ever from me, and someday, I hope to repay the debt. Other folks who've been instrumental in getting this workbook and course off the ground include Melodee Beals, John Bonnett, Chad Gaffield, Tamara Vaughan, the staff of the EDC at Carleton University, and of course, the digital history community on Twitter. My thanks to you all.\n\n\nThis class was first offered in the Winter 2015 semester at Carleton University in Ottawa Canada as HIST3907b. I am grateful to the participants in that class for the feedback and frank discussions of what worked and what didn't. To see the earlier version of the course, please feel free to browse its \ngithub repository\n\n\nand as Columbo was wont to say...\n\n\nJust one more thing. Some of you might like a pdf version of this workbook, to print out bits and pieces. I generated such a thing on October 5th 2015; you may download and print it out yourself if you so desire, but things will have probably changed somewhat in the interval, so \nalways\n check here first (especially as I chase down typos, bugs, or indeed, when platforms and tools change). Frankly, you'd be better off forking a copy of the HIST3907o repo, cloning it to your desktop, and converting the .md files to .pdf with \npandoc\n to get the most up-to-date versions. But: here you go. \nWorkbook-as-pdfs", 
            "title": "Getting Started"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#getting-yourself-ready", 
            "text": "", 
            "title": "Getting yourself ready"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#a-video-introduction-to-the-course", 
            "text": "", 
            "title": "A Video Introduction to the Course"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#welcome", 
            "text": "Welcome! This workbook is made by converting several plain-text files into a fully-operational website using the 'mkdocs'  static website generator . That means, if you want to keep a copy of all these files for your own records, you may. Simply click on the 'edit on github' link at the top right. This will bring you to the repository that houses this workbook. Then, when you're signed into 'github', you can 'fork' (that is, make a copy) the repo into your own account. Why 'forking'? It seems an odd phrase. Think of it like this:  Writing, crafting, coding: these are like a small river, flowing in one direction into the future. You get new ideas, new hunches: the river branches. There's a fork in its path. Sometimes new ideas, new hunches fold back into that original stream: they merge.   Github is a way of mapping that stream, and a guide to revisiting the interesting parts of it.  It's not the best metaphor, but it'll do. No doubt you've had that experience where, after working on an essay for days, you make a change that you regret. You wish you could go back to fix it, but ctrl+z only goes so far. You realize that everything you've done for the last week needs to be thrown out, and you start over.  Well, with 'versioning control', you can travel back upriver, back to where things were good. There are two tools that we will use to make this happen:  git  and  github . You'll learn more about making those things work in Module 1. You'll see why you'd want to do that, and how to keep an open notebook of your work on Github, writing things in a plain-text format called  'markdown' .   In module 2, we'll start exploring the wide variety of historical data out there. We'll talk about some of the ethical dilemmas posed by having so much data out there. 'Data' are not neutral 'things given'; rather, they are 'capta': things  taken .  In module 3, we'll see that data/capta are  messy , and that they make a kind of illusory order that as historians, we are not normally in the habit of thinking about. The big secret of digital history is that the majority of your time on  any  digital history project won't be finding the capta, won't be analyzing the capta, won't be thinking about the historical meaning of the capta. We'll be  cleaning it up . The exercises in this module will help you do that more efficiently (and be aware that 'efficiency' in computing terms is not a theoretically neutral idea!)  With module 4, we talk about doing the analysis. I present finding, cleaning, and analyzing as if they were sequential steps, but in reality they are circular. Movement in one aspect often requires revisting another one! This module explores how we do this, and what it means for us as historians.  In module 5, we begin at last to think about how we communicate all of this to our audiences. Look at how  one university lays out the expectations for digital history work  (and, do you see how this ties back to ideas about  paradata ?). We will think about things like typography and layout. We'll look at ways of making our visualizations more compelling, more effective.  Finally, while there is no formal requirement for you to do this, it would be a great way of launching yourself as a digital historian to think about how you would formally reveal your project work in this class to the wider world: and then do it. There's a lot of noise on the internet, especially when it comes to history. How do you, as a digital historian, make the strongest possible signal?", 
            "title": "Welcome!"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#some-readings-to-kick-things-off", 
            "text": "To get things underway, you should read and reflect on the following three pieces. We can discuss these in our initial face-to-face virtual meeting; you should also feel free to reflect upon these in your narrative blog.   William G Thomas III. 'What is digital scholarship? A typology'.  http://railroads.unl.edu/blog/?p=1159  Jefferson Bailey, Lily Pregill. 'Speak to the Eyes: The History and Practice of Information Visualization'  Art Documentation: Journal of the Art Libraries Society of North America , vol. 33 (fall 2014) 0730-7187/2014/3302-0002  text  Shawn Graham, Ian Milligan, Scott Weingart. 'Principles of Information Visualization'  The Historian's Macroscope  - working title. Under contract with Imperial College Press. Open Draft Version, Autumn 2013,  site  --- On Diversity in Digital History  The Macroscope", 
            "title": "Some Readings To Kick Things Off"
        }, 
        {
            "location": "/introduction/crafting-digital-history/#acknowledgements", 
            "text": "The writing of this workbook took place alongside the writing of my more formal book on  digital methods  co-authored with the exceptional  Ian Milligan  and  Scott Weingart . I learned far more about doing digital history from them than they ever from me, and someday, I hope to repay the debt. Other folks who've been instrumental in getting this workbook and course off the ground include Melodee Beals, John Bonnett, Chad Gaffield, Tamara Vaughan, the staff of the EDC at Carleton University, and of course, the digital history community on Twitter. My thanks to you all.  This class was first offered in the Winter 2015 semester at Carleton University in Ottawa Canada as HIST3907b. I am grateful to the participants in that class for the feedback and frank discussions of what worked and what didn't. To see the earlier version of the course, please feel free to browse its  github repository  and as Columbo was wont to say...  Just one more thing. Some of you might like a pdf version of this workbook, to print out bits and pieces. I generated such a thing on October 5th 2015; you may download and print it out yourself if you so desire, but things will have probably changed somewhat in the interval, so  always  check here first (especially as I chase down typos, bugs, or indeed, when platforms and tools change). Frankly, you'd be better off forking a copy of the HIST3907o repo, cloning it to your desktop, and converting the .md files to .pdf with  pandoc  to get the most up-to-date versions. But: here you go.  Workbook-as-pdfs", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/Open-Access-Research/Open-Access-Research/", 
            "text": "Open Access Research\n\n\nAs historians, we aren't all that accustomed to sharing our research notes. We go to the archives, we take our photographs, we spend \nhours\n pouring over documents, photographs, diaries, newspapers... why should someone else benefit from \nour\n work?\n\n\nThere are a number of reasons why you should want to do this. Over the two weeks of this module, we will read and discuss the arguments advanced by various historians, including\n\n\n\n\nTrevor Owens\n\n\nCaleb McDaniel\n\n\nIan Milligan\n\n\nanother post by Milligan\n\n\nJo Guldi and David Armitage, ch 4\n\n\nMike Caulfield\n\n\n\n\nBut most importantly, \nchange is coming whether historians like it or not\n. Here in Canada, SSHRC has a \nresearch data archiving policy\n\n\n\n\nAll research data collected with the use of SSHRC funds must be preserved and made available for use by others within a reasonable period of time. SSHRC considers \u201ca reasonable period\u201d to be within two years of the completion of the research project for which the data was collected.\n\n\n\n\nNote the \nconversation that ensued on Twitter after Milligan mentioned all this\n and also \nhere\n\n\nWe will explore\n\n\n\n\nhow we can make our research notes open, \n\n\nwhat that implies for how we do research, \n\n\nand how we can use this process to maintain our scholarly voice online.\n\n\n\n\nReally, it's also a kind of \n'knowledge mobilization'\n. In the folders for this module, you will find exercises related to setting up your github account, how to commit, fork, push and pull files to your own repository and to others'. There are exercises on how to use services like \nDillinger.io\n, \nProse.io\n, and \nStackedit.io\n to make changes to a \nmarkdown\n file. Really, it's about \nsustainable authorship\n. \n\n\nBy the end of this module you will know:\n\n\n\n\nhow to work with github to foster collaboration (including 'issues' and the 'wiki' pages)\n\n\nhow to set up, fork, and make changes to files and repositories\n\n\n\n\nYou will also have set up:\n\n\n\n\nyour own \nhome base\n or \nopen research notebook\n on the web\n\n\na workflow for pushing your research notes to the web, using a combination of various pieces of open source software\n\n\n\n\nI do expect you to click through every link I provide, and to read these materials. Otherwise, you'll begin to fall behind quite quickly.", 
            "title": "Why you should be open"
        }, 
        {
            "location": "/Open-Access-Research/Open-Access-Research/#open-access-research", 
            "text": "As historians, we aren't all that accustomed to sharing our research notes. We go to the archives, we take our photographs, we spend  hours  pouring over documents, photographs, diaries, newspapers... why should someone else benefit from  our  work?  There are a number of reasons why you should want to do this. Over the two weeks of this module, we will read and discuss the arguments advanced by various historians, including   Trevor Owens  Caleb McDaniel  Ian Milligan  another post by Milligan  Jo Guldi and David Armitage, ch 4  Mike Caulfield   But most importantly,  change is coming whether historians like it or not . Here in Canada, SSHRC has a  research data archiving policy   All research data collected with the use of SSHRC funds must be preserved and made available for use by others within a reasonable period of time. SSHRC considers \u201ca reasonable period\u201d to be within two years of the completion of the research project for which the data was collected.   Note the  conversation that ensued on Twitter after Milligan mentioned all this  and also  here", 
            "title": "Open Access Research"
        }, 
        {
            "location": "/Open-Access-Research/Open-Access-Research/#we-will-explore", 
            "text": "how we can make our research notes open,   what that implies for how we do research,   and how we can use this process to maintain our scholarly voice online.   Really, it's also a kind of  'knowledge mobilization' . In the folders for this module, you will find exercises related to setting up your github account, how to commit, fork, push and pull files to your own repository and to others'. There are exercises on how to use services like  Dillinger.io ,  Prose.io , and  Stackedit.io  to make changes to a  markdown  file. Really, it's about  sustainable authorship .", 
            "title": "We will explore"
        }, 
        {
            "location": "/Open-Access-Research/Open-Access-Research/#by-the-end-of-this-module-you-will-know", 
            "text": "how to work with github to foster collaboration (including 'issues' and the 'wiki' pages)  how to set up, fork, and make changes to files and repositories   You will also have set up:   your own  home base  or  open research notebook  on the web  a workflow for pushing your research notes to the web, using a combination of various pieces of open source software   I do expect you to click through every link I provide, and to read these materials. Otherwise, you'll begin to fall behind quite quickly.", 
            "title": "By the end of this module you will know:"
        }, 
        {
            "location": "/Open-Access-Research/Exercises/", 
            "text": "Module 1: Exercises\n\n\nAll 4 exercises are on this page. Remember to scroll! Push yourself to complete as many as you can. If these are not challening for you, there's an advanced one at the end.\n\n\nEXERCISE 1: learning markdown syntax with dillinger.io or prose.io\n\n\nHave you ever fought with Word or another wordprocessor, trying to get things just \nright\n? Word processing is a mess. It conflates writing with typesetting and layout. Sometimes, you just want to get the words out. Othertimes, you want to make your writing as accessible as possible... but your intended recipient can't open your file, because they don't use the same wordprocessor. Or perhaps you wrote up some great notes that you'd love to have in a slideshow; but you can't, because copying and pasting preserves a whole lot of extra \ngunk\n that messes up your materials.\n\n\nThe answer is to separate your \ncontent\n from your tool:\n\n\nThis is where Markdown shines. Markdown is a syntax for marking semantic elements within a document explicitly, not in some hidden layer. The idea is to identify units that are meaningful to humans, like titles, sections, subsections, footnotes, and illustrations. At the very least, your files will always remain comprehensible to you, even if the editor you are currently using stops working or \u201cgoes out of business.\u201d\n\n\nWriting in this way liberates the author from the tool. Markdown can be written in any plain text editor and offers a rich ecosystem of software that can render that text into beautiful looking documents. For this reason, Markdown is currently enjoying a period of growth, not just as as means for writing scholarly papers but as a convention for online editing in general.\n\n\nPopular general purpose plain text editors include TextWrangler and Sublime for Mac, Notepad++ for Windows, as well as Gedit and Kate for Linux. However, there are also editors that specialize in displaying and editing Markdown.\n\n\nIn this exercise, I want you to become familiar with Markdown syntax (\nhere is a quick primer on markdown by Sarah Simpkin\n). There are a number of 'flavours' for Markdown, but in essence, they all mimic the kinds of conventions you would see in an email, using asterisks to indicate emphasis and so on.\n\n\n\n\nYou will need this \ncheatsheet\n.\n\n\nGo to \ndillinger.io\n in a new browser window. This looks like a wordprocessor. The left hand side of the screen is where you write, the right hand side shows you what your text will look like. Dillinger 'saves' your work in your browser's memory. You can also point it to save to your dropbox, google drive, or github account (under the cogwheel icon. Do you have a dropbox etc account? You should set that up now, if you don't).\n\n\nWrite a short 200-500 word piece on why you are taking this class; what you hope to achieve here; the kind of digital experience you already have versus the kind of experience you wish to have.\n\n\nHave at least two images (creative commons licensed for re-use - do you know how to \nfind these\n?) and link outwards to four websites that are relevant to your piece.\n\n\nIn the 'document name' slot, make sure to add the file type .md at the end\n\n\n'Save to' your dropbox, github, google drive, or one drive account.\n\n\n\n\nSee how easy that was? Don't worry about submitting this.... yet.\n\n\n\n\nnb: the next time you go to dillinger.io the last document you were working on will load up. That's because dillinger stashes your work in the browser cache. If you clear your cache (from your browser's tools or settings) you'll lose it, which is why in step 6 I suggested saving to those various accounts.\n\n\n\n\n(For a richer discussion of some more ways markdown and pandoc can make your research sustainable, see Tenen and Wythoff, \nSustainable Authorship in Plain Text Using Pandoc and Markdown\n)\n\n\nEXERCISE 2: setting up your github space\n\n\nIn this exercise, you will 1. get an account on github; 2. download and install git \n the github desktop program; 3. setup a webpage using the gh-pages branch for a repo\n\n\n\n\nDownload this interactive tutorial, \"Git It\": \nmac\n, \nwindows\n, \nlinux\n. Unzip and install.\n\n\nDownload and install Github Desktop \nhere\n\n\nDownload and install a text editor. Both \nAtom\n and \nSublime Text\n are very good.\n\n\nWork your way through the Git It tutorial. As part of that tutorial, you will set up a github account. The instructions regarding that tutorial are \nhere\n\n\n\n\n\n\nPush yourself as far as you can. Ask for help in our module 1 channel in Slack! Find a friend, and work through this together.\n\n\nPlease note:\n\n\n\n\ngit is a program that keeps snapshots of the contents of a designated folder; it watches for 'difs' or differences between one snapshot and the next.\n\n\ngithub\n is a webservice that allows you to keep track of those \nversions\n online, and what's more, to share those files with multiple collaborators, and \nkeep everyone's changes straight!\n\n\n\n\nSome key terms:\n\n\n\n\nrepository\n: a single folder that holds all of the files and subfolders of your project\n\n\ncommit\n: this means, 'take a snapshot of the current state of my repostiory'\n\n\npublish\n: take my folder on my computer, and copy it and its contents to the web as a repository at github.com/myusername/repositoryname\n\n\nsync\n: update the web repository with the latest commit from my local folder\n\n\nbranch\n: make a copy of my repository with a 'working name'\n\n\nmerge\n: fold the changes I have made on a branch into another branch (typically, either \nmaster\n or \ngh-pages\n)\n\n\nfork\n: to make a copy of someone else's repo\n\n\nclone\n: to copy a repo online onto your own computer\n\n\npull\n request: to ask the original maker of a repo to 'pull' your changes into their master, original, repository\n\n\npush\n: to move your changes from your computer to the online repo\n\n\n\n\nNow, let's set up a website that lives in a github repo\n\n\ndon't worry, exercise 2 is already done. This is just to push you further...\n\n\nThe historian Jack Dougherty has an excellent video and step-by-step instructions for getting set up with git and github. Please \nread his steps\n and then watch his video:\n\n\n\n\n\nPlease note: Dougherty's step 4 - he's copied and pasted the embed code from an interactive chart from Google Sheets. I want you instead to paste the embed code from a youtube video that captures something essential about yourself into the new file \nindex.html\n. ALSO, make sure your repo/folder is called \ndemo\n the way Dougherty's is.\n\n\nDon't worry about submitting all this... yet.\n\n\n\n\nnb here are some more resources about git and github -\nGithub Guides\n this explain in more depth some of the concepts behind Github. You could also try this tutorial, but it's not mandatory: \ntrygit\n. This tutorial emulates using the version control software git itself (upon which Github is built) from the command line. Some more tutorials and resources\nhere\n and \nhere\n).\n\n\n\n\nEXERCISE 3: setting up a simple open notebook (blog + personal github repo + notational velocity or notation, as appropriate)\n\n\nIn this exercise, you will create a new repository, and then set up your note taking software so that it saves your notes into your repository. This allows you to keep track of changes to your notes, and also to have an open backup online. An online version of your notes allows others to contribute to, and build from, your research! When people 'fork' or copy your repository, you can consider that a form of citation, as well. There are other benefits; we can chat about these.\n\n\n\n\nThe essence of an open notebook is a) it's easy to update b) it is under version control c) it's open and can be viewed/share online. We do this by storing notes from our notetaking software in a folder on our computer \nwhich is also a repository that your github desktop software monitors.\n Periodically, you will make 'commits' and 'sync' your local folder with the online version.\n\n\nFirst, let's set up the folder/repository. We do this by going to github.com and logging in. Then, follow these \ndetailed instructions to create a new repository\n. Call it 'my research notebook' or something similar. The github website will ask you if wish to initialize your repository with a readme.me file. Say 'yes'.\n\n\nIn your github interface on your computer (the desktop app), select 'syncronize'. The repository you just created online is now on your computer (it will be a folder inside your github folder). Challenge: can you locate this folder?\n\n\nDownload and set up Notational Velocity Alt (nvAlt) \n(Mac)\n or ResophNotes \n(PC)\n.\n    a. In \nnotational velocity\n you can change the settings so that each note is its own plain text file, and you can change the location where notational velocity saves these notes. Go to preferences \n notes \n storage. Under 'recognize individual files with attributes' there are two lists. Hit the \n+\n under each list and add \nmd\n. Then highlight all the other file extensions and hit \n-\n in turn. Under \nread notes from folder\n select the location of the folder/repository you created in the previous step. (A quick guide for nvAlt lives \nhere\n)\n    b. For \nResophNotes\n, click on the cogwheel icon at the bottom right. Then select the storage tab. Tick the 'plain text file' radio button, and the 'save title in text file' option. Then, in 'file directory', change the path to your open notebook repository. Then click ok. Under 'options' click the 'enable markdown' radiobox. (A quick guide to Resophnotes lives \nhere\n)\n\n\n\n\nYou now have a system in place for quickly taking notes, and for archiving them online on github. Let's take some notes. \nAt this point, please put the piece you wrote for exercise 1 - the markdown version - into a new note in your notetaking software set up in the preceding step. Then, make a new note with the direct URL to the demo repository you created in exercise 2.\n\n\n\n\n\n\nThink of five tags (descriptive labels) you would often use in your research. Then, I would like you to make ten notes on this keynote by Tim Hitchock, \nBig Data For Dead People\n. Include at the start of each note's title one (or more) of your tags/descriptive labels. This way, if your tag was 'british history', each time you started a new note with that tag, you'd quickly see all the other notes! \n\n\n\n\n\n\nOnce you've finished your notes (summarize! don't copy out text! include some bibliography in every note, so you know where your thoughts came from! you can copy your notes into your essay drafts as part of your writing if you've used summaries!) open your github software. It will now be showing that changes have happened in your repository. Commit the changes, then synchronize. Remember: you have to leave a message in the 'commit summary' box in order to commit.\n\n\n\n\n\n\nGo to your repository online. Refresh the page. Your notes are now online. The final step is the 'narrative' of your notetaking. I often make blog posts to remind myself of what I was doing, what I was thinking, and to gather together links to useful resources etc. The narrative part of your open notebook ecology could be done in a variety of ways, but a blog post is probably simplest. Your choice. Explain your tagging system, and link to your repository.\n\n\n\n\n\n\nIf you get stuck, watch this video. I'm no \nKubrick\n, but hey.\n\n\n\nPostscript to this exercise\n Read James Baker's piece on \n'Preserving Your Research Data'\n and, when you come to start working on your final project, see if you can apply his lessons to organizing your research.\n\n\nEXERCISE 4: forking \n contributing to a repository\n\n\nIn this exercise, I would like you to\n\n\n\n\n\n\nFork the \n'who-we-are'\n repo for this course.\n\n\n\n\n\n\nRename the \ntemplate.md\n file with \nyourname\n.md\n. Change the file accordingly to edit in your name, and the location of your online HQ \n open notebook as properly formatted markdown. Remember, when we're working with files you will need a good text editor. If you're on a Mac, you might already have something like TextWrangler. If you're on Windows, you're out of luck (notepad and wordpad are not useful to us; you can ask me why). I've already mentioned Atom and Sublime Text; make sure you have something appropriate installed. \nNotepad++ is also good\n (for windows folks)\n\n\n\n\n\n\nMake a pull request.\n\n\n\n\n\n\nIf you get stuck, watch this video.\n\n\n\n\n\nAdvanced\n\n\nIt is possible to build a website with a single command from your open notebook markdown formatted pages. I use something called 'pykwiki' to generate my own \nopen notebook\n. Are you familiar with blogs? Blogs work by using a series of templates that dictate the look of the website, while dynamically building the website fresh each time some one visits (content is pulled out of a database). This is cool, but it can be hacked; it can be slow; and it puts a strain on computing resources. A website like my open notebook is generated by me, with a single command on my computer, whenever I want to update it. The version that lives online is just a series of static html pages: no dynamic loading, no vulnerabilities, easy to search, easy to archive, easy to share.\n\n\nYou can try this out for yourself with a python package called 'MkDocs'. If you're on a mac, you have the python programming language already installed. If you're on windows, you'll need to \ninstall it; get version 2.7.11\n. Go on, I'll wait.\n\n\nThen, install \nmk-docs\n (\ninstructions here\n). Since you already have a repository, navigate to it via your terminal or command line. Follow the instructions on that page to generate your site. Then, there's one command that will push your entire site to the \ngh-pages\n branch of your repository \ninstructions here\n. Ta da!\n\n\nNow, you might run into hiccups, depending on how your system is set up. That's why this part of the exercises is called 'advanced'. See if you can solve them. Re-write this exercise with all the tacit bits spelt out - put it into your open notebook. Often, a major part of being a 'digital humanist' is uncovering all the tacit bits of how our digital tools are affecting our lives.\n\n\nYou can also see this in action in \nthis video\n.\n\n\n\n\nConclusion\n\n\nNow that you've worked through all of that, I can tell you that there are many different ways you could approach the problem of making your research notes publicly available. You could connect dillinger.io or prose.io directly to your github accounts, and always make your notes that way. The advantage of notational velocity and resophnotes is that search \n note creation happen at the same time. There are also more features in both programs that let you cross-reference notes and so on. The key is to figure out a solution that works for you, \nand\n that lets you make your research open. 'What if I get scooped?' I hear you say. Well, let's talk about what \nCaleb McDaniel has to say\n. And we should also talk about the \nsmallest federated wiki\n.", 
            "title": "Exercises"
        }, 
        {
            "location": "/Open-Access-Research/Exercises/#module-1-exercises", 
            "text": "All 4 exercises are on this page. Remember to scroll! Push yourself to complete as many as you can. If these are not challening for you, there's an advanced one at the end.", 
            "title": "Module 1: Exercises"
        }, 
        {
            "location": "/Open-Access-Research/Exercises/#exercise-1-learning-markdown-syntax-with-dillingerio-or-proseio", 
            "text": "Have you ever fought with Word or another wordprocessor, trying to get things just  right ? Word processing is a mess. It conflates writing with typesetting and layout. Sometimes, you just want to get the words out. Othertimes, you want to make your writing as accessible as possible... but your intended recipient can't open your file, because they don't use the same wordprocessor. Or perhaps you wrote up some great notes that you'd love to have in a slideshow; but you can't, because copying and pasting preserves a whole lot of extra  gunk  that messes up your materials.  The answer is to separate your  content  from your tool:  This is where Markdown shines. Markdown is a syntax for marking semantic elements within a document explicitly, not in some hidden layer. The idea is to identify units that are meaningful to humans, like titles, sections, subsections, footnotes, and illustrations. At the very least, your files will always remain comprehensible to you, even if the editor you are currently using stops working or \u201cgoes out of business.\u201d  Writing in this way liberates the author from the tool. Markdown can be written in any plain text editor and offers a rich ecosystem of software that can render that text into beautiful looking documents. For this reason, Markdown is currently enjoying a period of growth, not just as as means for writing scholarly papers but as a convention for online editing in general.  Popular general purpose plain text editors include TextWrangler and Sublime for Mac, Notepad++ for Windows, as well as Gedit and Kate for Linux. However, there are also editors that specialize in displaying and editing Markdown.  In this exercise, I want you to become familiar with Markdown syntax ( here is a quick primer on markdown by Sarah Simpkin ). There are a number of 'flavours' for Markdown, but in essence, they all mimic the kinds of conventions you would see in an email, using asterisks to indicate emphasis and so on.   You will need this  cheatsheet .  Go to  dillinger.io  in a new browser window. This looks like a wordprocessor. The left hand side of the screen is where you write, the right hand side shows you what your text will look like. Dillinger 'saves' your work in your browser's memory. You can also point it to save to your dropbox, google drive, or github account (under the cogwheel icon. Do you have a dropbox etc account? You should set that up now, if you don't).  Write a short 200-500 word piece on why you are taking this class; what you hope to achieve here; the kind of digital experience you already have versus the kind of experience you wish to have.  Have at least two images (creative commons licensed for re-use - do you know how to  find these ?) and link outwards to four websites that are relevant to your piece.  In the 'document name' slot, make sure to add the file type .md at the end  'Save to' your dropbox, github, google drive, or one drive account.   See how easy that was? Don't worry about submitting this.... yet.   nb: the next time you go to dillinger.io the last document you were working on will load up. That's because dillinger stashes your work in the browser cache. If you clear your cache (from your browser's tools or settings) you'll lose it, which is why in step 6 I suggested saving to those various accounts.   (For a richer discussion of some more ways markdown and pandoc can make your research sustainable, see Tenen and Wythoff,  Sustainable Authorship in Plain Text Using Pandoc and Markdown )", 
            "title": "EXERCISE 1: learning markdown syntax with dillinger.io or prose.io"
        }, 
        {
            "location": "/Open-Access-Research/Exercises/#exercise-2-setting-up-your-github-space", 
            "text": "In this exercise, you will 1. get an account on github; 2. download and install git   the github desktop program; 3. setup a webpage using the gh-pages branch for a repo   Download this interactive tutorial, \"Git It\":  mac ,  windows ,  linux . Unzip and install.  Download and install Github Desktop  here  Download and install a text editor. Both  Atom  and  Sublime Text  are very good.  Work your way through the Git It tutorial. As part of that tutorial, you will set up a github account. The instructions regarding that tutorial are  here    Push yourself as far as you can. Ask for help in our module 1 channel in Slack! Find a friend, and work through this together.  Please note:   git is a program that keeps snapshots of the contents of a designated folder; it watches for 'difs' or differences between one snapshot and the next.  github  is a webservice that allows you to keep track of those  versions  online, and what's more, to share those files with multiple collaborators, and  keep everyone's changes straight!   Some key terms:   repository : a single folder that holds all of the files and subfolders of your project  commit : this means, 'take a snapshot of the current state of my repostiory'  publish : take my folder on my computer, and copy it and its contents to the web as a repository at github.com/myusername/repositoryname  sync : update the web repository with the latest commit from my local folder  branch : make a copy of my repository with a 'working name'  merge : fold the changes I have made on a branch into another branch (typically, either  master  or  gh-pages )  fork : to make a copy of someone else's repo  clone : to copy a repo online onto your own computer  pull  request: to ask the original maker of a repo to 'pull' your changes into their master, original, repository  push : to move your changes from your computer to the online repo   Now, let's set up a website that lives in a github repo  don't worry, exercise 2 is already done. This is just to push you further...  The historian Jack Dougherty has an excellent video and step-by-step instructions for getting set up with git and github. Please  read his steps  and then watch his video:   Please note: Dougherty's step 4 - he's copied and pasted the embed code from an interactive chart from Google Sheets. I want you instead to paste the embed code from a youtube video that captures something essential about yourself into the new file  index.html . ALSO, make sure your repo/folder is called  demo  the way Dougherty's is.  Don't worry about submitting all this... yet.   nb here are some more resources about git and github - Github Guides  this explain in more depth some of the concepts behind Github. You could also try this tutorial, but it's not mandatory:  trygit . This tutorial emulates using the version control software git itself (upon which Github is built) from the command line. Some more tutorials and resources here  and  here ).", 
            "title": "EXERCISE 2: setting up your github space"
        }, 
        {
            "location": "/Open-Access-Research/Exercises/#exercise-3-setting-up-a-simple-open-notebook-blog-personal-github-repo-notational-velocity-or-notation-as-appropriate", 
            "text": "In this exercise, you will create a new repository, and then set up your note taking software so that it saves your notes into your repository. This allows you to keep track of changes to your notes, and also to have an open backup online. An online version of your notes allows others to contribute to, and build from, your research! When people 'fork' or copy your repository, you can consider that a form of citation, as well. There are other benefits; we can chat about these.   The essence of an open notebook is a) it's easy to update b) it is under version control c) it's open and can be viewed/share online. We do this by storing notes from our notetaking software in a folder on our computer  which is also a repository that your github desktop software monitors.  Periodically, you will make 'commits' and 'sync' your local folder with the online version.  First, let's set up the folder/repository. We do this by going to github.com and logging in. Then, follow these  detailed instructions to create a new repository . Call it 'my research notebook' or something similar. The github website will ask you if wish to initialize your repository with a readme.me file. Say 'yes'.  In your github interface on your computer (the desktop app), select 'syncronize'. The repository you just created online is now on your computer (it will be a folder inside your github folder). Challenge: can you locate this folder?  Download and set up Notational Velocity Alt (nvAlt)  (Mac)  or ResophNotes  (PC) .\n    a. In  notational velocity  you can change the settings so that each note is its own plain text file, and you can change the location where notational velocity saves these notes. Go to preferences   notes   storage. Under 'recognize individual files with attributes' there are two lists. Hit the  +  under each list and add  md . Then highlight all the other file extensions and hit  -  in turn. Under  read notes from folder  select the location of the folder/repository you created in the previous step. (A quick guide for nvAlt lives  here )\n    b. For  ResophNotes , click on the cogwheel icon at the bottom right. Then select the storage tab. Tick the 'plain text file' radio button, and the 'save title in text file' option. Then, in 'file directory', change the path to your open notebook repository. Then click ok. Under 'options' click the 'enable markdown' radiobox. (A quick guide to Resophnotes lives  here )   You now have a system in place for quickly taking notes, and for archiving them online on github. Let's take some notes.  At this point, please put the piece you wrote for exercise 1 - the markdown version - into a new note in your notetaking software set up in the preceding step. Then, make a new note with the direct URL to the demo repository you created in exercise 2.    Think of five tags (descriptive labels) you would often use in your research. Then, I would like you to make ten notes on this keynote by Tim Hitchock,  Big Data For Dead People . Include at the start of each note's title one (or more) of your tags/descriptive labels. This way, if your tag was 'british history', each time you started a new note with that tag, you'd quickly see all the other notes!     Once you've finished your notes (summarize! don't copy out text! include some bibliography in every note, so you know where your thoughts came from! you can copy your notes into your essay drafts as part of your writing if you've used summaries!) open your github software. It will now be showing that changes have happened in your repository. Commit the changes, then synchronize. Remember: you have to leave a message in the 'commit summary' box in order to commit.    Go to your repository online. Refresh the page. Your notes are now online. The final step is the 'narrative' of your notetaking. I often make blog posts to remind myself of what I was doing, what I was thinking, and to gather together links to useful resources etc. The narrative part of your open notebook ecology could be done in a variety of ways, but a blog post is probably simplest. Your choice. Explain your tagging system, and link to your repository.    If you get stuck, watch this video. I'm no  Kubrick , but hey.  Postscript to this exercise  Read James Baker's piece on  'Preserving Your Research Data'  and, when you come to start working on your final project, see if you can apply his lessons to organizing your research.", 
            "title": "EXERCISE 3: setting up a simple open notebook (blog + personal github repo + notational velocity or notation, as appropriate)"
        }, 
        {
            "location": "/Open-Access-Research/Exercises/#exercise-4-forking-contributing-to-a-repository", 
            "text": "In this exercise, I would like you to    Fork the  'who-we-are'  repo for this course.    Rename the  template.md  file with  yourname .md . Change the file accordingly to edit in your name, and the location of your online HQ   open notebook as properly formatted markdown. Remember, when we're working with files you will need a good text editor. If you're on a Mac, you might already have something like TextWrangler. If you're on Windows, you're out of luck (notepad and wordpad are not useful to us; you can ask me why). I've already mentioned Atom and Sublime Text; make sure you have something appropriate installed.  Notepad++ is also good  (for windows folks)    Make a pull request.    If you get stuck, watch this video.", 
            "title": "EXERCISE 4: forking &amp; contributing to a repository"
        }, 
        {
            "location": "/Open-Access-Research/Exercises/#advanced", 
            "text": "It is possible to build a website with a single command from your open notebook markdown formatted pages. I use something called 'pykwiki' to generate my own  open notebook . Are you familiar with blogs? Blogs work by using a series of templates that dictate the look of the website, while dynamically building the website fresh each time some one visits (content is pulled out of a database). This is cool, but it can be hacked; it can be slow; and it puts a strain on computing resources. A website like my open notebook is generated by me, with a single command on my computer, whenever I want to update it. The version that lives online is just a series of static html pages: no dynamic loading, no vulnerabilities, easy to search, easy to archive, easy to share.  You can try this out for yourself with a python package called 'MkDocs'. If you're on a mac, you have the python programming language already installed. If you're on windows, you'll need to  install it; get version 2.7.11 . Go on, I'll wait.  Then, install  mk-docs  ( instructions here ). Since you already have a repository, navigate to it via your terminal or command line. Follow the instructions on that page to generate your site. Then, there's one command that will push your entire site to the  gh-pages  branch of your repository  instructions here . Ta da!  Now, you might run into hiccups, depending on how your system is set up. That's why this part of the exercises is called 'advanced'. See if you can solve them. Re-write this exercise with all the tacit bits spelt out - put it into your open notebook. Often, a major part of being a 'digital humanist' is uncovering all the tacit bits of how our digital tools are affecting our lives.  You can also see this in action in  this video .", 
            "title": "Advanced"
        }, 
        {
            "location": "/Open-Access-Research/Exercises/#conclusion", 
            "text": "Now that you've worked through all of that, I can tell you that there are many different ways you could approach the problem of making your research notes publicly available. You could connect dillinger.io or prose.io directly to your github accounts, and always make your notes that way. The advantage of notational velocity and resophnotes is that search   note creation happen at the same time. There are also more features in both programs that let you cross-reference notes and so on. The key is to figure out a solution that works for you,  and  that lets you make your research open. 'What if I get scooped?' I hear you say. Well, let's talk about what  Caleb McDaniel has to say . And we should also talk about the  smallest federated wiki .", 
            "title": "Conclusion"
        }, 
        {
            "location": "/module-2/Finding Data/", 
            "text": "How do we find data, anyway?\n\n\n\n\n'Something given'. That's a nice way of thinking about it. Of course, much of the data that we are 'given' wasn't really given \nwillingly\n. When we \ntopic model Martha Ballard's diary\n, did she \ngive\n this to us? Of course, she couldn't have imagined what we might try to do to it. Other kinds of data - census data, for instance - were compelled: folks had to answer the questions, on pain of punishment. This is all to suggest that there is a moral dimension to what we do with big data in history. Stop for a moment and read \n'the joys of big data'\n (if you haven't already) and then \n'the third wave of computational history'\n.\n\n\nBig data is not value-neutral; we need to think about, and talk about, what it means to collect, transform, analyze and visualize it. Who has the power here? (and you might also reflect on \n'the most profitable obsolete technology'\n ) Finally, you might also think about recent history - listen to Ian Milligan \ndiscuss how Yahoo's closure of Geocities represented a terrible blow to social history\n.\n\n\nAccepting that big data is out there, that there's more material than one person can usefully digest and understand, and that a big-picture, macroscopic point of view is a useful perspective, means also thinking about the digital milieu that makes this possible. But see this piece by Tim Sherratt on \nSeams and edges: Dreams of aggregation, access \n discovery in a broken world\n. We interact with the data we find, and in the process, we alter both it and ourselves! As you do your projects and work through this workbook, think about the ethical, moral, and legal dimensions to what you are doing. \nKeep track of your thoughts in your open notebook\n.\n\n\nOk, so?\n\n\nSo how can we find big data? The exercises in this module will teach you how to use wget on the command line to grab webpages; they will introduce you to the concept of APIs and what you might achieve with them as a historian; they will have you use some existing free and commercial tools for webscraping; and we will also learn how to grab social media data as well. \n\n\nYou might like to begin with \nthis list of resources; more being added in time.\n You should also perhaps dip into the \n'Data Fundamentals'\n part of \nData + Design\n; think of it as another excellent textbook to help you when you need another perspective on the materials in this course.\n\n\nAnd don't forget serendipity\n\n\nFollow researchers and institutions in your field of study. Once on Twitter I saw something that struck me as an excellent find. Penn Libraries tweeted, and I retweeted, \na link to a traveller's diary from the 19th century\n - a woman who sailed from the US to Europe and thence the Nile, which she ascended and explored. \n\nMy tweet\n led to a flurry of activity amongst scholars, and even now, the transcription has begun...\n\n\nBut first... let's set a bit of framework.\n\n\nIf we're going to find data, we need to be able to access the power of our machines, to get them to do what we want. It's worth thinking about what Corey Doctorow has called \nthe war on general purpose computing\n as we begin...\n\n\n...and then thinking about what 'search' actually means. Check out Ted Underwood's piece on \n'Theorizing Research Practices We Forgot to Theorize Twenty Years Ago'\n. \n\n\nFinally, Cameron Blevins has some thoughts on the \n'perpetual sunrise of methodology'\n.", 
            "title": "How do we find data?"
        }, 
        {
            "location": "/module-2/Finding Data/#how-do-we-find-data-anyway", 
            "text": "'Something given'. That's a nice way of thinking about it. Of course, much of the data that we are 'given' wasn't really given  willingly . When we  topic model Martha Ballard's diary , did she  give  this to us? Of course, she couldn't have imagined what we might try to do to it. Other kinds of data - census data, for instance - were compelled: folks had to answer the questions, on pain of punishment. This is all to suggest that there is a moral dimension to what we do with big data in history. Stop for a moment and read  'the joys of big data'  (if you haven't already) and then  'the third wave of computational history' .  Big data is not value-neutral; we need to think about, and talk about, what it means to collect, transform, analyze and visualize it. Who has the power here? (and you might also reflect on  'the most profitable obsolete technology'  ) Finally, you might also think about recent history - listen to Ian Milligan  discuss how Yahoo's closure of Geocities represented a terrible blow to social history .  Accepting that big data is out there, that there's more material than one person can usefully digest and understand, and that a big-picture, macroscopic point of view is a useful perspective, means also thinking about the digital milieu that makes this possible. But see this piece by Tim Sherratt on  Seams and edges: Dreams of aggregation, access   discovery in a broken world . We interact with the data we find, and in the process, we alter both it and ourselves! As you do your projects and work through this workbook, think about the ethical, moral, and legal dimensions to what you are doing.  Keep track of your thoughts in your open notebook .  Ok, so?  So how can we find big data? The exercises in this module will teach you how to use wget on the command line to grab webpages; they will introduce you to the concept of APIs and what you might achieve with them as a historian; they will have you use some existing free and commercial tools for webscraping; and we will also learn how to grab social media data as well.   You might like to begin with  this list of resources; more being added in time.  You should also perhaps dip into the  'Data Fundamentals'  part of  Data + Design ; think of it as another excellent textbook to help you when you need another perspective on the materials in this course.  And don't forget serendipity  Follow researchers and institutions in your field of study. Once on Twitter I saw something that struck me as an excellent find. Penn Libraries tweeted, and I retweeted,  a link to a traveller's diary from the 19th century  - a woman who sailed from the US to Europe and thence the Nile, which she ascended and explored.  My tweet  led to a flurry of activity amongst scholars, and even now, the transcription has begun...", 
            "title": "How do we find data, anyway?"
        }, 
        {
            "location": "/module-2/Finding Data/#but-first-lets-set-a-bit-of-framework", 
            "text": "If we're going to find data, we need to be able to access the power of our machines, to get them to do what we want. It's worth thinking about what Corey Doctorow has called  the war on general purpose computing  as we begin...  ...and then thinking about what 'search' actually means. Check out Ted Underwood's piece on  'Theorizing Research Practices We Forgot to Theorize Twenty Years Ago' .   Finally, Cameron Blevins has some thoughts on the  'perpetual sunrise of methodology' .", 
            "title": "But first... let's set a bit of framework."
        }, 
        {
            "location": "/module-2/Exercises/", 
            "text": "Module 2: Exercises\n\n\nAll four exercises are on this page. Don't forget to scroll. If you have difficulties, or if the instructions need clarification, please click the 'issues' button and leave a note. Feel free to fork and improve these instructions, if you are so inclined. Remember, these exercises get progressively more difficult, and will require you to either download materials or read materials on other websites. Give yourself plenty of time. Try them all, and remember you can turn to (I encourage you to!) your classmates for help. Work together!\n\n\nnb. The third exercise will be the most difficult because everyone's machine is slightly different. Mac users should experience the least difficulty; PC users the most. Sites like \nstackoverflow\n will be extremely helpful. DO NOT suffer in silence as you try these exercises! Ask for help, set up a video appointment, or find me in person.\n\n\nBackground\n\n\nWhere do we go to find data? Part of that problem is solved by knowing what question you are asking, and what \nkinds\n of data would help solve that question. Let's assume that you have a pretty good question you want an answer to - say, something concerning social and household history in early Ottawa, like what was the role of 'corner' stores (if such things exist?) in fostering a sense of neighbourhood - and begin thinking about how you'd find data to explore that question.\n\n\nIan Milligan\n recently gave a \nworkshop on these issues\n. He identifies a few different ways by which you might obtain your data, and we will follow him closely as we look at:\n\n\n\n\nThe Dream Case\n\n\nScraping Data yourself with free software (Outwit Hub)\n\n\nApplication Programming Interfaces (APIs)\n\n\nWget\n\n\n\n\nThere is so much data available; with these methods, we can gather enormous amounts that will let us see large-scale macroscopic patterns. At the same time, it allows us to dive into the details with comparative ease. The thing is, not all digital data are created equally. Google has spent millions digitizing \neverything\n; newspapers have digitized their own collections. Genealogists and local historical societies upload yoinks of digitized photographs, wills, local tax records, \nyou-name-it\n, \nevery day\n. But, consider what Milligan has to say about \n'illusionary order'\n:\n\n\n\n\n[...] poor and misunderstood use of online newspapers can skew historical research. In a conference presentation or a lecture, it\u2019s not uknown to see the familiar yellow highlighting of found searchwords on projected images: indicative of how the original primary material was obtained. But this historical approach generally usually remains unspoken, without a critical methodological reflection. As I hope I\u2019ll show here, using Pages of the Past uncritically for historical research is akin to using a volume of the Canadian Historical Review with 10% or so of the pages ripped out. Historians, journalists, policy researchers, genealogists, and amateur researchers need to at least have a basic understanding of what goes on behind the black box.\n\n\n\n\nPlease go and read that full article. You should makes some notes: what are some of the key dangers? Reflect: how have you used digitized resources uncritically in the past?\n\n\n\n\nRemember: 'To digitize' doesn't - or shouldn't - mean uploading a photograph of a document. There's a lot more going on that that. We'll get to that in a moment.\n\n\nExercise 1: The Dream Case\n\n\nIn the dream case, your data are not just images, but are actually sorted and structured into some kind of pre-existing database. There are choices made in the \ncreation\n of the database, but a good database, a good project, will lay out for you their decision making, their corpora, and how they've dealt with ambiguity and so on. You search using a robust interface, and you get a well-formed spreadsheet of data in return. Two examples of 'dream case' data:\n\n\n\n\nEpigraphic Database Heidelberg\n\n\nCommwealth War Graves Commission, Find War Dead\n\n\n\n\nExplore both databases. Perform a search of interest to you. In the case of the epigraphic database, if you've done any Latin, try searching for terms related to occupations; or you could search '\nFiglin*\n'. In the CWGC database, search your own surname. Download your results. You now have data that you can explore! In your online research notebook, make a record (or records) of what you searched, the URL for your search \n its results, and where you're keeping your data.\n\n\nExercise 2: Outwit Hub\n\n\nDownload, and install, \noutwit hub\n. Do not buy it (not unless you really want to; the free trial is good enough for our purposes here)\n\n\nOutwit hub is a piece of software that lets you scrape the html of a webpage. It comes with its own browser. In essence, you look at the html source code of your page to identify the \nmarkers\n that embrace the information you want. Outwit then copies just that information into a table for you, which you can then download. Take a look at the \nSuda Online\n and do a search for \npie\n (the Suda is a 10th century Byzantine Greek encyclopedia, and its online version is one of the earliest examples of what we'd now recognize as digital humanities scholarship).\n\n\nRight-click anywhere on the results page, and 'view source'. You'll see something like this:\n\n\nThe record number - the Adler number - is very clearly marked, as is the translation. Those are the bits we want. So\n\n\nOpen outwit hub. In the address bar at the top of the screen, type in \nhttp://www.stoa.org/sol/\n. The page will load up. Do your search again there by putting \npie\n in there. (Remember, we're not searching for pie, but rather, the transliterated characters that form a root in many ancient Greek words!). The results will load up. If you then click on 'source' on the left hand side of the screen, you'll see the underlying html. Now click on 'scrapers' (again, at left hand side of screen, under 'automators'). Your page will look very similar to this:\n\n\n\n\nAt the bottom of the page - that's where you tell Outwit how to scrape that source.  Click \u2018scrapers,\u2019 then \u2018new,\u2019 give it a name. Enter the markers that you are interested in:\n\n\n\nHit Execute to run the scraper. Press \u2018Catch\u2019 to move it into your memory. Press 'export' to generate (and save) a spreadsheet of your data.\n\n\nWrite this exercise up in your notebook. What other data does outwit include with your export? How might that data be useful?\n\n\nExercise 3: APIs\n\n\nSometimes, a website will have what is called an 'application programming interface'. In essence, this lets a program on your computer talk to the computer serving the website you're interested in, such that the website gives you the data that you're looking for.\n\n\nThat is, instead of \nyou\n punching in the search terms, and copying and pasting the results, you get the computer to do it. More or less. The thing is, the results come back to you in a machine-readable format - often, JSON, which is a kind of text format. It looks like this:\n\n\n\nThe 'Canadiana Discovery Portal' has tonnes of materials related to Canada's history, from a wide variety of sources. Its search page is at: http://search.canadiana.ca/\n\n\n\n\nGo there, and search \"ottawa\" and set the date range to 1800 to 1900. Hit enter. You are presented with a page of results -56 249 results! That's a lot of data. But do you notice the address bar of your browser? It'll say something like this:\n\n\n\n\nhttp://search.canadiana.ca/search?q=ottawa\nfield=\ndf=1800\ndt=1900\n\n\nYour search query has been put into the URL. You're looking at the API! Everything after /search is a command that you are sending to the Canadiana server.\n\n\nScroll through the results, and you'll see a number just before the ?\n\n\nhttp://search.canadiana.ca/search/2?df=1800\ndt=1900\nq=ottawa\nfield=\n\n\nhttp://search.canadiana.ca/search/3?df=1800\ndt=1900\nq=ottawa\nfield=\n\n\nhttp://search.canadiana.ca/search/4?df=1800\ndt=1900\nq=ottawa\nfield=\n\n\n....all the way up to 5625 (ie, 10 results per page, so 56249 / 10).\n\n\nIf you go to http://search.canadiana.ca/support/api you can see the full list of options. What we are particularly interested in now is the bit that looks like this:\n\n\nfmt=json\n\n\nAdd that to your query URL. How different the results now look! What's nice here is that the data is formatted in a way that makes sense to a machine - which we'll learn more about in due course.\n\n\nIf you look back at the full list of API options, you'll see at the bottom that one of the options is 'retrieving individual item records'; the key for that is a field called 'oocihm'. If you look at your page of json results, and scroll through them, you'll see that each individual record has its own oocihm number. If we could get a list of those, we'd be able to programmatically slot them into the commands for retrieving individual item records:\n\n\nhttp://eco.canadiana.ca/view/oocihm.16278/?r=0\ns=1\nfmt=json\napi_text=1\n\n\nThe problem is: how to retrieve those oocihm numbers. The answer is, 'we write a program'. And the program that you want can be \nfound here\n. Study that program carefully. There are a number of useful things happening in there, notably 'curl', 'jq', 'sed', 'awk'. curl  is a program for downloading webpages, jq for dealing with json, and sed and awk for searching within and cleaning up text. If this all sounds greek to you, there is an excellent gentle introduction over at \nWilliam Turkel's blog\n.\n\n\nMac instructions:\n\n\nFirst question: do you have wget installed on your computer? If you don't, you'll need it for this exercise and the next one (if you look in the program 'api-ex-mac.sh', you'll see that the final line of the program asks wget to go get the results you've scraped from Canadiana). Installing wget is quite straightforward - follow \nthe Programming Historian's instructions\n.\n\n\nOur program requires a helper program that reads the .json data - It's called JQ. You can install this by using HOMEBREW. But to install homebrew, visit \nhttp://brew.sh\n and follow instructions. Then, to install jq, type \nbrew install jq\n from your terminal.\n\n\nNow, download api-ex-mac.sh from the repository and save it to your desktop. Open your terminal program (which you can find under 'applications' and then 'utilities'.) Navigate to your desktop:\n\n\ncd desktop\n\n\nthen tell your computer that this 'api-ex-mac.sh' program is one that you want to run:\n\n\nsudo chmod 700 file.sh\n\n\nAnd then you can run it by typing:\n\n\n./api-ex-mac.sh\n  but \ndon't\n do that yet! You'll need to change the search parameters to reflect your own interests. Do you see how you can do that? Open the program in a text editor, make the relevant change, save with a new name, and then run the new command. Move your results into a sensible location on your computer. Make a new entry (or entries) into your research notebook about this process, what worked, what hasn't, what you've found, where things are, and so on. You might want to upload your script (your .sh file) to your repository. Remember: the goal is so that you can come back to all this in a month's time and figure out \nexactly what you did\n to collect your data.\n\n\nI've put a copy of the script for Mac folks \nhere.\n\n\nWindows7\n8\n10 instructions:\n\n\nBegin by making a folder for this exercise on your desktop.\n\n\n\n\nYou'll need \ngitbash\n (which comes with git; I know you already have github on your desktop, which has \ngit shell\n but that's not what we want. Download \ngit\n and install it, and that will give you the git bash utility, and will not mess with your github set up. 'Bash' is  \"a shell that runs commands once you type the name of a command and press \n on your keyboard.\" You can see screenshots and find help on all this \nhere\n. You will be running our scraper program from within this shell.\n\n\nYou'll need jq \ndownload here\n. You're going to put this in the folder you've made for this exercise. \nMake sure that you rename it 'jq.exe'\n (in somecases, the download name might be slightly different).\n\n\nYou'll need CoreUtils \nfrom here\n. Download and install this.\n\n\nYou need to tell your computer that CoreUtils now exists on your machine. Go to your computer's control panel. On 'my computer' (or whatever Windows calls it these days, possibly \ncontrol panel - system and security - system - advanced system settings\n) right click and select 'properties'. Then select 'advanced'.\n\n\nClick on the 'environment variables' button.\n\n\nIn the pop-up box that opens, there is a box marked 'system variables'. Highlight the one called 'Path'. Click on 'Edit'.\n\n\nIn the variable box that you can now edit, there should already be many things. Scroll to the very end of that (to the right) and add:\n\n\n\n\n;C:\\Program Files (x86)\\GnuWin32\\bin\n\n\n\n\nnb: make sure there's a space between files and (x86).\n\n\n\n\n...so yes, start that with a semicolon- and what you are putting in is the exact location of where the coreutils package of programs is located.\n\n\nFinally, you'll need the windows version of wget installed on your machine. Get it \nhere\n and download it to C:Windows directory.\n\n\nNow:  \n\n\n\n\n\n\ndownload this \nzip file\n. It contains the Canadiana shell script \nand\n all the necessary helper files. \nNb\n these all need to be in the same folder in order for this to work.\n\n\n\n\n\n\nopen \ngit bash\n - it'll be available via your programs menu. You do not want 'Git Gui' nor 'GitHub' nor 'Git Shell'. \nGit bash\n. (You can also navigate to a folder in explorer, then right-click in that folder and select 'open git bash here'.)\n\n\n\n\n\n\nmake sure you are working within the folder you made on your desktop. The command to change directory when you are in bash (or the command line, for that matter) is \ncd\n  ie \ncd course-notes\n would go one level down into a folder called 'course notes'. To go one level up, you'd type \ncd ..\n \n- ie, two periods. More help on finding your way around this interface is \nhere\n\n\n\n\n\n\nOnce you're in the right folder, all you have to do is type the name of our programme: \n./canadiana.sh\n and your program will query the Canadiana API, save the results, and then use wget to retrieve the full text of each result by asking the API for those results in turn. \nBut don't do that yet!\n\n\n\n\n\n\nYou'll need to change the search parameters to reflect your own interests. Do you see how you can do that? Open the program in a text editor, make the relevant change, save with a new name (make sure to keep the same file extenion, \n.sh\n - in notepad, change the save as file type to \nall files\n and then write the new name, e.g, \ncanadiana-2.sh\n, and then run your program by typing its name at the prompt in the git bash window. When it's finished, move your results into a sensible location on your computer. Make a new entry (or entries) into your research notebook about this process, what worked, what hasn't, what you've found, where things are, and so on. You might want to upload your script (your .sh file) to your repository. Remember: the goal is so that you can come back to all this in a month's time and figure out \nexactly what you did\n to collect your data.\n\n\nif you get an error message: jq or seq cannot be found\n\n\nIf this happens, your computer is not finding the coreutils installation or the \njq.exe\n program. First thing: when you downloaded jq, did you make sure to change the name to \njq.exe\n? When it downloads, it downloads as (eg) \njq-win64.exe\n. Shorten up the name. Second thing: is it in the same folder as your script? Third thing: sometimes, it just might be easier to move the \nseq.exe\n program into the same folder as your script, that is, your working folder. Go to \nC:\\Program Files (x86)\\GnuWin32\\bin\n and \ncopy\n \nseq.exe\n to your working folder. You will also need to copy:\n+ \nlibiconv2.dll\n\n+ \nlibintl3.dll\n\n...to the same folder. (\nshortcut\n - the same zip file I gave you that has the \ncanadiana.sh\n in it has all of the helper files in it too. You could navigate to this unzipped folder, open a gitbash window there, and run the scripts. In this case, FYI, all you needed installed was gitbash.)\n\n\nA Windows Gotcha\n It might happen that you have my zipfile with all the necessary files downloaded and extracted on your machine, and yet, despite everything, when you type the \ncanadiana.sh\n command you get a \nfile not found error\n. On Windows 10 machines (and perhaps some Windows 8 machines), you will need to add the dot slash at the beginning: \n./canadiana.sh\n. The script might then run, but if you look at the output carefully, nothing is being downloaded and there is also a \njq not found\n error being printed out. To solve this, you'll need to edit \ncanadiana.sh\n and find the spots where the script calls the \njq\n command and replace it with \n./jq\n. \n\n\nAn alternative Windows installation\n\n\nThis can also work for Windows 7 \nif\n you've got powershell 3 installed. Win7 comes with an earlier version, so you'd have to update it, \nwhich isn't straightforward\n. I'm grateful to \nDenis Gladkikh for his blog post on the matter\n._\n\n\n\n\nMake a folder somewhere handy for this exercise. Download the \napi-ex-pc.sh\n program into it, as well as \njq\n. Make sure you rename the downloaded jq file to \njq.exe\n.\n\n\nFind, and run, 'Powershell'\n\n\nAt the prompt, type in these commands in sequence:\n\n\n\n\nset-executionpolicy unrestricted -s cu\n\n\nSay 'yes' at the prompt that asks if you really want to do this.\n\n\niex (new-object net.webclient).downloadstring('https://get.scoop.sh')\n\n\nscoop install 7zip coreutils curl git grep openssh sed wget vim grep\n\n\nA number of components will download and get configured to work from within powershell. When they finish, at the command prompt, you can run your program like so:\n\n\n./canadiana.sh\n\n\nIf all goes well a new window will pop open, and you'll be downloading material from Canadiana! You can close that window where the downloading is happening to stop the process. If you open your program, you can adjust it to search for your own requests \nsee this discussion for hints on how to do this\n\n\nHelp! It just doesn't work!\n\n\nWatch this video; it might make things clearer. If it still doesn't come together for you, send me a DM in our Slack space.\n\n\n\n\n\nBut when it does, you've got a pretty powerful tool now for grabbing data from one of the largest portals for Canadian history!\n\n\nJust remember to move your results from your folder before running a new search.\n\n\nThe last step: splitting your output.txt\n\n\nNow you've got a file called \noutput.txt\n on your machine. If you open that up in a text editor, you'll see it is replete with text, with various field delimiters, and other extraneous stuff that will make it difficult for you to get a sense of what's going on. One way of splitting this file up into useful bits is to split it at a useful reference point - perhaps at \noocihm\n. There are many ways of achieving this: always google your problem! Ie, 'splitting file based on a pattern' will yield many different ways of doing this. Mac users, you could try this from your terminal prompt in the folder where you've been doing your work:\n\n\nsed 's/oocihm/\\n\n/2g' output.txt | split -dl1 --aditional-suffix=.txt - splitfile\n\n\nThere are two commands there, divided by the | ('pipe') character. Indeed, the output of the first command is 'piped' as the input into the second one. The first command, \nsed\n (which stands for 'stream editor') looks for the pattern in each line in your \noutput.txt\n. When it finds it, it pipes it to the \nsplit\n command which throws it into a new file called \nsplitfile000.txt\n, iterating the file name upwards each time. Google 'sed' and 'split' for examples to try on your own. \nWindows users\n things are not quite as easy, but not too bad. In the zip file I provided you there is a script called \nsplitthingsup.sh\n which contains the same \nsed\n command (modified slightly to work on your machine). Just run \nsplitthingsup.sh\n from the git bash prompt. Ta da! Having everything split up will allow you to start doing text analysis.\n\n\nExperiment with the \nsed\n command. Google \nsed patterns\n. Can you improve the pattern to make the resulting files more useful for you?\n\n\nA student in the open access version of the course, Lee Mordechai, suggests the following script (which Windows users can run in Git Bash)\n\n\nawk '/identifier/{\"F\"++i;}{print \n \"newoutput\"i\".txt\";}' output.txt\n\n\nwhere 'identifier' is the keyword where you want the split to occur. \n\n\nThere are many different ways of getting the job done; be prepared to explore and tinker! In the end, sometimes, the best solution is the one that works, however cludgy it might seem.\n\n\nExercise 4: Tracking the ephemeral web\n\n\nNothing lasts for ever on the internet. By some measures, for some kinds of content, the half-life is on the order of \nhours\n. In this exercise, I want you to \nread this post\n from Donna Yates of the \nTrafficking Culture\n project at the University of Glasgow. Consider her workflow, and develop your own workflow for preserving copies of what you find in the course of your research. You might wish to investigate \nBibDesk\n or \nZotero\n. Then, in the narrative of your work for me in this class (ie, on your blog or similar) describe your research interests and the ways in which your materials might prove ephemeral online. Describe your workflow for dealing with this problem (that is, show you understand how your chosen tools/flow work). Integrate what you have already learned in modules 1 \n 2.\n\n\nGoing Further: Wget\n\n\nFinally, we can use wget to retrieve data (or indeed, \nmirror\n an entire site) in cases where there is no API. If you look carefully at the program in exercise 3, you'll see we used the wget command. In this exercise, we search for a collection in the internet archive, grab all of the file identifiers, and feed these into wget \nfrom the command line or terminal\n. One command, thousands of documents!\n\n\n(If you skipped the previous exercise, you need to install wget. Go to \nthe programming historian tutorial on wget\n and follow the relevant instructions (depending on your operating system) to install it.)\n\n\nThis section will follow \nMilligan p52-64\n. I am not going to spell out the steps this time; I want you to carefully peruse Milligan's presentation, and see if you can follow along (pgs 52-64: he uses wget plus a text file so that wget auto-completes the URLs and scrapes the data found there - much like the final line of the program in exercise 3). There will often be cases where you want to do something, and you find someone's presentations or notes which \nalmost\n tell you everything you need to know. Alternatively, you can complete the tutorial from the \nProgramming historian\n for this exercise. In either case, keep notes on what you've done, what works, where the problems seem to lie, what the tacit knowledge is that isn't made clear (ie, are there assumptions being made in the tutorial or the presentation that are opaque to you?).\n\n\nhints:\n\n\n\n\nremember that wget is run from your terminal (Mac) or command line (PC). Do you know where to find these?\n\n\nif you're working on a mac, when you get to the point that Milligan directs you to save the search results file (which came as csv) as a .txt file, you need to select the unicode-16 txt file option.\n\n\nthe wget command is \nwget -r -H -nc -np -nH --cut-dirs=2 -A .txt -e robots=off -l1 -i ./itemlist.txt -B 'http://archive.org/download/'\n\n\nto make sure everything is working, perhaps make a copy of your itemlist.txt file with only ten or so items in it. Call it 'itemlist-short.txt' and put that in the command. That way, if there are errors, you'll find out sooner rather than later!\n\n\nYou might want to make a note in your notebook about what all those 'flags' in the command are doing. \nHere's the wget manual\n\n\nOnce it starts working, you might see something like this:\n\n\n\n\n\nbut\n if that's all you get, and nothing downloads, the problem is in your txt file. \nMake sure\n to create your txt file with a text editor (textwrangler, sublime text, notepad) and \nnot\n from \nsave as...txt\n in excel. (If you have an option when you create the text file, make sure the encoding is 'utf-8' rather than 'utf-16').\n- if you're really stuck, see this \nblog post from the Internet Archive\n\n\nGoing Further Still: Archiving Twitter\n\n\n\n\nuse Ed Summer's \nTWARC\n to grab, archive, share, inflate, and visualize tweets\n\n\nconvert JSON to CSV with \nthis tool\n\n\nuse R to grab resources from \ndp.la\n with this wee R script on my \ngists\n (What's R? a great environment for doing many many things. Get it \nhere\n and use \nRStudio to work in it\n (grab the free version))\n\n\n\n\nDid you know?\n\n\n\n\nyou can drop geojson into github, and \ngithub will turn it into a map\n\n\ntwarc can be used to extract the coordinates from tweets, and turn them into geojson data.\n\n\nOpen Refine can also work with APIs - \ngive this tutorial using Outwith hub, Google Refine and the UK postcode API a whirl", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-2/Exercises/#module-2-exercises", 
            "text": "All four exercises are on this page. Don't forget to scroll. If you have difficulties, or if the instructions need clarification, please click the 'issues' button and leave a note. Feel free to fork and improve these instructions, if you are so inclined. Remember, these exercises get progressively more difficult, and will require you to either download materials or read materials on other websites. Give yourself plenty of time. Try them all, and remember you can turn to (I encourage you to!) your classmates for help. Work together!  nb. The third exercise will be the most difficult because everyone's machine is slightly different. Mac users should experience the least difficulty; PC users the most. Sites like  stackoverflow  will be extremely helpful. DO NOT suffer in silence as you try these exercises! Ask for help, set up a video appointment, or find me in person.", 
            "title": "Module 2: Exercises"
        }, 
        {
            "location": "/module-2/Exercises/#background", 
            "text": "Where do we go to find data? Part of that problem is solved by knowing what question you are asking, and what  kinds  of data would help solve that question. Let's assume that you have a pretty good question you want an answer to - say, something concerning social and household history in early Ottawa, like what was the role of 'corner' stores (if such things exist?) in fostering a sense of neighbourhood - and begin thinking about how you'd find data to explore that question.  Ian Milligan  recently gave a  workshop on these issues . He identifies a few different ways by which you might obtain your data, and we will follow him closely as we look at:   The Dream Case  Scraping Data yourself with free software (Outwit Hub)  Application Programming Interfaces (APIs)  Wget   There is so much data available; with these methods, we can gather enormous amounts that will let us see large-scale macroscopic patterns. At the same time, it allows us to dive into the details with comparative ease. The thing is, not all digital data are created equally. Google has spent millions digitizing  everything ; newspapers have digitized their own collections. Genealogists and local historical societies upload yoinks of digitized photographs, wills, local tax records,  you-name-it ,  every day . But, consider what Milligan has to say about  'illusionary order' :   [...] poor and misunderstood use of online newspapers can skew historical research. In a conference presentation or a lecture, it\u2019s not uknown to see the familiar yellow highlighting of found searchwords on projected images: indicative of how the original primary material was obtained. But this historical approach generally usually remains unspoken, without a critical methodological reflection. As I hope I\u2019ll show here, using Pages of the Past uncritically for historical research is akin to using a volume of the Canadian Historical Review with 10% or so of the pages ripped out. Historians, journalists, policy researchers, genealogists, and amateur researchers need to at least have a basic understanding of what goes on behind the black box.   Please go and read that full article. You should makes some notes: what are some of the key dangers? Reflect: how have you used digitized resources uncritically in the past?   Remember: 'To digitize' doesn't - or shouldn't - mean uploading a photograph of a document. There's a lot more going on that that. We'll get to that in a moment.", 
            "title": "Background"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-1-the-dream-case", 
            "text": "In the dream case, your data are not just images, but are actually sorted and structured into some kind of pre-existing database. There are choices made in the  creation  of the database, but a good database, a good project, will lay out for you their decision making, their corpora, and how they've dealt with ambiguity and so on. You search using a robust interface, and you get a well-formed spreadsheet of data in return. Two examples of 'dream case' data:   Epigraphic Database Heidelberg  Commwealth War Graves Commission, Find War Dead   Explore both databases. Perform a search of interest to you. In the case of the epigraphic database, if you've done any Latin, try searching for terms related to occupations; or you could search ' Figlin* '. In the CWGC database, search your own surname. Download your results. You now have data that you can explore! In your online research notebook, make a record (or records) of what you searched, the URL for your search   its results, and where you're keeping your data.", 
            "title": "Exercise 1: The Dream Case"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-2-outwit-hub", 
            "text": "Download, and install,  outwit hub . Do not buy it (not unless you really want to; the free trial is good enough for our purposes here)  Outwit hub is a piece of software that lets you scrape the html of a webpage. It comes with its own browser. In essence, you look at the html source code of your page to identify the  markers  that embrace the information you want. Outwit then copies just that information into a table for you, which you can then download. Take a look at the  Suda Online  and do a search for  pie  (the Suda is a 10th century Byzantine Greek encyclopedia, and its online version is one of the earliest examples of what we'd now recognize as digital humanities scholarship).  Right-click anywhere on the results page, and 'view source'. You'll see something like this: \nThe record number - the Adler number - is very clearly marked, as is the translation. Those are the bits we want. So  Open outwit hub. In the address bar at the top of the screen, type in  http://www.stoa.org/sol/ . The page will load up. Do your search again there by putting  pie  in there. (Remember, we're not searching for pie, but rather, the transliterated characters that form a root in many ancient Greek words!). The results will load up. If you then click on 'source' on the left hand side of the screen, you'll see the underlying html. Now click on 'scrapers' (again, at left hand side of screen, under 'automators'). Your page will look very similar to this:   At the bottom of the page - that's where you tell Outwit how to scrape that source.  Click \u2018scrapers,\u2019 then \u2018new,\u2019 give it a name. Enter the markers that you are interested in:  Hit Execute to run the scraper. Press \u2018Catch\u2019 to move it into your memory. Press 'export' to generate (and save) a spreadsheet of your data.  Write this exercise up in your notebook. What other data does outwit include with your export? How might that data be useful?", 
            "title": "Exercise 2: Outwit Hub"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-3-apis", 
            "text": "Sometimes, a website will have what is called an 'application programming interface'. In essence, this lets a program on your computer talk to the computer serving the website you're interested in, such that the website gives you the data that you're looking for.  That is, instead of  you  punching in the search terms, and copying and pasting the results, you get the computer to do it. More or less. The thing is, the results come back to you in a machine-readable format - often, JSON, which is a kind of text format. It looks like this:  The 'Canadiana Discovery Portal' has tonnes of materials related to Canada's history, from a wide variety of sources. Its search page is at: http://search.canadiana.ca/   Go there, and search \"ottawa\" and set the date range to 1800 to 1900. Hit enter. You are presented with a page of results -56 249 results! That's a lot of data. But do you notice the address bar of your browser? It'll say something like this:   http://search.canadiana.ca/search?q=ottawa field= df=1800 dt=1900  Your search query has been put into the URL. You're looking at the API! Everything after /search is a command that you are sending to the Canadiana server.  Scroll through the results, and you'll see a number just before the ?  http://search.canadiana.ca/search/2?df=1800 dt=1900 q=ottawa field=  http://search.canadiana.ca/search/3?df=1800 dt=1900 q=ottawa field=  http://search.canadiana.ca/search/4?df=1800 dt=1900 q=ottawa field=  ....all the way up to 5625 (ie, 10 results per page, so 56249 / 10).  If you go to http://search.canadiana.ca/support/api you can see the full list of options. What we are particularly interested in now is the bit that looks like this:  fmt=json  Add that to your query URL. How different the results now look! What's nice here is that the data is formatted in a way that makes sense to a machine - which we'll learn more about in due course.  If you look back at the full list of API options, you'll see at the bottom that one of the options is 'retrieving individual item records'; the key for that is a field called 'oocihm'. If you look at your page of json results, and scroll through them, you'll see that each individual record has its own oocihm number. If we could get a list of those, we'd be able to programmatically slot them into the commands for retrieving individual item records:  http://eco.canadiana.ca/view/oocihm.16278/?r=0 s=1 fmt=json api_text=1  The problem is: how to retrieve those oocihm numbers. The answer is, 'we write a program'. And the program that you want can be  found here . Study that program carefully. There are a number of useful things happening in there, notably 'curl', 'jq', 'sed', 'awk'. curl  is a program for downloading webpages, jq for dealing with json, and sed and awk for searching within and cleaning up text. If this all sounds greek to you, there is an excellent gentle introduction over at  William Turkel's blog .  Mac instructions:  First question: do you have wget installed on your computer? If you don't, you'll need it for this exercise and the next one (if you look in the program 'api-ex-mac.sh', you'll see that the final line of the program asks wget to go get the results you've scraped from Canadiana). Installing wget is quite straightforward - follow  the Programming Historian's instructions .  Our program requires a helper program that reads the .json data - It's called JQ. You can install this by using HOMEBREW. But to install homebrew, visit  http://brew.sh  and follow instructions. Then, to install jq, type  brew install jq  from your terminal.  Now, download api-ex-mac.sh from the repository and save it to your desktop. Open your terminal program (which you can find under 'applications' and then 'utilities'.) Navigate to your desktop:  cd desktop  then tell your computer that this 'api-ex-mac.sh' program is one that you want to run:  sudo chmod 700 file.sh  And then you can run it by typing:  ./api-ex-mac.sh   but  don't  do that yet! You'll need to change the search parameters to reflect your own interests. Do you see how you can do that? Open the program in a text editor, make the relevant change, save with a new name, and then run the new command. Move your results into a sensible location on your computer. Make a new entry (or entries) into your research notebook about this process, what worked, what hasn't, what you've found, where things are, and so on. You might want to upload your script (your .sh file) to your repository. Remember: the goal is so that you can come back to all this in a month's time and figure out  exactly what you did  to collect your data.  I've put a copy of the script for Mac folks  here.  Windows7 8 10 instructions:  Begin by making a folder for this exercise on your desktop.   You'll need  gitbash  (which comes with git; I know you already have github on your desktop, which has  git shell  but that's not what we want. Download  git  and install it, and that will give you the git bash utility, and will not mess with your github set up. 'Bash' is  \"a shell that runs commands once you type the name of a command and press   on your keyboard.\" You can see screenshots and find help on all this  here . You will be running our scraper program from within this shell.  You'll need jq  download here . You're going to put this in the folder you've made for this exercise.  Make sure that you rename it 'jq.exe'  (in somecases, the download name might be slightly different).  You'll need CoreUtils  from here . Download and install this.  You need to tell your computer that CoreUtils now exists on your machine. Go to your computer's control panel. On 'my computer' (or whatever Windows calls it these days, possibly  control panel - system and security - system - advanced system settings ) right click and select 'properties'. Then select 'advanced'.  Click on the 'environment variables' button.  In the pop-up box that opens, there is a box marked 'system variables'. Highlight the one called 'Path'. Click on 'Edit'.  In the variable box that you can now edit, there should already be many things. Scroll to the very end of that (to the right) and add:   ;C:\\Program Files (x86)\\GnuWin32\\bin   nb: make sure there's a space between files and (x86).   ...so yes, start that with a semicolon- and what you are putting in is the exact location of where the coreutils package of programs is located.  Finally, you'll need the windows version of wget installed on your machine. Get it  here  and download it to C:Windows directory.  Now:      download this  zip file . It contains the Canadiana shell script  and  all the necessary helper files.  Nb  these all need to be in the same folder in order for this to work.    open  git bash  - it'll be available via your programs menu. You do not want 'Git Gui' nor 'GitHub' nor 'Git Shell'.  Git bash . (You can also navigate to a folder in explorer, then right-click in that folder and select 'open git bash here'.)    make sure you are working within the folder you made on your desktop. The command to change directory when you are in bash (or the command line, for that matter) is  cd   ie  cd course-notes  would go one level down into a folder called 'course notes'. To go one level up, you'd type  cd ..   - ie, two periods. More help on finding your way around this interface is  here    Once you're in the right folder, all you have to do is type the name of our programme:  ./canadiana.sh  and your program will query the Canadiana API, save the results, and then use wget to retrieve the full text of each result by asking the API for those results in turn.  But don't do that yet!    You'll need to change the search parameters to reflect your own interests. Do you see how you can do that? Open the program in a text editor, make the relevant change, save with a new name (make sure to keep the same file extenion,  .sh  - in notepad, change the save as file type to  all files  and then write the new name, e.g,  canadiana-2.sh , and then run your program by typing its name at the prompt in the git bash window. When it's finished, move your results into a sensible location on your computer. Make a new entry (or entries) into your research notebook about this process, what worked, what hasn't, what you've found, where things are, and so on. You might want to upload your script (your .sh file) to your repository. Remember: the goal is so that you can come back to all this in a month's time and figure out  exactly what you did  to collect your data.  if you get an error message: jq or seq cannot be found  If this happens, your computer is not finding the coreutils installation or the  jq.exe  program. First thing: when you downloaded jq, did you make sure to change the name to  jq.exe ? When it downloads, it downloads as (eg)  jq-win64.exe . Shorten up the name. Second thing: is it in the same folder as your script? Third thing: sometimes, it just might be easier to move the  seq.exe  program into the same folder as your script, that is, your working folder. Go to  C:\\Program Files (x86)\\GnuWin32\\bin  and  copy   seq.exe  to your working folder. You will also need to copy:\n+  libiconv2.dll \n+  libintl3.dll \n...to the same folder. ( shortcut  - the same zip file I gave you that has the  canadiana.sh  in it has all of the helper files in it too. You could navigate to this unzipped folder, open a gitbash window there, and run the scripts. In this case, FYI, all you needed installed was gitbash.)  A Windows Gotcha  It might happen that you have my zipfile with all the necessary files downloaded and extracted on your machine, and yet, despite everything, when you type the  canadiana.sh  command you get a  file not found error . On Windows 10 machines (and perhaps some Windows 8 machines), you will need to add the dot slash at the beginning:  ./canadiana.sh . The script might then run, but if you look at the output carefully, nothing is being downloaded and there is also a  jq not found  error being printed out. To solve this, you'll need to edit  canadiana.sh  and find the spots where the script calls the  jq  command and replace it with  ./jq .   An alternative Windows installation  This can also work for Windows 7  if  you've got powershell 3 installed. Win7 comes with an earlier version, so you'd have to update it,  which isn't straightforward . I'm grateful to  Denis Gladkikh for his blog post on the matter ._   Make a folder somewhere handy for this exercise. Download the  api-ex-pc.sh  program into it, as well as  jq . Make sure you rename the downloaded jq file to  jq.exe .  Find, and run, 'Powershell'  At the prompt, type in these commands in sequence:   set-executionpolicy unrestricted -s cu  Say 'yes' at the prompt that asks if you really want to do this.  iex (new-object net.webclient).downloadstring('https://get.scoop.sh')  scoop install 7zip coreutils curl git grep openssh sed wget vim grep  A number of components will download and get configured to work from within powershell. When they finish, at the command prompt, you can run your program like so:  ./canadiana.sh  If all goes well a new window will pop open, and you'll be downloading material from Canadiana! You can close that window where the downloading is happening to stop the process. If you open your program, you can adjust it to search for your own requests  see this discussion for hints on how to do this  Help! It just doesn't work!  Watch this video; it might make things clearer. If it still doesn't come together for you, send me a DM in our Slack space.   But when it does, you've got a pretty powerful tool now for grabbing data from one of the largest portals for Canadian history!  Just remember to move your results from your folder before running a new search.", 
            "title": "Exercise 3: APIs"
        }, 
        {
            "location": "/module-2/Exercises/#the-last-step-splitting-your-outputtxt", 
            "text": "Now you've got a file called  output.txt  on your machine. If you open that up in a text editor, you'll see it is replete with text, with various field delimiters, and other extraneous stuff that will make it difficult for you to get a sense of what's going on. One way of splitting this file up into useful bits is to split it at a useful reference point - perhaps at  oocihm . There are many ways of achieving this: always google your problem! Ie, 'splitting file based on a pattern' will yield many different ways of doing this. Mac users, you could try this from your terminal prompt in the folder where you've been doing your work:  sed 's/oocihm/\\n /2g' output.txt | split -dl1 --aditional-suffix=.txt - splitfile  There are two commands there, divided by the | ('pipe') character. Indeed, the output of the first command is 'piped' as the input into the second one. The first command,  sed  (which stands for 'stream editor') looks for the pattern in each line in your  output.txt . When it finds it, it pipes it to the  split  command which throws it into a new file called  splitfile000.txt , iterating the file name upwards each time. Google 'sed' and 'split' for examples to try on your own.  Windows users  things are not quite as easy, but not too bad. In the zip file I provided you there is a script called  splitthingsup.sh  which contains the same  sed  command (modified slightly to work on your machine). Just run  splitthingsup.sh  from the git bash prompt. Ta da! Having everything split up will allow you to start doing text analysis.  Experiment with the  sed  command. Google  sed patterns . Can you improve the pattern to make the resulting files more useful for you?  A student in the open access version of the course, Lee Mordechai, suggests the following script (which Windows users can run in Git Bash)  awk '/identifier/{\"F\"++i;}{print   \"newoutput\"i\".txt\";}' output.txt  where 'identifier' is the keyword where you want the split to occur.   There are many different ways of getting the job done; be prepared to explore and tinker! In the end, sometimes, the best solution is the one that works, however cludgy it might seem.", 
            "title": "The last step: splitting your output.txt"
        }, 
        {
            "location": "/module-2/Exercises/#exercise-4-tracking-the-ephemeral-web", 
            "text": "Nothing lasts for ever on the internet. By some measures, for some kinds of content, the half-life is on the order of  hours . In this exercise, I want you to  read this post  from Donna Yates of the  Trafficking Culture  project at the University of Glasgow. Consider her workflow, and develop your own workflow for preserving copies of what you find in the course of your research. You might wish to investigate  BibDesk  or  Zotero . Then, in the narrative of your work for me in this class (ie, on your blog or similar) describe your research interests and the ways in which your materials might prove ephemeral online. Describe your workflow for dealing with this problem (that is, show you understand how your chosen tools/flow work). Integrate what you have already learned in modules 1   2.", 
            "title": "Exercise 4: Tracking the ephemeral web"
        }, 
        {
            "location": "/module-2/Exercises/#going-further-wget", 
            "text": "Finally, we can use wget to retrieve data (or indeed,  mirror  an entire site) in cases where there is no API. If you look carefully at the program in exercise 3, you'll see we used the wget command. In this exercise, we search for a collection in the internet archive, grab all of the file identifiers, and feed these into wget  from the command line or terminal . One command, thousands of documents!  (If you skipped the previous exercise, you need to install wget. Go to  the programming historian tutorial on wget  and follow the relevant instructions (depending on your operating system) to install it.)  This section will follow  Milligan p52-64 . I am not going to spell out the steps this time; I want you to carefully peruse Milligan's presentation, and see if you can follow along (pgs 52-64: he uses wget plus a text file so that wget auto-completes the URLs and scrapes the data found there - much like the final line of the program in exercise 3). There will often be cases where you want to do something, and you find someone's presentations or notes which  almost  tell you everything you need to know. Alternatively, you can complete the tutorial from the  Programming historian  for this exercise. In either case, keep notes on what you've done, what works, where the problems seem to lie, what the tacit knowledge is that isn't made clear (ie, are there assumptions being made in the tutorial or the presentation that are opaque to you?).  hints:   remember that wget is run from your terminal (Mac) or command line (PC). Do you know where to find these?  if you're working on a mac, when you get to the point that Milligan directs you to save the search results file (which came as csv) as a .txt file, you need to select the unicode-16 txt file option.  the wget command is  wget -r -H -nc -np -nH --cut-dirs=2 -A .txt -e robots=off -l1 -i ./itemlist.txt -B 'http://archive.org/download/'  to make sure everything is working, perhaps make a copy of your itemlist.txt file with only ten or so items in it. Call it 'itemlist-short.txt' and put that in the command. That way, if there are errors, you'll find out sooner rather than later!  You might want to make a note in your notebook about what all those 'flags' in the command are doing.  Here's the wget manual  Once it starts working, you might see something like this:   but  if that's all you get, and nothing downloads, the problem is in your txt file.  Make sure  to create your txt file with a text editor (textwrangler, sublime text, notepad) and  not  from  save as...txt  in excel. (If you have an option when you create the text file, make sure the encoding is 'utf-8' rather than 'utf-16').\n- if you're really stuck, see this  blog post from the Internet Archive", 
            "title": "Going Further: Wget"
        }, 
        {
            "location": "/module-2/Exercises/#going-further-still-archiving-twitter", 
            "text": "use Ed Summer's  TWARC  to grab, archive, share, inflate, and visualize tweets  convert JSON to CSV with  this tool  use R to grab resources from  dp.la  with this wee R script on my  gists  (What's R? a great environment for doing many many things. Get it  here  and use  RStudio to work in it  (grab the free version))   Did you know?   you can drop geojson into github, and  github will turn it into a map  twarc can be used to extract the coordinates from tweets, and turn them into geojson data.  Open Refine can also work with APIs -  give this tutorial using Outwith hub, Google Refine and the UK postcode API a whirl", 
            "title": "Going Further Still: Archiving Twitter"
        }, 
        {
            "location": "/module-3/Wrangling Data/", 
            "text": "Wrangling Data\n\n\nIn the previous module, we successfully grabbed \nlots\n of data from various online repositories. Some of it was already in well-structured tables; much of it was not. All of it was text though. Initially, it (or most of it) was just scanned images of documents. At some point, \nobject character recognition\n was used to identify the black dots from the white dots in those images, to recognize the patterns that make up letters, numbers, and punctuation. There are commercial products that can do this (and we have some installed in the Underhill Research Room that you can use), and there are \nfree products that you can install\n on your computer to do it yourself.\n\n\nIt all looks so neat and tidy. Ian Milligan discusses this \n'illusionary order'\n and its implications for historians:\n\n\n\n\nIn this article, I make two arguments. Firstly, online historical databases have profoundly shaped Canadian historiography. In a shift that is rarely \u2013 if ever \u2013 made explicit, Canadian historians have profoundly reacted to the availability of online databases. Secondly, historians need to understand how OCR works, in order to bring a level of methodological rigor to their work that use these sources.\n\n\n\n\nJust as we saw with Ted Underwood's article on \ntheorizing search\n, these 'simple' steps in the research process are anything but. They are also profoundly theoretical in how they change what it is we can know. In archaeology, every step of the method, every stage in the process, has a profound impact on the stories we eventually tell about the past. Decisions we make destroy data, and create new data. Historians aren't used to thinking about these kinds of issues!\n\n\nThere are also \nmanual\n ways of doing the same thing as OCR does - we call these things 'humans', and we organize their work through 'crowdsourcing'. We break the process up into wee manageable steps, and make these available over the net. Sometimes we gamify these steps, to make them more 'fun'. If several people all work on the same piece of text, the thinking is that errors will cancel each other out: a proper transcription will emerge from the work of the crowd. Take a look at these two pieces concerning the \nTranscribe Bentham\n project:\n\n\n\n\nCauser \n Wallace, \nBuilding A Volunteer Community: Results and Findings from Transcribe Bentham\n DHQ 6.2, 2012\n\n\nCauser, Tonra, \n Wallace \nTranscription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham\nLLC 27.2, 2012\n\n\n\n\nWhile transcriptions might've provided the earliest examples of crowdsourcing research (but see also \nThe HeritageCrowd Project\n and the subsequent \n'How I Lost the Crowd'\n), other tasks are now finding their way into the crowdsourced world - see the archaeological applications within the \nMicroPasts\n platform. These include things like 'masking' artefact photographs in order to develop 3d photogrammetric models.\n\n\nBut often, we don't have a whole crowd. We're just one person, alone, with a computer, at the archive. Or working with someone else's \ndigitized image that we found online\n. How do we wrangle that data? Let's start with \nM. H. Beal's account of how she 'xml'd her way to data management'\n and then we'll consider a few more of the nuts and bolts of her work in \nOA TEI-XML DH on the WWW; or, My Guide to Acronymic Success\n.\n\n\nThis kind of work is extraordinarily important!\n So we're going to try our hand at it too. (Now, if we had a \nseriously\n big project where we were transcribing lots of text, we'd invest in a dedicated XML editor like \nOxygen\n - there are \nplugins available and frameworks for doing historical transcription\n on this platform. There is a 30 day free trial license if you want to give it a try. But for now, Notepad++, Textwrangler, Komodo Edit, Sublime text, or any of a number of good text editors will do all that we need to do). Also, check out the \nTEI\n. Take 15 minutes and read throuhg \nWhat is XML and Why Should Humanists Care?\n by David Birnbaum. Keep notes in your notebook!\n\n\nExercises\n\n\nIn the exercises for this week we are going to focus on some bare-bones wrangling of data. First, we are going to do some activities and exercises to get in the right frame of mind. Then, we'll transcribe and mark up some text that has already been digitized (in the sense that there exists a digital image). If you've done HIST2809 with me, this will feel rather familiar - but instead of making a transcription that sits in our notebook, we'll make one that lives online. Since it is online, the use of xml tags like \n or \n etc allow us to do some other quite interesting things. We'll look at M. Beal's Colonial Newspaper Database in more detail. This will introduce us to the 'Text Encoding Initiative' and some of the scholarly work surrounding making scholarly editions online. Some of you might be interested in how all of this ties into \nlinked data\n, so I'll provide further resources to that world for those who wish to go exploring.\n\n\nThen, we'll switch gears and we'll use \nregular expressions\n to search and extract information from the Diplomatic Correspondence of the Republic of Texas, which you'll find at the Internet Archive. If you're on a PC, download \nNotepad++\n - this is a souped-up version of the simple notepad application, and allows us to do very useful things indeed. If you're on a Mac, TextWrangler is probably already installed and is all you need. If you're working in Linux, you can use whatever text editor you're familiar with.\n\n\nWe'll conclude by using 'Open Refine' to tidy up the information we extracted from the Texan correspondence. \n\n\nThings you will learn in this module:\n\n\n\n\nthe basic concepts of XML and TEI for marking up text\n\n\nthe power of regular expressions. 'Search' and 'Replace' in Word just won't cut it any more for you! (Another reason why you should write in Markdown in the first place and then convert to Word for the final typesetting - Indeed, there will be an optional exercise to install and use \nPandoc\n to convert your .md files to .docx).\n\n\nOpen Refine as a powerful engine for tidying up the messiness that is ocr'd text.\n\n\n\n\nArticles where the wrangling of data is discussed\n\n\nBlevins, \nMining and Mapping the Production of Space A View of the World from Houston\n\n\nBlevins, \nSpace, Nation, and the Triumph of Region: A View of the World from Houston\n\n\nBlog posts\n\n\nIan Milligan on Imageplot\n and \nhere\n\n\nShawn Graham on extracting text \n diy OCR\n\n\nmore will be added in due time", 
            "title": "Data is messy"
        }, 
        {
            "location": "/module-3/Wrangling Data/#wrangling-data", 
            "text": "In the previous module, we successfully grabbed  lots  of data from various online repositories. Some of it was already in well-structured tables; much of it was not. All of it was text though. Initially, it (or most of it) was just scanned images of documents. At some point,  object character recognition  was used to identify the black dots from the white dots in those images, to recognize the patterns that make up letters, numbers, and punctuation. There are commercial products that can do this (and we have some installed in the Underhill Research Room that you can use), and there are  free products that you can install  on your computer to do it yourself.  It all looks so neat and tidy. Ian Milligan discusses this  'illusionary order'  and its implications for historians:   In this article, I make two arguments. Firstly, online historical databases have profoundly shaped Canadian historiography. In a shift that is rarely \u2013 if ever \u2013 made explicit, Canadian historians have profoundly reacted to the availability of online databases. Secondly, historians need to understand how OCR works, in order to bring a level of methodological rigor to their work that use these sources.   Just as we saw with Ted Underwood's article on  theorizing search , these 'simple' steps in the research process are anything but. They are also profoundly theoretical in how they change what it is we can know. In archaeology, every step of the method, every stage in the process, has a profound impact on the stories we eventually tell about the past. Decisions we make destroy data, and create new data. Historians aren't used to thinking about these kinds of issues!  There are also  manual  ways of doing the same thing as OCR does - we call these things 'humans', and we organize their work through 'crowdsourcing'. We break the process up into wee manageable steps, and make these available over the net. Sometimes we gamify these steps, to make them more 'fun'. If several people all work on the same piece of text, the thinking is that errors will cancel each other out: a proper transcription will emerge from the work of the crowd. Take a look at these two pieces concerning the  Transcribe Bentham  project:   Causer   Wallace,  Building A Volunteer Community: Results and Findings from Transcribe Bentham  DHQ 6.2, 2012  Causer, Tonra,   Wallace  Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham LLC 27.2, 2012   While transcriptions might've provided the earliest examples of crowdsourcing research (but see also  The HeritageCrowd Project  and the subsequent  'How I Lost the Crowd' ), other tasks are now finding their way into the crowdsourced world - see the archaeological applications within the  MicroPasts  platform. These include things like 'masking' artefact photographs in order to develop 3d photogrammetric models.  But often, we don't have a whole crowd. We're just one person, alone, with a computer, at the archive. Or working with someone else's  digitized image that we found online . How do we wrangle that data? Let's start with  M. H. Beal's account of how she 'xml'd her way to data management'  and then we'll consider a few more of the nuts and bolts of her work in  OA TEI-XML DH on the WWW; or, My Guide to Acronymic Success .  This kind of work is extraordinarily important!  So we're going to try our hand at it too. (Now, if we had a  seriously  big project where we were transcribing lots of text, we'd invest in a dedicated XML editor like  Oxygen  - there are  plugins available and frameworks for doing historical transcription  on this platform. There is a 30 day free trial license if you want to give it a try. But for now, Notepad++, Textwrangler, Komodo Edit, Sublime text, or any of a number of good text editors will do all that we need to do). Also, check out the  TEI . Take 15 minutes and read throuhg  What is XML and Why Should Humanists Care?  by David Birnbaum. Keep notes in your notebook!  Exercises  In the exercises for this week we are going to focus on some bare-bones wrangling of data. First, we are going to do some activities and exercises to get in the right frame of mind. Then, we'll transcribe and mark up some text that has already been digitized (in the sense that there exists a digital image). If you've done HIST2809 with me, this will feel rather familiar - but instead of making a transcription that sits in our notebook, we'll make one that lives online. Since it is online, the use of xml tags like   or   etc allow us to do some other quite interesting things. We'll look at M. Beal's Colonial Newspaper Database in more detail. This will introduce us to the 'Text Encoding Initiative' and some of the scholarly work surrounding making scholarly editions online. Some of you might be interested in how all of this ties into  linked data , so I'll provide further resources to that world for those who wish to go exploring.  Then, we'll switch gears and we'll use  regular expressions  to search and extract information from the Diplomatic Correspondence of the Republic of Texas, which you'll find at the Internet Archive. If you're on a PC, download  Notepad++  - this is a souped-up version of the simple notepad application, and allows us to do very useful things indeed. If you're on a Mac, TextWrangler is probably already installed and is all you need. If you're working in Linux, you can use whatever text editor you're familiar with.  We'll conclude by using 'Open Refine' to tidy up the information we extracted from the Texan correspondence.   Things you will learn in this module:   the basic concepts of XML and TEI for marking up text  the power of regular expressions. 'Search' and 'Replace' in Word just won't cut it any more for you! (Another reason why you should write in Markdown in the first place and then convert to Word for the final typesetting - Indeed, there will be an optional exercise to install and use  Pandoc  to convert your .md files to .docx).  Open Refine as a powerful engine for tidying up the messiness that is ocr'd text.   Articles where the wrangling of data is discussed  Blevins,  Mining and Mapping the Production of Space A View of the World from Houston  Blevins,  Space, Nation, and the Triumph of Region: A View of the World from Houston  Blog posts  Ian Milligan on Imageplot  and  here  Shawn Graham on extracting text   diy OCR  more will be added in due time", 
            "title": "Wrangling Data"
        }, 
        {
            "location": "/module-3/Exercises/", 
            "text": "Module 3: Exercises\n\n\nExercise 1\n\n\nWhen we mark up a text with the semantic hooks and signs that explain we are talking about 'London, Ontario' rather than 'London, UK', we've made the text a whole lot more useful for other scholars or tools. In this exercise, you will do some basic marking up of a text using standards from the Text Encoding Initiative. (Some of the earliest digital history work was along these lines).  The \nTEI\n exercise requires carefully attention to detail. Read through it before you try it. In this exercise, you'll transcribe a page from an abolitionist's pamphlet. You'll also think about ways of transforming the resulting XML into other formats. Make your notes in your open notebook, and upload your XML and your XSL file to your own repository. (As an added optional challenge, create a gh-pages branch and figure out the direct URL to your XML file, and email that URL to me).\n\n\nExercise 2\n\n\nWhen we have text that has been marked up, we can do interesting things with it. In the previous exercise, you saw that XSL can be used to add styles to your XML (which a browser can interpret and display). You can see how having those tags makes for easier searching for information as well, right? Sometimes things are not well marked up. Sometimes, it's all a real mess. In exercise two, we'll explore 'regular expressions', (aka 'Regex') or ways of asking the computer to search for \npatterns\n rather than matching exact text. \n\n\nThis exercise follows a tutorial written for \nThe Macroscope\n.(Another good reference is \nhttp://www.regular-expressions.info/\n). \n\n\n\n\nStart with \na gentle introduction to regex\n and then begin the \nregex exercise\n. \n\n\n\n\nYou should have \nRegeXr\n, an interactive tutorial that shows us what various regular expressions can do, open in a browser window somewhere so you can test your regular expressions out \nfirst\n before applying them to your data!  We will use the power of regular expressions to search for particular patterns in a published volume of the correspondence of the Republic of Texas. We'll use regex to massage the information into a format that we can then use to generate a social network of letter writers over time. (You might want to give this tutorial a try, too: \nUnderstanding Regular Expressions\n).\n\n\nWhat if you had \nlots\n of documents that you needed to clean up? One way of doing it would be to write a python program that applied your regex automatically, across all the files in a folder. \nOptional advanced exercise\n: \nCleaning OCR'd Text with Regular Expressions\n\n\nExercise 3\n\n\nOpen Refine\n is the final tool we'll explore in this module. This engine allows us to clean up our messy data. We will feed it the results from Exercise 2 in order to consolidate individuals (ie, 'Shawn' and 'S4awn' are probably the same person, so Open Refine will consolidate that information for us). This exercise also follows a tutorial originally written for \nThe Macroscope\n. \nHere are the instructions\n.\n\n\nFor more advanced usage of Open Refine, as an optional exercise you can also try \nThe Programming Historian's Tutorial on Open Refine\n. \n\n\nExercise 4\n\n\nThe *Stanford Named Entity Recognizer' is a program that enables you to automatically tag words in your corpus according to whether or not they are place names, individuals, and so on. The output can then be subsequently extracted and visualized, which you'll learn in Module 4. \nWorking with the Stanford NER\n.\n\n\nOptional exercises\n\n\nIntroduction to the Command Line\n\n\nCounting and Mining Data with Unix", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-3/Exercises/#module-3-exercises", 
            "text": "", 
            "title": "Module 3: Exercises"
        }, 
        {
            "location": "/module-3/Exercises/#exercise-1", 
            "text": "When we mark up a text with the semantic hooks and signs that explain we are talking about 'London, Ontario' rather than 'London, UK', we've made the text a whole lot more useful for other scholars or tools. In this exercise, you will do some basic marking up of a text using standards from the Text Encoding Initiative. (Some of the earliest digital history work was along these lines).  The  TEI  exercise requires carefully attention to detail. Read through it before you try it. In this exercise, you'll transcribe a page from an abolitionist's pamphlet. You'll also think about ways of transforming the resulting XML into other formats. Make your notes in your open notebook, and upload your XML and your XSL file to your own repository. (As an added optional challenge, create a gh-pages branch and figure out the direct URL to your XML file, and email that URL to me).", 
            "title": "Exercise 1"
        }, 
        {
            "location": "/module-3/Exercises/#exercise-2", 
            "text": "When we have text that has been marked up, we can do interesting things with it. In the previous exercise, you saw that XSL can be used to add styles to your XML (which a browser can interpret and display). You can see how having those tags makes for easier searching for information as well, right? Sometimes things are not well marked up. Sometimes, it's all a real mess. In exercise two, we'll explore 'regular expressions', (aka 'Regex') or ways of asking the computer to search for  patterns  rather than matching exact text.   This exercise follows a tutorial written for  The Macroscope .(Another good reference is  http://www.regular-expressions.info/ ).    Start with  a gentle introduction to regex  and then begin the  regex exercise .    You should have  RegeXr , an interactive tutorial that shows us what various regular expressions can do, open in a browser window somewhere so you can test your regular expressions out  first  before applying them to your data!  We will use the power of regular expressions to search for particular patterns in a published volume of the correspondence of the Republic of Texas. We'll use regex to massage the information into a format that we can then use to generate a social network of letter writers over time. (You might want to give this tutorial a try, too:  Understanding Regular Expressions ).  What if you had  lots  of documents that you needed to clean up? One way of doing it would be to write a python program that applied your regex automatically, across all the files in a folder.  Optional advanced exercise :  Cleaning OCR'd Text with Regular Expressions", 
            "title": "Exercise 2"
        }, 
        {
            "location": "/module-3/Exercises/#exercise-3", 
            "text": "Open Refine  is the final tool we'll explore in this module. This engine allows us to clean up our messy data. We will feed it the results from Exercise 2 in order to consolidate individuals (ie, 'Shawn' and 'S4awn' are probably the same person, so Open Refine will consolidate that information for us). This exercise also follows a tutorial originally written for  The Macroscope .  Here are the instructions .  For more advanced usage of Open Refine, as an optional exercise you can also try  The Programming Historian's Tutorial on Open Refine .", 
            "title": "Exercise 3"
        }, 
        {
            "location": "/module-3/Exercises/#exercise-4", 
            "text": "The *Stanford Named Entity Recognizer' is a program that enables you to automatically tag words in your corpus according to whether or not they are place names, individuals, and so on. The output can then be subsequently extracted and visualized, which you'll learn in Module 4.  Working with the Stanford NER .", 
            "title": "Exercise 4"
        }, 
        {
            "location": "/module-3/Exercises/#optional-exercises", 
            "text": "Introduction to the Command Line  Counting and Mining Data with Unix", 
            "title": "Optional exercises"
        }, 
        {
            "location": "/module-4/Seeing Patterns/", 
            "text": "Seeing Patterns\n\n\nIn the previous module, we collected and cleaned vast amounts of historical data. Let's see what patterns emerge. Part exploration, part reflection, and part argument, these two stages are not linear but feed into one another. Exploration and preliminary visualizations are often about finding the holes. Sometimes, a quick visualization helps you understand what is missing, or what is a glitch in your method, or what is just plain mistaken in your assumptions. I once spent quite a lot of time staring at a network graph before I realized that I'd neglected to import the entire file - I was trying to interpret just a subset of it! Back to my wrangling I went, and in the process of cleaning that data, I realized there were patterns that would be better visualized as simple line plots. I visualized these - and found other errors. Back to wrangling. In the end, a combination of network graph and simple line plots allowed me to identify a story about influence and control of landholding (I was working with archaeological data).\n\n\nIn this module, let us begin by looking at a couple of pieces that explore this cyclical dynamic between data wrangling and exploration. In our Slack space for this module or in your notebooks, I want you to explicitly discuss the work of the following scholars. Make explicit connections with things you've read/explored/studied/developed in other history classes! \n\n\nIn essence: what would your last essay last term have looked like, if you'd been thinking along these lines?\n\n\nBegin by taking a look at the \nMapping Texts\n project. Look at the work of Michelle Moravec who is using \ncorpus linguistics tools to understand the history of women's suffrage\n (and also \nhere\n). Listen to Micki Kaufmann on \nQuantifying Kissinger\n. (Her \nmethods are detailed here\n.) Talk about \nnetwork analysis\n. Talk about \ntopic modeling\n (and also \nhere\n). Talk about \nprinciples of visualization\n. You should probably talk about \nEdward Tufte\n, too.\n\n\nTalk. What could history be like if more of our materials went through these mills?\n\n\nExercises\n\n\nThe \nexercises\n in this module will pick up where we left off, with the Texan Correspondence. You'll represent the exchange of letters as a network. We'll try to extract place names from it. We'll topic model the letters themselves. We'll explore various ways of visualizing those results.\n\n\nThings you will learn in this module\n\n\n\n\nimporting, querying, and visualizing networks with \nGephi\n\n\ntopic modeling with the GUI topic modeling tool, and MALLET at the command line, and in R\n\n\nsimple maps with CartoDB (which may include georectifying and displaying historical maps as base layers)\n\n\ncorpus linguistics with AntConc\n\n\nTF-IDF with Overview\n\n\nquick visualizations using \nRAW\n \n\n\n\n\nWe will be busy in this module. \nDo not be afraid\n to ask myself, your peers, or the wider DH community for help and advice! It's the only way we grow.\n\n\nAnd finally...\n\n\nJust because there is a package, or a routine, or an approach to doing \nx\n with your data \ndoes not mean\n that you park your critical apparatus at the door. Consider: Matthew Jockers has done some amazing work putting together \na package for the R statistical language that helps one analyze plot arcs in thousands of novels at once\n. He describes how it works on his \nblog\n. Annie Swafford \nexplores this package\n in a comprehensive blog post. She highlights important issues in the way the package deals with the complexity of the world, and how that might have an impact on results and conclusions drawn from using the package. The interplay of the work that Jockers has done, and the dialogue that Swafford has started with Jockers, is an important feature (and perhaps, a defining feature) of digital humanities scholarship. Jocker builds; and Swafford approaches the code from a humanistic perspective; and in the dialectic of their exchange, the code and our ability to understand the code's limitations and potentials will improve. And so we understand the world better.\n\n\nWhen you choose your exploratory method, you need to consider that the method has its own agenda!", 
            "title": "Seeing Patterns"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#seeing-patterns", 
            "text": "In the previous module, we collected and cleaned vast amounts of historical data. Let's see what patterns emerge. Part exploration, part reflection, and part argument, these two stages are not linear but feed into one another. Exploration and preliminary visualizations are often about finding the holes. Sometimes, a quick visualization helps you understand what is missing, or what is a glitch in your method, or what is just plain mistaken in your assumptions. I once spent quite a lot of time staring at a network graph before I realized that I'd neglected to import the entire file - I was trying to interpret just a subset of it! Back to my wrangling I went, and in the process of cleaning that data, I realized there were patterns that would be better visualized as simple line plots. I visualized these - and found other errors. Back to wrangling. In the end, a combination of network graph and simple line plots allowed me to identify a story about influence and control of landholding (I was working with archaeological data).  In this module, let us begin by looking at a couple of pieces that explore this cyclical dynamic between data wrangling and exploration. In our Slack space for this module or in your notebooks, I want you to explicitly discuss the work of the following scholars. Make explicit connections with things you've read/explored/studied/developed in other history classes!   In essence: what would your last essay last term have looked like, if you'd been thinking along these lines?  Begin by taking a look at the  Mapping Texts  project. Look at the work of Michelle Moravec who is using  corpus linguistics tools to understand the history of women's suffrage  (and also  here ). Listen to Micki Kaufmann on  Quantifying Kissinger . (Her  methods are detailed here .) Talk about  network analysis . Talk about  topic modeling  (and also  here ). Talk about  principles of visualization . You should probably talk about  Edward Tufte , too.  Talk. What could history be like if more of our materials went through these mills?", 
            "title": "Seeing Patterns"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#exercises", 
            "text": "The  exercises  in this module will pick up where we left off, with the Texan Correspondence. You'll represent the exchange of letters as a network. We'll try to extract place names from it. We'll topic model the letters themselves. We'll explore various ways of visualizing those results.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#things-you-will-learn-in-this-module", 
            "text": "importing, querying, and visualizing networks with  Gephi  topic modeling with the GUI topic modeling tool, and MALLET at the command line, and in R  simple maps with CartoDB (which may include georectifying and displaying historical maps as base layers)  corpus linguistics with AntConc  TF-IDF with Overview  quick visualizations using  RAW     We will be busy in this module.  Do not be afraid  to ask myself, your peers, or the wider DH community for help and advice! It's the only way we grow.", 
            "title": "Things you will learn in this module"
        }, 
        {
            "location": "/module-4/Seeing Patterns/#and-finally", 
            "text": "Just because there is a package, or a routine, or an approach to doing  x  with your data  does not mean  that you park your critical apparatus at the door. Consider: Matthew Jockers has done some amazing work putting together  a package for the R statistical language that helps one analyze plot arcs in thousands of novels at once . He describes how it works on his  blog . Annie Swafford  explores this package  in a comprehensive blog post. She highlights important issues in the way the package deals with the complexity of the world, and how that might have an impact on results and conclusions drawn from using the package. The interplay of the work that Jockers has done, and the dialogue that Swafford has started with Jockers, is an important feature (and perhaps, a defining feature) of digital humanities scholarship. Jocker builds; and Swafford approaches the code from a humanistic perspective; and in the dialectic of their exchange, the code and our ability to understand the code's limitations and potentials will improve. And so we understand the world better.  When you choose your exploratory method, you need to consider that the method has its own agenda!", 
            "title": "And finally..."
        }, 
        {
            "location": "/module-4/Exercises/", 
            "text": "Module 4 Exercises\n\n\nThere are \nmany\n different tools and approaches\n you could use to visualize your data, both as a preliminary pass to spot the holes and also for more formal analysis. In which case, for this module, I would like you to select two of these exercises which seem most germane for your own research project. You are welcome to work through more of them, of course, but I want the exercises to move your own research forward. Some of these I wrote; some are adapted from \nThe Macroscope\n; others are adapted or used holus-bolus from scholars like \nMiriam Posner\n, \nFred Gibbs\n, and \nHeather Froehlich\n (and I'm grateful that they shared their materials!)\n\n\nnb\n If you start working with the R exercises below, I would suggest you read the introductory bits from Lincoln Mullen's book-in-progress, \nDH Methods in R\n, especially the 'setup' part under 'getting started' (pay attention to the bit on installing packages and dependencies). If you spot any exercises in Mullen's book that seem relevant to your project, you may do those as an alternative to the ones here. \nAlternatively\n, go to \nSwirl\n and \nlearn the basics of R within R\n (It's an interactive tutorial. Try it out.)\n\n\n\n\n\n\n\n\nTexts\n\n\nNetworks\n\n\nMaps\n\n\nCharts\n\n\n\n\n\n\n\n\n\n\nTopic Modeling Tool\n\n\nNetwork analysis and visualization\n\n\nSimple mapping \n georectifying\n\n\nQuick charts using RAW\n\n\n\n\n\n\nTopic Modeling in R\n\n\nConverting 2-mode to 1-mode\n\n\nQGIS (tutorials by Fred Gibbs)\n\n\n\n\n\n\n\n\nText Analysis with OverviewProject\n\n\nGraphing the Net\n\n\nGeoparsing with Python\n\n\n\n\n\n\n\n\nCorpus Linguistics with AntConc\n\n\nChoose your own adventure\n\n\nPalladio with Posner\n\n\n\n\n\n\n\n\nText Analysis with Voyant\n\n\n\n\n\n\n\n\n\n\n\n\nText Analysis in R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1\n\n\nNetwork Visualization\n\n\nIn exercise 1, you will transform your Texan Correspondence data into a network, which you will then visualize with Gephi. The detailed instructions are \nhere\n. I would recommend that you also take a long look at Scott Weingart's series, \nNetworks Demystified\n. Finally, \nheed our warning\n.\n\n\n\n\nExercise 2\n\n\nTopic Modeling Tool\n\n\nIn exercise 2, you will use the 'Topic Modeling Tool' to create a simple topic model and a webpage that allows you to browse the results. \n\n\n\n\nDownload the \ntool\n. (The site for the tool is \nhttps://code.google.com/p/topic-modeling-tool/\n. \n\n\nMake sure you have the Colonial Newspaper Database handy on your machine. (You can grab my copy from \nhere\n).\n\n\nDouble-click on the file you downloaded in step 1. This will open a java-based graphical user interface with one of the most common topic-modeling approaches, 'Latent Dirichlet Allocation'.\n\n\nSet the input to be the Colonial Newspaper Database.\n\n\nSet the output to be somewhere neat and tidy on your computer.\n\n\nSet the number of topics you'd like to model.\n\n\nClick 'train topics' to run the algorithm.\n\n\nWhen it finishes, go to the folder you selected for output, and find the file 'all_topics.html' in the 'output_html' folder. Click on that, and you now have a browser-based way of navigating your topics and documents. In the output_csv folder created, you will find the same information as csv, which you could then input into a spreadsheet for other kinds of visualizations (which we'll talk about in class.)\n\n\n\n\nMake a note in your open notebook about your process and your observations. How does reading this material in this way change/challenge/or focus your understanding of the material?\n\n\nGoing Further\n Remember when we did the Canadiana API and WGET exercises in Module 2? Somewhere on your machine you have a collection of those materials. Now, you can load those materials into the Topic Modeling Tool if you have all of the txt files in a single folder. In the case of the slavery documents, that was something like 7500 items. That's a lot of drag-and-drop. You can 'flatten' the folder structure so that all of the documents in your subfolders are put into a single folder. If you are on a Mac, \ntry these instructions\n. On a PC, \ntry this one\n (there are scripts you can use, but for the time being this is probably simplest).  Then, you can point your topic modeling tool at your flattened folder, and \nboom\n you have a topic model fitted to your collection.\n\n\n\n\nexercise 3\n\n\nTopic Modeling in R\n\n\nExercise 2 was quite a simple way to do topic modeling. There are more powerful ways, and one of these uses a program called R Studio, which is an interface for the R statistical programming language. R is a powerful language for exploring, visualizing, and manipulating all kinds of data, including textual. It is however not the most intutive of environments to work in. In which case, \nRStudio\n is what we need. Download the free version and install it on your machine. *Note also that you need to have R \ndownloaded \n installed first\n! Then, go to \nhttp://tryr.codeschool.com/\n and work your way through some of that tutorial. This tutorial mimics working right in the R console. Remember working in git bash or the terminal in Module 3? It's somewhat similar to that, but just for R. A handy pdf that explains a bit more about working within the R Studio enivornment can be had \nhere\n. In essence, you put your code in the script window, execute each line of it, and the output appears in the console. Or in the image plots window. \nThis handout will guide you around the interface\n.\n\n\nIn this exercise, we're going to grab the Colonial Newspaper Database from my github page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi). The walkthrough \ncan be found here\n. Each gray block is something to copy-and-paste into your script window in R Studio. Then, put the cursor at the start of the first line, and hit ctrl+enter to get RStudio to execute each line. In the walkthrough, when you get to another gray block, just copy and paste it into your script window after the earlier block. Work your way through the walkthrough. The walkthrough gives you an indication of what the output should look like as you move through it. (The walkthrough was written inside R, and then turned into HTML using an R package called 'Knittr'. You can see that this has implications for open research! For reference, \nhere's the original Rmd (R markdown) file that generated the walkthrough\n.)\n\n\nBy the way: when you run this line: \ntopic.model$train(1000)\n your console will fill up with data as it iterates 1000 times over the entire corpus, fitting a topic model to it. This is as it should be!\n\n\nIn this way, you'll build up an entire script for topic modeling materials you find on the web. You can then save your script and upload it to your open notebook. In the future, you'd be able to make just a few changes here and there in order to grab and explore different data.\n\n\nMake a note in your open notebook about your process and your observations.\n\n\nGoing further\n If you wanted to use that script on the materials you collected in module 2, you would have to tell R to load up those materials from a directory, rather than by reading a csv file. Take a look at \nmy script for topic modeling the Ferguson Grand Jury documents\n, especially this line:\n\n\ndocuments \n- mallet.read.dir(\"originaldocs/1000chunks/\")\n\n\nYou feed it the path to your documents. If you are on a windows machine, the path would look a bit different, for instance: \n\n\n\"C:\\\\research\\\\originaldocs\\\\1000chunks\\\\\"\n\n\n\n\nexercise 4\n\n\nText Analysis with Overview\n\n\nIn exercise 4, we're going to look at the Colonial Newspaper Database again, but this time using a tool called 'Overview'. Overview uses a different approach that the topic models we've been discussing. In essence, it looks at word frequencies and their distributions within a document, and within a corpus, to organize the documents into folders of progressively similar word use. \n\n\nYou can download Overview to run on your own machine, but for our purposes, the hosted version at \nhttps://www.overviewdocs.com/\n is sufficient. Go to that page, watch the video, create an account, and then log in. (More help about how Overview works \nmay be found on their blog\n, including helpful videos.)\n\n\nOnce you're inside, click 'import from a CSV file', and upload the CND.csv (which you can download and save to your own machine from \nhere\n \n- right-click and save as. On the 'UPLOAD A CSV FILE' page in Overview click 'browse' and select the CND.csv. It will give you a preview. There are a number of options here - you can tell Overview which words to ignore, and which words to give added importance to. What words will you select? Make a note in your notebook. Then hit 'upload'.\n\n\nA new page appears, called 'YOUR DOCUMENT SETS'. Click on the one you just uploaded. A file folder tree showing documents of progressively greater similarity will open; on the right hand side will be the list of documents within each box (the box in question will be greyed out when you click on it, so you know where you are). You can search for words in your document, and Overview will tell you where they are; you can tag documents that you find interesting. The Overview system allows you to jump between a distant, macroscopic view and a close, document level view. Jump back and forth, see what you can find. For suggestions about how to use Overview effectively, try \ntheir blog\n. Make notes about what you observe in your notebook. Also, you can export your tagged document set from Overview, so that you could visualize the patterns of tagging in a spreadsheet (for instance). \n\n\nGoing further\n Do you see how you could upload your documents that you collected during Module 2?\n\n\n\n\nexercise 5\n\n\nCorpus Linguistics with AntConc\n\n\nHeather Froelich has put together an excellent step-by-step with using AntConc for exploring textual patterns within, and across, corpora of texts. Work your way through her \ntutorial\n\n\nCan you get our example materials (from the Colonial Newspaper Database) into AntConc? \nThis might help you\n to split the csv into individual txt files. Alternatively, do you have any materials of your own, already collected? Feed them into AntConc. What patterns do you see? What if you compare your materials against other corpora of texts?\n\n\nFYI, \nhere is a collection of corpora that you can explore\n\n\n\n\nexercise 6\n\n\nText Analysis with Voyant\n\n\nIn module 3 if you recall, we worked through how to transform XML using stylesheets. Melodee Beals used a \nstylesheet \n to transform her database into a series of individual txt files. In the exercises above, a transformer was used to make the database into a single CSV file. In this exercise, we are going to use \nVoyant Tools\n to visualize patterns in word use in the database. Voyant can read either a CSV \nor\n text files. The advantage of uploading a folder of text files is that, if the files are in chronological order, Voyant's default visualizations will also be arranged in chronological order and thus we can see change over time.\n\n\nGo to \nhttp://voyant-tools.org\n. Paste the URL to the csv of the CND database: \nhttps://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv\n . \n\n\nNow, open a new browser window, and go here \nhttp://voyant-tools.org/?corpus=colonial-newspapers\nstopList=stop.en.taporware.txt\n\n\nDo you see the difference? In the latter window, the individual articles have been uploaded individually, and thus are treated as individual documents in chronological order.\n\n\nExplore the corpus, comparing terms over time, looking at keywords in context, and using the RezoViz tool to create a graph where people, places, and organizations that appear in the same documents (and across documents) are connected (you can find 'rezoviz' under the cogwheel icon at the top right of the panel). You can embed any of the tools in your blogs by using the 'save' icon and getting the iframe or embed code.  You can apply 'stopwords' by clicking on the cogwheel in any of the different tools, and selecting stopwords. Apply the stopwords globally, and you'll only have to do this once! What patterns do you see? What do different tools highlight? Which ones are useful? What patterns do you see that strike you as interesting? Note this all down. \n\n\nGoing further\n Upload materials you collected in module 2 and explore them.\n\n\n\n\nexercise 7\n\n\nQuick Charts Using RAW\n\n\nA quick chart can be a handy thing to have. Google spreadsheets, Microsoft Excel, and a host of other programs can make excellent charts quickly with their wizard functions. Never hesitate to turn to these. However, they are not always good with non-numeric data. In module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like \nthis\n. Can we do a quick visualization of this information? One useful tool is \nRAW\n. Open that in a new window. Copy the table of data of places mentioned in the Texan correspondence, and paste it into the data input box at the top of the RAW screen. \n\n\nA quick data munge\n\n\nYou should get an error message, to the effect that you need to check 'line 2'. What's gone wrong? RAW has checked the number of values you have in that row, and compared it to the number of columns in row 1 (which contains all the column names). It sees that the two don't match. What we need to do is add a default null value in those cells. So, go to \nGoogle Sheets\n, click the 'go to google sheets' button, and then click on the big green plus sign to start a new sheet. Paste the following into the top-left cell (cell A1):\n\n\n=IMPORTDATA(\"https://raw.githubusercontent.com/hist3907b-winter2015/module4-holes/master/texas.csv\")\n\n\nPretty neat, eh? Now, here's the thing: even though your sheet \nlooks\n like it is filled with information, it's not (at least, as far as the script we are about to run is concerned). That is to say, the sheet itself only has one cell of data, and that one cell is grabbing info from elsewhere on the web and dynamically filling the sheet. The script we're going to run works only on static values (more or less). \n\n\nSo, place your cursor in cell B1. On a Mac, hit \nshift+cmnd+downarrow\n. On a Windows machine, hit \nshift+ctrl+downarrow\n. Then on Mac \nshit+cmnd+rightarrow\n, on Windows \nshitf+crtl+rightarrow\n. Then copy all of that data (\ncmnd+c\n or \nctrl+c\n). Then, under 'Edit' select 'paste special' -\n 'paste VALUES only'.\n\n\nThe formula you put in cell A1 now says \n#REF!\n. You can delete this now. This mucking about is necessary so that the add on script we are about to run will work.\n\n\nWe now need to fill those empty values. In the tool bar, click \nadd ons\n -\n \nget add ons\n. Search for \nblanks\n. You want to add \nBlank Detector\n.\n\n\nNow, click somewhere in your data. On Mac, hit \ncmnd+a\n. On Windows, hit \nctrl+a\n. This highlights all of your data. Click \nAdd ons\n -\n \nblank detector\n -\n \ndetect cells\n. A dialogue panel will open on the right hand side of your screen. Click the button beside \nset value\n and type in \nnull\n. Hit \nrun\n. All of the blank cells will fill with the word \nnull\n. Delete column A (which formerly had record numbers, but is now just filled with the word \nnull\n. We don't need it). \nIf you get the error, run exceeded maximum time\n just hit the run button again. This script might take a few minutes.\n\n\nYou can now copy and paste your table of data into the data input box in RAW, and you should get the green thumbs up saying x records have been successfully parsed!\n\n\nPlaying with RAW\n\n\nRAW takes your data, and depending on your choices, passes it into chart templates built on the d3.js code library. D3.js is a powerful library for making all sorts of charts (including interactive ones). If this sort of thing interests you, you can follow the tutorials in \nElijah Meeks' excellent new book\n.\n\n\nWith your data pasted in, you can now experiment with a number of different visualizations that are all built on the d3.js code library.  Try the \u2018alluvial\u2019 diagram.  Pick place1 and place2 as your dimensions - you click and drag the green boxes under 'map your data' into the 'steps' box. Leave the 'size' box empty. Under 'customize your visualization' you can click inside the 'width' box to make the diagram wider and more legible.\n\n\nDoes anything jump out? Try place3 and place 4. Try place1, place2, place3, and place4 in a single alluvial diagram. When we look at the original letters, we see that the writer often identified the town in which he was writing, and the town of the addressee. Why choose the third and fourth places? Perhaps it makes sense, for a given research question, to assume that with the pleasantries out of the way the writers will discuss the places important to their message. Experiment! This is one of the joys of working with data, experimenting to see how you can deform your materials to see them in a new light.\n\n\nYou can export your visualization under the 'download' box at the bottom of the RAW page - your choices are as a simple raster image (png), a vector image (svg) or a data representation (json).\n\n\n\n\nexercise 8\n\n\nSimple Mapping and Georectifying\n\n\nIn this exercise, you will find a historical map online, upload a copy to a mapwarper service, georectify it, and then display the map online, via a hosted service like CartoDB, and also through a map you will build yourself using leaflet.js. Finally, we will also convert csv to geojson using http://togeojson.com/, and we'll map that as a github gist. We'll also grab a geojson file hosted on github gist and import it into cartodb.\n\n\nGeorectifying\n\n\nGeorectifying is the process of taking an image (whether it is of a historical map, chart, airphoto, or whatever) and manipulating its geometry so that it matches a geographic projection. Think of it like this: you take your handdrawn map, and use pushpins to pin down known locations on your map to a globe. As you pin, your image stretches and warps. Traditionally, this has not been an easy thing to do, if you are new to GIS. In recent years, the curve has flattened significantly. In this exercise, we'll grab an image, upload it to the Harvard Library MapWarper service, and then export it as a tileset which can be used in other mapping programs.\n\n\n\n\nGet a historical map. I like the Fire Insurance plans from the \nGatineau Valley Historical Society\n; I'm sure you can find others to suit your interests.\n\n\nRight-click, save as.... grab a copy. Save it somewhere handy.\n\n\nGo to \nHarvard World MapWarp\n and sign up for an account. Then login.\n\n\nGo to the upload screen: \n \n\n\nFill in as much of the metadata as you can. Then select your map from your computer, and upload it.\n\n\nOn the next page, click 'rectify'. \n \n\n\nPan and zoom both maps until you're sure you're looking at the same area in both. Double click in a map, select the pencil icon, and click on a point (location) you are sure you can match in the other window. Then click on the other map window, select the pencil, and then click on the same point. The 'add control point' button below and between both maps will light up. Click on this to confirm that this is a control point you want. Do this at least three times; the more times you can do it, the better the map warp.\n\n\nHaving selected your control points, click on 'warp image'.\n\n\nYou can now click on the 'export' panel, and get the URL for your georectified image in a few different formats. If you clicked on the KML option, a google map window will open \nlike so\n. For many webmapping applications, the Tiles (Google/OSM scheme): Tiles Based URL is what you want. You'll get a URL like this: \nhttp://warp.worldmap.harvard.edu/maps/tile/4152/z/x/y.png\n   Save that info. You'll need it later.\n\n\n\n\nYou have now georectified a map. Let's use that map as a base layer in \nPalladio\n\n\nWe need some place data for Palladio. Here's what I'm using \n \n \n Note how I've formatted this data. I'll be copying and pasting it into Palladio. (For more on how to input geographic data into Palladio, see \nthis tutorial\n). Basically, you want something like this:\n\n\n\n\n\n\n\n\n\n\nPlace\n\n\nCoordinates\n\n\n\n\n\n\n\n\n\n\n\n\nMexico\n\n\n23.634501,-102.552784\n\n\n\n\n\n\n\n\nCalifornia\n\n\n36.778261,-119.4179324\n\n\n\n\n\n\n\n\nBrazos\n\n\n32.661389,-98.121667\n\n\n\n\n\n\n\n\netc: that is, a tab between 'place' and 'coordinates' in the first line, a tab between 'mexico' and the latitude, and a comma between latitude and logitude. \n\n\n\n\nGo to \nPalladio\n. Hit 'start' then 'upload spreadsheet or csv'. In the box, paste in your data. \nYou can progress to the next step without having any real data: just paste or type something in - see the video below.\n Obviously, you won't have any points on your map, but if you were having trouble with that step, this allows you to bypass it to continue on with this tutorial.\n\n\nClick on 'map'. Under 'places', select 'coordinates'. Then, click 'add new layer'. In the popup, beside 'Choose one of Palladio default layers or create a new one.', select 'custom'. This is where you're going to paste it that tiles based URL from the map warper. Paste it in, but \nreplace\n the \n/z/x/y\n part with \n{z}/{x}/{y}\n. Click add.\n\n\n\n\nHere is a video walk through; places where you might have got into trouble include getting past the initial data entry box on Palladio, and finding where exactly to past in your georectified map url.\n\n\n\n\n\nCongratulations! You've georectified a map, and used it as a base layer for a visualization of some point data. Here are some \nnotes on using a georectified map with the CartoDB service\n.\n\n\n\n\n\n\nexercise 9\n\n\nText Analysis in R\n\n\nI would suggest, before you try this, that you look at the walkthrough for exercise 3, and that you become familiar with R. Then, you can try \nthis tutorial\n, starting at page 3. On that page, the author tells you to create a folder called /corpus/text, and to fill it with text files you'd like to analyse. So why not grab some of the materials you collected in module 2? The problem is, where is this folder supposed to go? In R studio, find out where your working director is by typing\n\n\ngetwd()\n\n\nin the console. Then, you can create the /corpus/text folder \n subfolder at that location. Alternatively, you can set the working directory to wherever you like like so:\n\n\nsetwd(\"C://my-working-folder//\")\n on a pc, or \nsetwd(\"~/my-working-folder/\")\n on a mac.\n\n\nThen, to get going, you'd need\n\n\ninstall.packages(\"tm\")\n\n\nlibrary(tm)\n \n\n\nYou can then work through the entire pdf, or jump ahead to page 37 to see what the completed script would look like (here's \nmy version using the CND again\n. Makes notes of what you find. Google any error messages you find to try to figure out a solution.\n\n\nexercise 10 QGIS\n\n\nQGIS\n\n\nThere are many excellent tutorials around concerning how to get started with GIS. Our own library, in the \nMADGIC centre\n has tremendous resources and I would encourage you to speak with the map librarians before embarking on any \nserious\n mapping projects. In the short term, the historian \nFred Gibbs\n has an excellent series on using the open source GIS platform \nQGIS\n to make and map historical data.\n\n\nFor this exercise, I would recommend you try Gibbs' first tutorial,\n\n\n'Making a map with QGIS'\n\n\n...and then, try georectifying a historical map and adding it to your GIS:\n\n\n'Using historica maps with qgis'\n\n\nGoing Further\n\n\nThere are many tutorials at \nThe Programming Historian\n that are appropriate here. Try some under the 'data manipulation' or 'distant reading' headings.\n\n\nIf you're into social media as a data source, you might \ntry Twarc\n.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-4/Exercises/#module-4-exercises", 
            "text": "There are  many  different tools and approaches  you could use to visualize your data, both as a preliminary pass to spot the holes and also for more formal analysis. In which case, for this module, I would like you to select two of these exercises which seem most germane for your own research project. You are welcome to work through more of them, of course, but I want the exercises to move your own research forward. Some of these I wrote; some are adapted from  The Macroscope ; others are adapted or used holus-bolus from scholars like  Miriam Posner ,  Fred Gibbs , and  Heather Froehlich  (and I'm grateful that they shared their materials!)  nb  If you start working with the R exercises below, I would suggest you read the introductory bits from Lincoln Mullen's book-in-progress,  DH Methods in R , especially the 'setup' part under 'getting started' (pay attention to the bit on installing packages and dependencies). If you spot any exercises in Mullen's book that seem relevant to your project, you may do those as an alternative to the ones here.  Alternatively , go to  Swirl  and  learn the basics of R within R  (It's an interactive tutorial. Try it out.)     Texts  Networks  Maps  Charts      Topic Modeling Tool  Network analysis and visualization  Simple mapping   georectifying  Quick charts using RAW    Topic Modeling in R  Converting 2-mode to 1-mode  QGIS (tutorials by Fred Gibbs)     Text Analysis with OverviewProject  Graphing the Net  Geoparsing with Python     Corpus Linguistics with AntConc  Choose your own adventure  Palladio with Posner     Text Analysis with Voyant       Text Analysis in R", 
            "title": "Module 4 Exercises"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-1", 
            "text": "Network Visualization  In exercise 1, you will transform your Texan Correspondence data into a network, which you will then visualize with Gephi. The detailed instructions are  here . I would recommend that you also take a long look at Scott Weingart's series,  Networks Demystified . Finally,  heed our warning .", 
            "title": "Exercise 1"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-2", 
            "text": "Topic Modeling Tool  In exercise 2, you will use the 'Topic Modeling Tool' to create a simple topic model and a webpage that allows you to browse the results.    Download the  tool . (The site for the tool is  https://code.google.com/p/topic-modeling-tool/ .   Make sure you have the Colonial Newspaper Database handy on your machine. (You can grab my copy from  here ).  Double-click on the file you downloaded in step 1. This will open a java-based graphical user interface with one of the most common topic-modeling approaches, 'Latent Dirichlet Allocation'.  Set the input to be the Colonial Newspaper Database.  Set the output to be somewhere neat and tidy on your computer.  Set the number of topics you'd like to model.  Click 'train topics' to run the algorithm.  When it finishes, go to the folder you selected for output, and find the file 'all_topics.html' in the 'output_html' folder. Click on that, and you now have a browser-based way of navigating your topics and documents. In the output_csv folder created, you will find the same information as csv, which you could then input into a spreadsheet for other kinds of visualizations (which we'll talk about in class.)   Make a note in your open notebook about your process and your observations. How does reading this material in this way change/challenge/or focus your understanding of the material?  Going Further  Remember when we did the Canadiana API and WGET exercises in Module 2? Somewhere on your machine you have a collection of those materials. Now, you can load those materials into the Topic Modeling Tool if you have all of the txt files in a single folder. In the case of the slavery documents, that was something like 7500 items. That's a lot of drag-and-drop. You can 'flatten' the folder structure so that all of the documents in your subfolders are put into a single folder. If you are on a Mac,  try these instructions . On a PC,  try this one  (there are scripts you can use, but for the time being this is probably simplest).  Then, you can point your topic modeling tool at your flattened folder, and  boom  you have a topic model fitted to your collection.", 
            "title": "Exercise 2"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-3", 
            "text": "Topic Modeling in R  Exercise 2 was quite a simple way to do topic modeling. There are more powerful ways, and one of these uses a program called R Studio, which is an interface for the R statistical programming language. R is a powerful language for exploring, visualizing, and manipulating all kinds of data, including textual. It is however not the most intutive of environments to work in. In which case,  RStudio  is what we need. Download the free version and install it on your machine. *Note also that you need to have R  downloaded   installed first ! Then, go to  http://tryr.codeschool.com/  and work your way through some of that tutorial. This tutorial mimics working right in the R console. Remember working in git bash or the terminal in Module 3? It's somewhat similar to that, but just for R. A handy pdf that explains a bit more about working within the R Studio enivornment can be had  here . In essence, you put your code in the script window, execute each line of it, and the output appears in the console. Or in the image plots window.  This handout will guide you around the interface .  In this exercise, we're going to grab the Colonial Newspaper Database from my github page, do some exploratory visualizations, and then create a topic model whose output can then be visualized further in other platforms (including as a network in Gephi). The walkthrough  can be found here . Each gray block is something to copy-and-paste into your script window in R Studio. Then, put the cursor at the start of the first line, and hit ctrl+enter to get RStudio to execute each line. In the walkthrough, when you get to another gray block, just copy and paste it into your script window after the earlier block. Work your way through the walkthrough. The walkthrough gives you an indication of what the output should look like as you move through it. (The walkthrough was written inside R, and then turned into HTML using an R package called 'Knittr'. You can see that this has implications for open research! For reference,  here's the original Rmd (R markdown) file that generated the walkthrough .)  By the way: when you run this line:  topic.model$train(1000)  your console will fill up with data as it iterates 1000 times over the entire corpus, fitting a topic model to it. This is as it should be!  In this way, you'll build up an entire script for topic modeling materials you find on the web. You can then save your script and upload it to your open notebook. In the future, you'd be able to make just a few changes here and there in order to grab and explore different data.  Make a note in your open notebook about your process and your observations.  Going further  If you wanted to use that script on the materials you collected in module 2, you would have to tell R to load up those materials from a directory, rather than by reading a csv file. Take a look at  my script for topic modeling the Ferguson Grand Jury documents , especially this line:  documents  - mallet.read.dir(\"originaldocs/1000chunks/\")  You feed it the path to your documents. If you are on a windows machine, the path would look a bit different, for instance:   \"C:\\\\research\\\\originaldocs\\\\1000chunks\\\\\"", 
            "title": "exercise 3"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-4", 
            "text": "Text Analysis with Overview  In exercise 4, we're going to look at the Colonial Newspaper Database again, but this time using a tool called 'Overview'. Overview uses a different approach that the topic models we've been discussing. In essence, it looks at word frequencies and their distributions within a document, and within a corpus, to organize the documents into folders of progressively similar word use.   You can download Overview to run on your own machine, but for our purposes, the hosted version at  https://www.overviewdocs.com/  is sufficient. Go to that page, watch the video, create an account, and then log in. (More help about how Overview works  may be found on their blog , including helpful videos.)  Once you're inside, click 'import from a CSV file', and upload the CND.csv (which you can download and save to your own machine from  here   - right-click and save as. On the 'UPLOAD A CSV FILE' page in Overview click 'browse' and select the CND.csv. It will give you a preview. There are a number of options here - you can tell Overview which words to ignore, and which words to give added importance to. What words will you select? Make a note in your notebook. Then hit 'upload'.  A new page appears, called 'YOUR DOCUMENT SETS'. Click on the one you just uploaded. A file folder tree showing documents of progressively greater similarity will open; on the right hand side will be the list of documents within each box (the box in question will be greyed out when you click on it, so you know where you are). You can search for words in your document, and Overview will tell you where they are; you can tag documents that you find interesting. The Overview system allows you to jump between a distant, macroscopic view and a close, document level view. Jump back and forth, see what you can find. For suggestions about how to use Overview effectively, try  their blog . Make notes about what you observe in your notebook. Also, you can export your tagged document set from Overview, so that you could visualize the patterns of tagging in a spreadsheet (for instance).   Going further  Do you see how you could upload your documents that you collected during Module 2?", 
            "title": "exercise 4"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-5", 
            "text": "Corpus Linguistics with AntConc  Heather Froelich has put together an excellent step-by-step with using AntConc for exploring textual patterns within, and across, corpora of texts. Work your way through her  tutorial  Can you get our example materials (from the Colonial Newspaper Database) into AntConc?  This might help you  to split the csv into individual txt files. Alternatively, do you have any materials of your own, already collected? Feed them into AntConc. What patterns do you see? What if you compare your materials against other corpora of texts?  FYI,  here is a collection of corpora that you can explore", 
            "title": "exercise 5"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-6", 
            "text": "Text Analysis with Voyant  In module 3 if you recall, we worked through how to transform XML using stylesheets. Melodee Beals used a  stylesheet   to transform her database into a series of individual txt files. In the exercises above, a transformer was used to make the database into a single CSV file. In this exercise, we are going to use  Voyant Tools  to visualize patterns in word use in the database. Voyant can read either a CSV  or  text files. The advantage of uploading a folder of text files is that, if the files are in chronological order, Voyant's default visualizations will also be arranged in chronological order and thus we can see change over time.  Go to  http://voyant-tools.org . Paste the URL to the csv of the CND database:  https://raw.githubusercontent.com/shawngraham/exercise/gh-pages/CND.csv  .   Now, open a new browser window, and go here  http://voyant-tools.org/?corpus=colonial-newspapers stopList=stop.en.taporware.txt  Do you see the difference? In the latter window, the individual articles have been uploaded individually, and thus are treated as individual documents in chronological order.  Explore the corpus, comparing terms over time, looking at keywords in context, and using the RezoViz tool to create a graph where people, places, and organizations that appear in the same documents (and across documents) are connected (you can find 'rezoviz' under the cogwheel icon at the top right of the panel). You can embed any of the tools in your blogs by using the 'save' icon and getting the iframe or embed code.  You can apply 'stopwords' by clicking on the cogwheel in any of the different tools, and selecting stopwords. Apply the stopwords globally, and you'll only have to do this once! What patterns do you see? What do different tools highlight? Which ones are useful? What patterns do you see that strike you as interesting? Note this all down.   Going further  Upload materials you collected in module 2 and explore them.", 
            "title": "exercise 6"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-7", 
            "text": "Quick Charts Using RAW  A quick chart can be a handy thing to have. Google spreadsheets, Microsoft Excel, and a host of other programs can make excellent charts quickly with their wizard functions. Never hesitate to turn to these. However, they are not always good with non-numeric data. In module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like  this . Can we do a quick visualization of this information? One useful tool is  RAW . Open that in a new window. Copy the table of data of places mentioned in the Texan correspondence, and paste it into the data input box at the top of the RAW screen.   A quick data munge  You should get an error message, to the effect that you need to check 'line 2'. What's gone wrong? RAW has checked the number of values you have in that row, and compared it to the number of columns in row 1 (which contains all the column names). It sees that the two don't match. What we need to do is add a default null value in those cells. So, go to  Google Sheets , click the 'go to google sheets' button, and then click on the big green plus sign to start a new sheet. Paste the following into the top-left cell (cell A1):  =IMPORTDATA(\"https://raw.githubusercontent.com/hist3907b-winter2015/module4-holes/master/texas.csv\")  Pretty neat, eh? Now, here's the thing: even though your sheet  looks  like it is filled with information, it's not (at least, as far as the script we are about to run is concerned). That is to say, the sheet itself only has one cell of data, and that one cell is grabbing info from elsewhere on the web and dynamically filling the sheet. The script we're going to run works only on static values (more or less).   So, place your cursor in cell B1. On a Mac, hit  shift+cmnd+downarrow . On a Windows machine, hit  shift+ctrl+downarrow . Then on Mac  shit+cmnd+rightarrow , on Windows  shitf+crtl+rightarrow . Then copy all of that data ( cmnd+c  or  ctrl+c ). Then, under 'Edit' select 'paste special' -  'paste VALUES only'.  The formula you put in cell A1 now says  #REF! . You can delete this now. This mucking about is necessary so that the add on script we are about to run will work.  We now need to fill those empty values. In the tool bar, click  add ons  -   get add ons . Search for  blanks . You want to add  Blank Detector .  Now, click somewhere in your data. On Mac, hit  cmnd+a . On Windows, hit  ctrl+a . This highlights all of your data. Click  Add ons  -   blank detector  -   detect cells . A dialogue panel will open on the right hand side of your screen. Click the button beside  set value  and type in  null . Hit  run . All of the blank cells will fill with the word  null . Delete column A (which formerly had record numbers, but is now just filled with the word  null . We don't need it).  If you get the error, run exceeded maximum time  just hit the run button again. This script might take a few minutes.  You can now copy and paste your table of data into the data input box in RAW, and you should get the green thumbs up saying x records have been successfully parsed!  Playing with RAW  RAW takes your data, and depending on your choices, passes it into chart templates built on the d3.js code library. D3.js is a powerful library for making all sorts of charts (including interactive ones). If this sort of thing interests you, you can follow the tutorials in  Elijah Meeks' excellent new book .  With your data pasted in, you can now experiment with a number of different visualizations that are all built on the d3.js code library.  Try the \u2018alluvial\u2019 diagram.  Pick place1 and place2 as your dimensions - you click and drag the green boxes under 'map your data' into the 'steps' box. Leave the 'size' box empty. Under 'customize your visualization' you can click inside the 'width' box to make the diagram wider and more legible.  Does anything jump out? Try place3 and place 4. Try place1, place2, place3, and place4 in a single alluvial diagram. When we look at the original letters, we see that the writer often identified the town in which he was writing, and the town of the addressee. Why choose the third and fourth places? Perhaps it makes sense, for a given research question, to assume that with the pleasantries out of the way the writers will discuss the places important to their message. Experiment! This is one of the joys of working with data, experimenting to see how you can deform your materials to see them in a new light.  You can export your visualization under the 'download' box at the bottom of the RAW page - your choices are as a simple raster image (png), a vector image (svg) or a data representation (json).", 
            "title": "exercise 7"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-8", 
            "text": "Simple Mapping and Georectifying  In this exercise, you will find a historical map online, upload a copy to a mapwarper service, georectify it, and then display the map online, via a hosted service like CartoDB, and also through a map you will build yourself using leaflet.js. Finally, we will also convert csv to geojson using http://togeojson.com/, and we'll map that as a github gist. We'll also grab a geojson file hosted on github gist and import it into cartodb.  Georectifying  Georectifying is the process of taking an image (whether it is of a historical map, chart, airphoto, or whatever) and manipulating its geometry so that it matches a geographic projection. Think of it like this: you take your handdrawn map, and use pushpins to pin down known locations on your map to a globe. As you pin, your image stretches and warps. Traditionally, this has not been an easy thing to do, if you are new to GIS. In recent years, the curve has flattened significantly. In this exercise, we'll grab an image, upload it to the Harvard Library MapWarper service, and then export it as a tileset which can be used in other mapping programs.   Get a historical map. I like the Fire Insurance plans from the  Gatineau Valley Historical Society ; I'm sure you can find others to suit your interests.  Right-click, save as.... grab a copy. Save it somewhere handy.  Go to  Harvard World MapWarp  and sign up for an account. Then login.  Go to the upload screen:     Fill in as much of the metadata as you can. Then select your map from your computer, and upload it.  On the next page, click 'rectify'.     Pan and zoom both maps until you're sure you're looking at the same area in both. Double click in a map, select the pencil icon, and click on a point (location) you are sure you can match in the other window. Then click on the other map window, select the pencil, and then click on the same point. The 'add control point' button below and between both maps will light up. Click on this to confirm that this is a control point you want. Do this at least three times; the more times you can do it, the better the map warp.  Having selected your control points, click on 'warp image'.  You can now click on the 'export' panel, and get the URL for your georectified image in a few different formats. If you clicked on the KML option, a google map window will open  like so . For many webmapping applications, the Tiles (Google/OSM scheme): Tiles Based URL is what you want. You'll get a URL like this:  http://warp.worldmap.harvard.edu/maps/tile/4152/z/x/y.png    Save that info. You'll need it later.   You have now georectified a map. Let's use that map as a base layer in  Palladio  We need some place data for Palladio. Here's what I'm using       Note how I've formatted this data. I'll be copying and pasting it into Palladio. (For more on how to input geographic data into Palladio, see  this tutorial ). Basically, you want something like this:      Place  Coordinates       Mexico  23.634501,-102.552784     California  36.778261,-119.4179324     Brazos  32.661389,-98.121667     etc: that is, a tab between 'place' and 'coordinates' in the first line, a tab between 'mexico' and the latitude, and a comma between latitude and logitude.    Go to  Palladio . Hit 'start' then 'upload spreadsheet or csv'. In the box, paste in your data.  You can progress to the next step without having any real data: just paste or type something in - see the video below.  Obviously, you won't have any points on your map, but if you were having trouble with that step, this allows you to bypass it to continue on with this tutorial.  Click on 'map'. Under 'places', select 'coordinates'. Then, click 'add new layer'. In the popup, beside 'Choose one of Palladio default layers or create a new one.', select 'custom'. This is where you're going to paste it that tiles based URL from the map warper. Paste it in, but  replace  the  /z/x/y  part with  {z}/{x}/{y} . Click add.   Here is a video walk through; places where you might have got into trouble include getting past the initial data entry box on Palladio, and finding where exactly to past in your georectified map url.   Congratulations! You've georectified a map, and used it as a base layer for a visualization of some point data. Here are some  notes on using a georectified map with the CartoDB service .", 
            "title": "exercise 8"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-9", 
            "text": "Text Analysis in R  I would suggest, before you try this, that you look at the walkthrough for exercise 3, and that you become familiar with R. Then, you can try  this tutorial , starting at page 3. On that page, the author tells you to create a folder called /corpus/text, and to fill it with text files you'd like to analyse. So why not grab some of the materials you collected in module 2? The problem is, where is this folder supposed to go? In R studio, find out where your working director is by typing  getwd()  in the console. Then, you can create the /corpus/text folder   subfolder at that location. Alternatively, you can set the working directory to wherever you like like so:  setwd(\"C://my-working-folder//\")  on a pc, or  setwd(\"~/my-working-folder/\")  on a mac.  Then, to get going, you'd need  install.packages(\"tm\")  library(tm)    You can then work through the entire pdf, or jump ahead to page 37 to see what the completed script would look like (here's  my version using the CND again . Makes notes of what you find. Google any error messages you find to try to figure out a solution.", 
            "title": "exercise 9"
        }, 
        {
            "location": "/module-4/Exercises/#exercise-10-qgis", 
            "text": "QGIS  There are many excellent tutorials around concerning how to get started with GIS. Our own library, in the  MADGIC centre  has tremendous resources and I would encourage you to speak with the map librarians before embarking on any  serious  mapping projects. In the short term, the historian  Fred Gibbs  has an excellent series on using the open source GIS platform  QGIS  to make and map historical data.  For this exercise, I would recommend you try Gibbs' first tutorial,  'Making a map with QGIS'  ...and then, try georectifying a historical map and adding it to your GIS:  'Using historica maps with qgis'", 
            "title": "exercise 10 QGIS"
        }, 
        {
            "location": "/module-4/Exercises/#going-further", 
            "text": "There are many tutorials at  The Programming Historian  that are appropriate here. Try some under the 'data manipulation' or 'distant reading' headings.  If you're into social media as a data source, you might  try Twarc .", 
            "title": "Going Further"
        }, 
        {
            "location": "/module-5/Humanities Visualization/", 
            "text": "Humanities Visualization\n\n\nIn this module, we will be exploring the nuts and bolts of visualization. However, we will also be thinking about what it means to visualize 'data' from a \nhumanities\n perspective. Following Drucker, we're going to imagine what it means to think about our 'data' not as things received (ie, empirically observed) but rather as 'capta', as things taken, transformed.\n\n\nIt means visualizing and dealing with the intepretive process that got us to this point. What's more, we need to be aware of 'screen essentialism' and how it might be blinkering us to the possibilities of what humanities visualization could be. Finally, we need to be aware of the ways our digital 'templates' that we use reproduce ways of thinking and being that are antithetical to humanities' perspectives.\n\n\nTo begin with, I'd like you to read:\n\n\n\n\nDrucker, J. \"Humanities approaches to graphical display\". \nDHQ\n 2011.5 \nhttp://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html\n\n\nWilliams, G. \"Disability, Universal Design, and the Digital Humanities\" \nDebates in the Digital Humanities\n 2012. \nhttp://dhdebates.gc.cuny.edu/debates/text/44\n\n\nOwens, T. \"Discovery and Justification Are Different: Notes on Science-ing the Humanities\" \nhttp://www.trevorowens.org/2012/11/discovery-and-justification-are-different-notes-on-sciencing-the-humanities/\n\n\nOwens, T. \"Defining Data for Humanists: Text, Artifact, Information, or Evidence?\" \nJournal of Digital Humanities\n 2011 1.1. \nhttp://journalofdigitalhumanities.org/1-1/defining-data-for-humanists-by-trevor-owens/\n\n\nWatters, Audrey. \n\"Men (Still) Explain Technology to Me: Gender and Education Technology\" \nHackeducation\n\n\n\n\nI also have a \nnumber of pieces of my own archaeological work\n that I think provide examples of how humanistic visualization can be a driver of interpretation and understanding. For instance, one thing I am currently working on is the possibility for \nsound\n \nto be a better representation of humanistic data\n. Oh, and by the way: maps are great, but sometimes, maps of ideas are even better; check out this \nlandscape of Last.fm Folksonomy\n. (If you have any facility with Python, you might like \nthis library that allows you to generate similar self-organizing maps\n).", 
            "title": "Communicating your Findings"
        }, 
        {
            "location": "/module-5/Humanities Visualization/#humanities-visualization", 
            "text": "In this module, we will be exploring the nuts and bolts of visualization. However, we will also be thinking about what it means to visualize 'data' from a  humanities  perspective. Following Drucker, we're going to imagine what it means to think about our 'data' not as things received (ie, empirically observed) but rather as 'capta', as things taken, transformed.  It means visualizing and dealing with the intepretive process that got us to this point. What's more, we need to be aware of 'screen essentialism' and how it might be blinkering us to the possibilities of what humanities visualization could be. Finally, we need to be aware of the ways our digital 'templates' that we use reproduce ways of thinking and being that are antithetical to humanities' perspectives.  To begin with, I'd like you to read:   Drucker, J. \"Humanities approaches to graphical display\".  DHQ  2011.5  http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html  Williams, G. \"Disability, Universal Design, and the Digital Humanities\"  Debates in the Digital Humanities  2012.  http://dhdebates.gc.cuny.edu/debates/text/44  Owens, T. \"Discovery and Justification Are Different: Notes on Science-ing the Humanities\"  http://www.trevorowens.org/2012/11/discovery-and-justification-are-different-notes-on-sciencing-the-humanities/  Owens, T. \"Defining Data for Humanists: Text, Artifact, Information, or Evidence?\"  Journal of Digital Humanities  2011 1.1.  http://journalofdigitalhumanities.org/1-1/defining-data-for-humanists-by-trevor-owens/  Watters, Audrey.  \"Men (Still) Explain Technology to Me: Gender and Education Technology\"  Hackeducation   I also have a  number of pieces of my own archaeological work  that I think provide examples of how humanistic visualization can be a driver of interpretation and understanding. For instance, one thing I am currently working on is the possibility for  sound   to be a better representation of humanistic data . Oh, and by the way: maps are great, but sometimes, maps of ideas are even better; check out this  landscape of Last.fm Folksonomy . (If you have any facility with Python, you might like  this library that allows you to generate similar self-organizing maps ).", 
            "title": "Humanities Visualization"
        }, 
        {
            "location": "/module-5/Exercises/", 
            "text": "Exercises\n\n\nThe exercises in this module are about colour, layout, and manipulating graphics - but not necessarily in that order. All students must complete the 'Sprucing up a PDF in Inkscape' exercise, as well as the 'Typography', 'Colour', and 'Layout' exercises. Everything after that is optional (including the option to not do them).\n\n\nFinally: while we haven't explored the possibilities here, I do not want to leave you thinking that all digital history work is necessarily text based. To that end, if you are looking for a challenge or for exposure to something rather different, I suggest you try my \nseries of tutorials on augmented reality, games, and 3d models for history and archaeology\n.\n\n\nSprucing up a PDF in Inkscape\n | \nTypography\n | \nColour\n | \nLayout\n | \nMore\n\n\n\n\nUkraine fist fight\n\n\nBy the way: Raster v Vector\n\n\nThe first thing to know is that graphic images come in two distince flavours - raster, and vector.\n\n\nRaster images are composed of pixels, such that if you zoom into them, they become a pointilist blur (if you remember \nFerris Beuller's Day Off\n, there's a scene where Cameron stares and stares at a painting, falling into its individual points of colour...). Vector images on the other hand are described by mathematical functions, of lines and arcs and areas. No matter how deep you delve into these kinds of images, the image is always sharp - because the zoom is just another function of the mathematics describing the image.\n\n\nRasters: blocks of colours\n\n\nVectors: descriptions of points and arcs\n\n\nSprucing up a pdf in Inkscape\n\n\nSome of the tools that we used in Module 4 give visual output as raster images, others as vectors. Sometimes, we would like to tweak these outputs to make them more visually appealling, or clearer, or more useful. A program like MS Paint is only useful for dealing with raster images (and then, only in certain kinds of cases). We need a program that can deal with both, and also, lets us edit the image by treating each edit we do as a mostly-transparent layer on top of the image. That way, we can add, rearrange, hide or reveal, our edits to create a composite image. A free program that is immensely useful in this regard is \nInkscape\n. Inkscape is also quite useful in that we can open a pdf file in it, break the visual elements of the pdf into individual layers, and then rearrange/touch up/fix them up to make them more esthetically appealing.\n\n\nIn this first exercise, we will take the plot we generated in Module 4's exercise on topic modeling in R where we made a bar chart showing the number of articles by year. In R we exported that plot as a PDF. In Inkscape, we can import that pdf and 'explode' it so that we can manipulate its parts individually. We are going to take the simple bar chart and make it more legible, more visually appealing, for incorporation on a webpage.\n\n\n\n\nDownload and install \nInkscape\n \nnb Mac\n the installation instructions are a bit more complicated for Mac. Pay attention and follow closely!\n\n\nDownload the pdf we generated in R \npublication-year.pdf\n \n\n\nOpen that pdf. It's a pretty plain graphic. Right away there are at least two things we could do to make it more visually appealling. We could change the orientation of the characters in the y-axis to make them more legible. We can highlight bars of interest. And we could apply a colour scheme more generally that would make our graphic legible to folks with colour-blindness (see the 'going further' section at bottom).\n\n\nStart Inkscape. Click File \n Import \n and then navigate to where you save the 'publication-year.pdf'. Click Ok when you've got it selected. In the next pop-up, just accept the default settings and click 'ok'. Your Inkscape window should now look like this:\n\n\nThe pdf is now a layer in the new image you are creating in Inkscape. You can save this drawing, \nwith its information about the layers and what is in each one\n by clicking File \n Save As. (\nhere's my version\n). 'SVG' stands for 'scalable vector graphic'. (SVG is a kind of text file that describes the complete geometry of your illustration).\n\n\nDo you see the bounding box around the plot? If you grab any of those handles (the double-arrow things), you can make it bigger or smaller on your sheet. We can't edit any of the other elements yet - we can't change the colour of the bars, or the fonts of the text. We have to tell Inkscape to 'explode' these elements into their own 'objects'. In the menu bar at top, got to Object \n Ungroup. There are now a series of overlapping bounding boxes around each object.\n\n\nZoom in (by pressing the + sign on your keyboard) so that you're looking at the numbers of the y-axis. We're going to rotate these by 90 degrees to make them more legible. Select the arrow icon from the toolbar on the left side of the screen.\n\n\nClick on the '50'. You'll get a bounding box around it. Click Object \n Rotate 90 CW. The 50 is now rotated! Do the same for the other numbers. Save. (If you double-click on the number, you might trigger the 'text edit' function. If you do that, no problem - you can change the font, change the number... although if you did that, it'd be a bit dishonest, right? Click on the arrow pointer icon in the toolbar again to get out of the text-editing function).\n\n\nLet's imagine, for whatever reason, that you wanted to change one of the bars to a different colour, to highlight its importance to your argument. With the arrow icon, click on one of the bars so that you get the bounding box around it. Then, click on one of the colours from the palette at the bottom. Boom! You've got a newly colourized bar. Save.\n\n\nAdd a legend. Write it so that the important message you want your viewer to get is immediately clear. Choose a font from Inkscape's included fonts that \nsupports\n your message.\n\n\nTo export your image so that you can use it in a website or paper, click Edit \n Select All in All Layers. Every element of your image will now have a bounding box around it.\n\n\nGo to File \n Export Bitmap. Never mind the options in the popup; just hit 'Export'. Inkscape will automatically assign your drawing a name with .png; here's \nmine\n. Remember, if you want to edit this image again later, hit the 'save' button to save it as an svg. The svg will preserve all your layer information, while the png file is the visual representation (the png is in fact a raster graphic). Most browsers can handle svg files, so you could use that in your website; programs like Word seem to be able to handle raster graphics better than they do svg. You might want to experiment. In any event, every journal has different requirements for image formats. You would export your image to whatever those specifications are.\n\n\n\n\nGoing further\n\n\nIn this \ntutorial, you will learn how to load a custom colour palette\n. Why might you want to do that? You should be designing your work so that it is as universally accessible as possible. Many folks are colour-blind. Use \nColor-brewer\n to generate a colour-blind safe palette. Then look for the 'GIMP and Inkscape - GIMP color palette for this scheme.' Click on that link, and you'll get a text file with the scheme you generated. Use that scheme to alter the colours on your plot.\n\n\n\n\nTypography\n\n\nTypographic plays an important role in visual communication. It can do everything from reduce eyestrain (do a search for 'easiest fonts to read on screen') to communicate power and authority. Is it any wonder that strong bold capitals come to be known as 'Roman'? But first: \nare you a comic sans criminal?\n\n\nIn this exercise, \n\n\n\n\n\n\nI want you to read and understand the section on \nfont choices from the Owl at Purdue\n. \n\n\n\n\n\n\nThen, play some rounds of \nTypeconnection\n. Pay attention to why - or why not - various pairings work. \n\n\n\n\n\n\nThen, I want to consider the document you will be preparing for me that accounts for your learning in this course - the document where you choose your best exercises from the modules and explain how your approach to history, data, the digital, etc, has changed over this course. What typographic pair would work best for you? \n\n\n\n\n\n\nFinally, you'll make a webpage that uses those fonts and explains why they work.\n\n\n\n\n\n\nThe first part of this exercise then is to find a pair and to understand why they work best for you - go read the materials above, and once you're done with the Typeconnection site, go to \nGoogle Fonts\n and search for a pair that are \nsuitable for your purpose\n. When you find a font you like, click the 'add to collection' button. Then, at the bottom of the screen, you'll see a link for 'use'. Click on this - google will ask you if you want to use any of the variants of the font. Tick off accordingly. Then, do you see the embed code that google provides, and the code to integrate the font into your CSS (stylesheet)? Leave this window open - we're going to use it in a moment.\n\n\n\n\nMake a new repository for this exercise. In your repository, click the button beside 'branch'. In the search box that opens, type in \ngh-pages\n. This will create a version of your repository that can be served as a website.\n\n\nYou're now in the gh-pages branch. Click on the + sign beside the repository name. This will create a new file in your gh-branch of your repository. Call it \nmyfontchoice.html\n \n- the .html is important to specify; otherwise your browser will not know how to render your page.\n\n\nYou now need to put some html in there. I've written a simple webpage that will use two fonts from Google Fonts, and then applies the font to my text depending on whether or not it is a header, which you specify like this: \nh1\n this is a header in between header tags \n/h1\n or a paragraph, which you specify like this: \np\nblah blah blah a paragraph of text blah blah blah\n/p\n. My webpage is \nhere\n; right-click the link and open in a new tab. Copy the html into your new html document. Commit your changes (ie, save!).\n\n\nLet's see what we've got. To see the website version of your gh-pages branch, you go to \nyourusername\n.github.io/\nreponame\n/myfontchoice.html\n \n- ie, the final part of the url is the name of the document in your repo. Do that now. You should see a simple webpage, with two very distinctive fonts.\n\n\nNow: let's slide your font choices into the html. Go to your html page in your gh-pages repo (ie, not the \ngithub.io\n version, but the \ngithub.com/\nyourusername\n/\nrepo\n/myfontchoice.html\n version. Hit the edit button. Look closely at line 6. Do you see my two fonts? Do you see the pipe character between them? That tells google you want \nboth\n fonts. Go look at the google fonts page again to grab the exact name for your fonts (ie, uppercase letters make a difference!) and paste them into line 5 appropriately.\n\n\nLines 8 and 14 specify which font to use for headers, and which font to use for body text. Change appropriately. \n\n\nChange my silly text for a paragraph that explains what the fonts are, and why they work for this purpose. Commit your changes.\n\n\nGo to the \ngithub.io\n version of your repository (if you forget the address, you can find it under the 'settings' button on your normal repository page when you're in the gh-pages branch). Reload the page a couple of times to clear the older version you've already looked at and to replace it with your version. Ta da! Not only have you thought carefully about typography and fonts, you've also learned how to serve a webpage from a Github repository.\n\n\n\n\nHint\n You could use this as the basics of your submission for assessment, for your exercises. Build a webpage, link to your evidence, embed your images... For basic html \nhere's a really good guide to keep around\n. \n\n\n\n\nColour\n\n\nThere's a lot of bumpf out there on the 'pyschology of colour'. Google it to see what I mean (\nhere's an example\n). A lot of what you see here isn't so much psychology as it is associations (and indeed, western associations, at that). Associations are important of course; you don't want to undermine your message by making some unfortunate connections. \n\n\n\n\nIn this exercise, I want you to take the webpage you developed in the previous exercise, and make two versions of it: one, where the colours support the broader message of the page (a prototype for your exercises assessment piece, remember?), and the other where the colours \nundermine\n that broader message. Explain, in both cases, how your colour choices enhance/detract.\n\n\n\n\n\n\nalternatively, you can make a one page slide in powerpoint doing the same thing.\n\n\n\n\nResources\n\n\nHere's a graphic \n a movie to help with the theoretical side of things:\n\n\n\n\nUnderstanding the rules of color, Lynda.com\n\n\nTo learn how to style your webpage appropriately, you can \nfollow this tutorial on CSS from codeacademy.com\n.\n\n\nColours in Cultures:\n\n\n\n\n\n\nLayout\n\n\n'Layout' can mean different things in different contexts. A general overview on issues in layout is covered by \n'What makes design great, Lynda.com\n and \n'Exploring principles of layout and composition'\n. This \nslideshare\n gives you a sense of things to watch out for as well.\n\n\nFor academic posters in particular, consider \nthese suggestions from the APA\n.\n\n\nIn essence, good layout makes your argument legible, intuitive, and reinforces the rhetorical points you are trying to make. You should take into account 'principles of universal design' - consider these issues with \npowerpoint\n and with \nwebsites\n (although some of the technical solutions proposed in those two documents are a bit out of date, the general principles hold!) \n\n\nIn this exercise, you will design a new poster OR modify an existing poster. You can use either Inkscape or Powerpoint. \n\n\n\n\n\n\nInkscape: download one of the scientific poster templates from \nUgo Sangiorgi\n (These are developed from \nthis blog post\n; note his discussion on design). Open it in Inkscape. Make notes in your open notebook: \nfrom the point of view of layout\n, what elements of the design work? What aren't working? How would you repurpose this poster to fit the requirements of the assessment exercise (\nremember, details here\n)?  \n Here's \nhelp with the basics of Inkscape\n. Modify the poster, and upload the svg or pdf or png version to your repository.\n\n\n\n\n\n\nPPT: there's a lot of information out there on making posters with powerpoint. \nRead parts 1,2, and 3 here\n and then consider \nthe advice here\n. Once you've read and digested, pick a poster from \nPimp my poster\n that strikes use. Make notes in your open notebook: \nfrom the point of view of layout\n what elements of the design work? What aren't working? How would you repurpose this posert to fit the requirements ot he assessment exercies (\nremember, details here\n)? \n Grab a template from \nhere\n, and use it to prototype a poster that works. Upload the poster as a pdf to your repository.\n\n\n\n\n\n\nIf you want to explore layout in the context of webpage creation, I would point you to the the roster of lessons at \nCodeacademy\n. Same instructions: find an existing site that you will consider from the point of view of what works, what doesn't, and use that reflection to guide the construction of your own.\n\n\n\n\n\n\nMore\n\n\nMore resources, tutorials, and things to explore.\n\n\nAccessibility and Design\n\n\nWhile most of this applies to websites, think also about how the general principles apply to other ways \n means of telling our data stories.\n\n\n\n\nHow People with Disabilities Use the Web\n\n\nWeb Content Accessibility and Mobile Web\n\n\nAccessibility in HTML5\n\n\nWeb Accessibility Evaluation Tool\n\n\nConsidering the User Perspective\n\n\nConstructing a POUR Website\n Percievable, Operable, Understandable, Robust. Applies much wider than design of websites.\n\n\nChecking Microsoft Office-generated documents for accessibility\n\n\n\n\nInfographics / Storytelling\n\n\nInfographics\n\n\n\n\nThe Difference between Infographics and Visualizations\n\n\nDesign hints for infographics\n\n\nPiktochart\n\n\nInfogr.am\n\n\nInfoactive.co\n (has a free option, but might not allow exports. You'd have to check).\n\n\nEasel.ly\n\n\n\n\nStorytelling\n\n\n\n\nCreatavist\n\n\nMedium\n\n\nCowbird\n\n\nExposure\n\n\nTimeline.js\n\n\nStorymap\n\n\n\n\n\n\nManyLines\n\n\nManylines\n is an application that allows you to create narratives from network graphs. In essence, you upload a network file in .gexf format (which you can export from Gephi) and it renders it on the screen. There are some layout options to make the graph more intelligible. Then, you take a series of snapshots zoomed in on the graph in different places, and add text to describe what it is that's important about these networks. The app puts a Prezi-like wrapper around your snapshots, and the whole can then be embedded in a website or be used as a standalone website. \nHere's my first attempt.\n \n\n\nYou can also embed nearly anything in the narrative panels - youtube videos, \ntimeline.js\n, as long as you know how to correctly format an \niframe\n.  \n\n\nTo give this a try, why not use the Texan Correspondence network we generated in earlier modules? Export it in .gexf format from gephi, import to ManyLines, and go! The interface is fairly straightforward. Just follow the prompts. \n\n\nCaveat Utilitor\n I don't know how long anything made with ManyLines will live on their website. But, knowing what you know about wget and other tools, do you see how you could archive a copy on your own machine? ManyLines is available on \ngithub\n so you can certainly use it locally.\n\n\nLeaflet\n\n\nMaps can be created through a variety of services (\ntilemill\n, \ncartodb\n, \nmapbox\n, for instance). These can then be embedded in your webpages or documents. Often, that's enough. But sometimes, you'd like to take control, and keep all the data, all the map, under your own name, in your own webspace. Here is a gentle introduction to using \nleaflet.js\n to make, style, and serve your maps. \nHere is a template\n for mapping with leaflet, drawing all of your point data and ancillary information from a csv file.\n\n\nOh, and here's a list of background maps you can use: \nhttp://leaflet-extras.github.io/leaflet-providers/preview/index.html\n.\n\n\nDesigning Maps with Processing and Illustrator\n\n\nNicolas Felton is a renowned designer; this 90 minute workshop is worth watching and working through. \nSkillshare\n\n\nCaveat: I do not use processing or illustrator, but processing is free and Inkscape can do many of the things that Illustrator does.", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-5/Exercises/#exercises", 
            "text": "The exercises in this module are about colour, layout, and manipulating graphics - but not necessarily in that order. All students must complete the 'Sprucing up a PDF in Inkscape' exercise, as well as the 'Typography', 'Colour', and 'Layout' exercises. Everything after that is optional (including the option to not do them).  Finally: while we haven't explored the possibilities here, I do not want to leave you thinking that all digital history work is necessarily text based. To that end, if you are looking for a challenge or for exposure to something rather different, I suggest you try my  series of tutorials on augmented reality, games, and 3d models for history and archaeology .  Sprucing up a PDF in Inkscape  |  Typography  |  Colour  |  Layout  |  More   Ukraine fist fight  By the way: Raster v Vector  The first thing to know is that graphic images come in two distince flavours - raster, and vector.  Raster images are composed of pixels, such that if you zoom into them, they become a pointilist blur (if you remember  Ferris Beuller's Day Off , there's a scene where Cameron stares and stares at a painting, falling into its individual points of colour...). Vector images on the other hand are described by mathematical functions, of lines and arcs and areas. No matter how deep you delve into these kinds of images, the image is always sharp - because the zoom is just another function of the mathematics describing the image.  Rasters: blocks of colours  Vectors: descriptions of points and arcs", 
            "title": "Exercises"
        }, 
        {
            "location": "/module-5/Exercises/#sprucing-up-a-pdf-in-inkscape", 
            "text": "Some of the tools that we used in Module 4 give visual output as raster images, others as vectors. Sometimes, we would like to tweak these outputs to make them more visually appealling, or clearer, or more useful. A program like MS Paint is only useful for dealing with raster images (and then, only in certain kinds of cases). We need a program that can deal with both, and also, lets us edit the image by treating each edit we do as a mostly-transparent layer on top of the image. That way, we can add, rearrange, hide or reveal, our edits to create a composite image. A free program that is immensely useful in this regard is  Inkscape . Inkscape is also quite useful in that we can open a pdf file in it, break the visual elements of the pdf into individual layers, and then rearrange/touch up/fix them up to make them more esthetically appealing.  In this first exercise, we will take the plot we generated in Module 4's exercise on topic modeling in R where we made a bar chart showing the number of articles by year. In R we exported that plot as a PDF. In Inkscape, we can import that pdf and 'explode' it so that we can manipulate its parts individually. We are going to take the simple bar chart and make it more legible, more visually appealing, for incorporation on a webpage.   Download and install  Inkscape   nb Mac  the installation instructions are a bit more complicated for Mac. Pay attention and follow closely!  Download the pdf we generated in R  publication-year.pdf    Open that pdf. It's a pretty plain graphic. Right away there are at least two things we could do to make it more visually appealling. We could change the orientation of the characters in the y-axis to make them more legible. We can highlight bars of interest. And we could apply a colour scheme more generally that would make our graphic legible to folks with colour-blindness (see the 'going further' section at bottom).  Start Inkscape. Click File   Import   and then navigate to where you save the 'publication-year.pdf'. Click Ok when you've got it selected. In the next pop-up, just accept the default settings and click 'ok'. Your Inkscape window should now look like this:  The pdf is now a layer in the new image you are creating in Inkscape. You can save this drawing,  with its information about the layers and what is in each one  by clicking File   Save As. ( here's my version ). 'SVG' stands for 'scalable vector graphic'. (SVG is a kind of text file that describes the complete geometry of your illustration).  Do you see the bounding box around the plot? If you grab any of those handles (the double-arrow things), you can make it bigger or smaller on your sheet. We can't edit any of the other elements yet - we can't change the colour of the bars, or the fonts of the text. We have to tell Inkscape to 'explode' these elements into their own 'objects'. In the menu bar at top, got to Object   Ungroup. There are now a series of overlapping bounding boxes around each object.  Zoom in (by pressing the + sign on your keyboard) so that you're looking at the numbers of the y-axis. We're going to rotate these by 90 degrees to make them more legible. Select the arrow icon from the toolbar on the left side of the screen.  Click on the '50'. You'll get a bounding box around it. Click Object   Rotate 90 CW. The 50 is now rotated! Do the same for the other numbers. Save. (If you double-click on the number, you might trigger the 'text edit' function. If you do that, no problem - you can change the font, change the number... although if you did that, it'd be a bit dishonest, right? Click on the arrow pointer icon in the toolbar again to get out of the text-editing function).  Let's imagine, for whatever reason, that you wanted to change one of the bars to a different colour, to highlight its importance to your argument. With the arrow icon, click on one of the bars so that you get the bounding box around it. Then, click on one of the colours from the palette at the bottom. Boom! You've got a newly colourized bar. Save.  Add a legend. Write it so that the important message you want your viewer to get is immediately clear. Choose a font from Inkscape's included fonts that  supports  your message.  To export your image so that you can use it in a website or paper, click Edit   Select All in All Layers. Every element of your image will now have a bounding box around it.  Go to File   Export Bitmap. Never mind the options in the popup; just hit 'Export'. Inkscape will automatically assign your drawing a name with .png; here's  mine . Remember, if you want to edit this image again later, hit the 'save' button to save it as an svg. The svg will preserve all your layer information, while the png file is the visual representation (the png is in fact a raster graphic). Most browsers can handle svg files, so you could use that in your website; programs like Word seem to be able to handle raster graphics better than they do svg. You might want to experiment. In any event, every journal has different requirements for image formats. You would export your image to whatever those specifications are.   Going further  In this  tutorial, you will learn how to load a custom colour palette . Why might you want to do that? You should be designing your work so that it is as universally accessible as possible. Many folks are colour-blind. Use  Color-brewer  to generate a colour-blind safe palette. Then look for the 'GIMP and Inkscape - GIMP color palette for this scheme.' Click on that link, and you'll get a text file with the scheme you generated. Use that scheme to alter the colours on your plot.", 
            "title": "Sprucing up a pdf in Inkscape"
        }, 
        {
            "location": "/module-5/Exercises/#typography", 
            "text": "Typographic plays an important role in visual communication. It can do everything from reduce eyestrain (do a search for 'easiest fonts to read on screen') to communicate power and authority. Is it any wonder that strong bold capitals come to be known as 'Roman'? But first:  are you a comic sans criminal?  In this exercise,     I want you to read and understand the section on  font choices from the Owl at Purdue .     Then, play some rounds of  Typeconnection . Pay attention to why - or why not - various pairings work.     Then, I want to consider the document you will be preparing for me that accounts for your learning in this course - the document where you choose your best exercises from the modules and explain how your approach to history, data, the digital, etc, has changed over this course. What typographic pair would work best for you?     Finally, you'll make a webpage that uses those fonts and explains why they work.    The first part of this exercise then is to find a pair and to understand why they work best for you - go read the materials above, and once you're done with the Typeconnection site, go to  Google Fonts  and search for a pair that are  suitable for your purpose . When you find a font you like, click the 'add to collection' button. Then, at the bottom of the screen, you'll see a link for 'use'. Click on this - google will ask you if you want to use any of the variants of the font. Tick off accordingly. Then, do you see the embed code that google provides, and the code to integrate the font into your CSS (stylesheet)? Leave this window open - we're going to use it in a moment.   Make a new repository for this exercise. In your repository, click the button beside 'branch'. In the search box that opens, type in  gh-pages . This will create a version of your repository that can be served as a website.  You're now in the gh-pages branch. Click on the + sign beside the repository name. This will create a new file in your gh-branch of your repository. Call it  myfontchoice.html   - the .html is important to specify; otherwise your browser will not know how to render your page.  You now need to put some html in there. I've written a simple webpage that will use two fonts from Google Fonts, and then applies the font to my text depending on whether or not it is a header, which you specify like this:  h1  this is a header in between header tags  /h1  or a paragraph, which you specify like this:  p blah blah blah a paragraph of text blah blah blah /p . My webpage is  here ; right-click the link and open in a new tab. Copy the html into your new html document. Commit your changes (ie, save!).  Let's see what we've got. To see the website version of your gh-pages branch, you go to  yourusername .github.io/ reponame /myfontchoice.html   - ie, the final part of the url is the name of the document in your repo. Do that now. You should see a simple webpage, with two very distinctive fonts.  Now: let's slide your font choices into the html. Go to your html page in your gh-pages repo (ie, not the  github.io  version, but the  github.com/ yourusername / repo /myfontchoice.html  version. Hit the edit button. Look closely at line 6. Do you see my two fonts? Do you see the pipe character between them? That tells google you want  both  fonts. Go look at the google fonts page again to grab the exact name for your fonts (ie, uppercase letters make a difference!) and paste them into line 5 appropriately.  Lines 8 and 14 specify which font to use for headers, and which font to use for body text. Change appropriately.   Change my silly text for a paragraph that explains what the fonts are, and why they work for this purpose. Commit your changes.  Go to the  github.io  version of your repository (if you forget the address, you can find it under the 'settings' button on your normal repository page when you're in the gh-pages branch). Reload the page a couple of times to clear the older version you've already looked at and to replace it with your version. Ta da! Not only have you thought carefully about typography and fonts, you've also learned how to serve a webpage from a Github repository.   Hint  You could use this as the basics of your submission for assessment, for your exercises. Build a webpage, link to your evidence, embed your images... For basic html  here's a really good guide to keep around .", 
            "title": "Typography"
        }, 
        {
            "location": "/module-5/Exercises/#colour", 
            "text": "There's a lot of bumpf out there on the 'pyschology of colour'. Google it to see what I mean ( here's an example ). A lot of what you see here isn't so much psychology as it is associations (and indeed, western associations, at that). Associations are important of course; you don't want to undermine your message by making some unfortunate connections.    In this exercise, I want you to take the webpage you developed in the previous exercise, and make two versions of it: one, where the colours support the broader message of the page (a prototype for your exercises assessment piece, remember?), and the other where the colours  undermine  that broader message. Explain, in both cases, how your colour choices enhance/detract.    alternatively, you can make a one page slide in powerpoint doing the same thing.   Resources  Here's a graphic   a movie to help with the theoretical side of things:   Understanding the rules of color, Lynda.com  To learn how to style your webpage appropriately, you can  follow this tutorial on CSS from codeacademy.com .  Colours in Cultures:", 
            "title": "Colour"
        }, 
        {
            "location": "/module-5/Exercises/#layout", 
            "text": "'Layout' can mean different things in different contexts. A general overview on issues in layout is covered by  'What makes design great, Lynda.com  and  'Exploring principles of layout and composition' . This  slideshare  gives you a sense of things to watch out for as well.  For academic posters in particular, consider  these suggestions from the APA .  In essence, good layout makes your argument legible, intuitive, and reinforces the rhetorical points you are trying to make. You should take into account 'principles of universal design' - consider these issues with  powerpoint  and with  websites  (although some of the technical solutions proposed in those two documents are a bit out of date, the general principles hold!)   In this exercise, you will design a new poster OR modify an existing poster. You can use either Inkscape or Powerpoint.     Inkscape: download one of the scientific poster templates from  Ugo Sangiorgi  (These are developed from  this blog post ; note his discussion on design). Open it in Inkscape. Make notes in your open notebook:  from the point of view of layout , what elements of the design work? What aren't working? How would you repurpose this poster to fit the requirements of the assessment exercise ( remember, details here )?    Here's  help with the basics of Inkscape . Modify the poster, and upload the svg or pdf or png version to your repository.    PPT: there's a lot of information out there on making posters with powerpoint.  Read parts 1,2, and 3 here  and then consider  the advice here . Once you've read and digested, pick a poster from  Pimp my poster  that strikes use. Make notes in your open notebook:  from the point of view of layout  what elements of the design work? What aren't working? How would you repurpose this posert to fit the requirements ot he assessment exercies ( remember, details here )?   Grab a template from  here , and use it to prototype a poster that works. Upload the poster as a pdf to your repository.    If you want to explore layout in the context of webpage creation, I would point you to the the roster of lessons at  Codeacademy . Same instructions: find an existing site that you will consider from the point of view of what works, what doesn't, and use that reflection to guide the construction of your own.", 
            "title": "Layout"
        }, 
        {
            "location": "/module-5/Exercises/#more", 
            "text": "More resources, tutorials, and things to explore.", 
            "title": "More"
        }, 
        {
            "location": "/module-5/Exercises/#accessibility-and-design", 
            "text": "While most of this applies to websites, think also about how the general principles apply to other ways   means of telling our data stories.   How People with Disabilities Use the Web  Web Content Accessibility and Mobile Web  Accessibility in HTML5  Web Accessibility Evaluation Tool  Considering the User Perspective  Constructing a POUR Website  Percievable, Operable, Understandable, Robust. Applies much wider than design of websites.  Checking Microsoft Office-generated documents for accessibility", 
            "title": "Accessibility and Design"
        }, 
        {
            "location": "/module-5/Exercises/#infographics-storytelling", 
            "text": "Infographics   The Difference between Infographics and Visualizations  Design hints for infographics  Piktochart  Infogr.am  Infoactive.co  (has a free option, but might not allow exports. You'd have to check).  Easel.ly   Storytelling   Creatavist  Medium  Cowbird  Exposure  Timeline.js  Storymap", 
            "title": "Infographics / Storytelling"
        }, 
        {
            "location": "/module-5/Exercises/#manylines", 
            "text": "Manylines  is an application that allows you to create narratives from network graphs. In essence, you upload a network file in .gexf format (which you can export from Gephi) and it renders it on the screen. There are some layout options to make the graph more intelligible. Then, you take a series of snapshots zoomed in on the graph in different places, and add text to describe what it is that's important about these networks. The app puts a Prezi-like wrapper around your snapshots, and the whole can then be embedded in a website or be used as a standalone website.  Here's my first attempt.    You can also embed nearly anything in the narrative panels - youtube videos,  timeline.js , as long as you know how to correctly format an  iframe .    To give this a try, why not use the Texan Correspondence network we generated in earlier modules? Export it in .gexf format from gephi, import to ManyLines, and go! The interface is fairly straightforward. Just follow the prompts.   Caveat Utilitor  I don't know how long anything made with ManyLines will live on their website. But, knowing what you know about wget and other tools, do you see how you could archive a copy on your own machine? ManyLines is available on  github  so you can certainly use it locally.", 
            "title": "ManyLines"
        }, 
        {
            "location": "/module-5/Exercises/#leaflet", 
            "text": "Maps can be created through a variety of services ( tilemill ,  cartodb ,  mapbox , for instance). These can then be embedded in your webpages or documents. Often, that's enough. But sometimes, you'd like to take control, and keep all the data, all the map, under your own name, in your own webspace. Here is a gentle introduction to using  leaflet.js  to make, style, and serve your maps.  Here is a template  for mapping with leaflet, drawing all of your point data and ancillary information from a csv file.  Oh, and here's a list of background maps you can use:  http://leaflet-extras.github.io/leaflet-providers/preview/index.html .", 
            "title": "Leaflet"
        }, 
        {
            "location": "/module-5/Exercises/#designing-maps-with-processing-and-illustrator", 
            "text": "Nicolas Felton is a renowned designer; this 90 minute workshop is worth watching and working through.  Skillshare  Caveat: I do not use processing or illustrator, but processing is free and Inkscape can do many of the things that Illustrator does.", 
            "title": "Designing Maps with Processing and Illustrator"
        }, 
        {
            "location": "/conclusion/conclusion/", 
            "text": "Conclusion\n\n\nThere will be a profound and witty concluding paragraph here, some day. But if you've noticed anything at all, it's that the process of crafting digital history never comes to an end. \nAdam Crymble\n published a piece recently that throws this into high relief - pay attention to his second paragraph in his main diagram!\n\n\nIn the meantime, I do want you to know that you too can craft excellent digital history. There were some great moments in the first iteration of this course, and you will have great moments too: like when Matt forked one of my tutorials and rewrote it for the better, or built a virtual machine. Or when Patrick finally slayed Github! Or when Allison got the Canadiana API to work. Or when Phoebe finally persuaded Inkscape to play nice. Or when Matt conquered TWARC. Or when TEI blew Ryan's mind. Or when Christina forked an Anthropology class project at MSU to repurpose for her project. Or... or... or. We covered a lot of ground.\n\n\nYou will too. \n\n\nA word to the wise\n\n\nIf, in your other courses, you decide to use some of the methods here, I will be most gratified. However - in course work as in life, \nknow\n your audience. Will your prof appreciate this work? Is your prof familiar with the underlying issues - will she know what to look for, the hidden gotchas, the places where things might get, erm, fudged? It is \nyour\n responsibility to make the argument, in your work, why a particular methodological choice is appropriate. It is \nyour\n responsibility to show how these choices are theoretically informed and meaningful. As in all history, you have to make the argument. Never fall into the trap of thinking the method speaks for itself. This is why I compelled you to create the paradata document for your project. As that master historian Yoda once said,\n\n\n\n\nDo or do not. There is no try.\n\n\n\n\nAlright, not exactly appropriate. But you get the gist: digital history requires explicit paradata. Do. There is no try.", 
            "title": "6. Final Thoughts"
        }, 
        {
            "location": "/conclusion/conclusion/#conclusion", 
            "text": "There will be a profound and witty concluding paragraph here, some day. But if you've noticed anything at all, it's that the process of crafting digital history never comes to an end.  Adam Crymble  published a piece recently that throws this into high relief - pay attention to his second paragraph in his main diagram!  In the meantime, I do want you to know that you too can craft excellent digital history. There were some great moments in the first iteration of this course, and you will have great moments too: like when Matt forked one of my tutorials and rewrote it for the better, or built a virtual machine. Or when Patrick finally slayed Github! Or when Allison got the Canadiana API to work. Or when Phoebe finally persuaded Inkscape to play nice. Or when Matt conquered TWARC. Or when TEI blew Ryan's mind. Or when Christina forked an Anthropology class project at MSU to repurpose for her project. Or... or... or. We covered a lot of ground.  You will too.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/conclusion/conclusion/#a-word-to-the-wise", 
            "text": "If, in your other courses, you decide to use some of the methods here, I will be most gratified. However - in course work as in life,  know  your audience. Will your prof appreciate this work? Is your prof familiar with the underlying issues - will she know what to look for, the hidden gotchas, the places where things might get, erm, fudged? It is  your  responsibility to make the argument, in your work, why a particular methodological choice is appropriate. It is  your  responsibility to show how these choices are theoretically informed and meaningful. As in all history, you have to make the argument. Never fall into the trap of thinking the method speaks for itself. This is why I compelled you to create the paradata document for your project. As that master historian Yoda once said,   Do or do not. There is no try.   Alright, not exactly appropriate. But you get the gist: digital history requires explicit paradata. Do. There is no try.", 
            "title": "A word to the wise"
        }, 
        {
            "location": "/supporting materials/tei/", 
            "text": "Close Reading with TEI\n\n\nThis worksheet, and all related files, are released CC-BY. \n\n\nBy M. H. Beals\n; Adapted for HIST3907b by S Graham\n\n\nYou will need the files in \nthis subfolder\n so download that repository as a zip file and unzip it somewhere handy on your machine.\n\n\nThis exercise will explore a historical text and help you create a digital record of your analysis\n\n\nVetting a Website\n\n\nVisit the Recovered Histories Website at \nhttp://www.recoveredhistories.org\n\n\nExamine the site's layout and read its introduction\n\n\n\n\nWhat makes you believe this site is a trustworthy provider of historical texts?\n\n\nWhat makes you believe this site is NOT a trustworthy provider of historical texts? \n\n\n\n\nFinding a Source\n\n\nVisit the site's collections via the 'Browse' function\n\n\nLocate the pamphlet \nNegro Slavery\n by Zachary Macaulay and open it\n\n\nThis is an abolitionist pamphlet regarding the Atlantic slave trade, presenting and examine evidence of how it is run.  When you approach a primary source like this, it is tempting to read through it from beginning to end, to get an overview of its contents, and then 'mine' or 'cherry-pick' good quotations to include in your assessments.  However, we are going to focus on examining a very small part of the text in a very high level of detail.  \n\n\nSetting Up Your Workspace\n\n\nArrange your workspace so that you have the scanned text of the pamphlet easily visible on one side of your screen. Open the 'blanktemplate.txt' file in Sublime Text, Textwrangler or Notepadd++ (or any text editor that understands encoding) and have that on the other side of the screen.\n\n\nThe last lines will be \n\n/body\n/text\n/TEI\n/teiCorpus\n. Everything you write today should be just above \n/body\n.\n\n\nTranscribing Your Page\n\n\nThe first thing you will need is go to the tag \n\n\nbiblScope\n1\n/biblScope\n\n\nand replace the number one (1) with the page number you are transcribing. Which page should you transcribe? Select a page in the document that you find interesting.\n\n\nNext, you will need to \nvery carefully\n transcribe your page of text from the image into your document.  Make sure you do not make any changes to the text, even if you think they author has used poor grammar or misspelled a word.  You do not need to worry about line breaks but should start every new paragraph (or heading) with a \np\n and end every paragraph (or heading) with a \n/p\n.\n\n\nOnce you have completed your transcription, look away from your computer for 30-45 seconds.  Staring into the distance every 10-20 minutes will keep your eyes from straining.  Also, shake out your hands at the wrists, to prevent repetitive stress injuries to your fingers.  \n\n\nEncoding Your Transcription\n\n\nYou are now going to \nencode\n or \nmark-up\n your text.  \n\n\nRe-read your page and highlight / colour the following things:\n\n\n\n\nAny persons mentioned (including any he/she if they refer to a specific person)\n\n\nAny places mentioned\n\n\nAny claims, assertions or arguments made\nNow that you have highlighted these, you are going to put proper code around them.\n\n\n\n\nFor persons, surround your text with these\n\n\npersName key=\"Last, First\" from=\"YYYY\" to=\"YYYY\" role=\"Occupation\" ref=\"http://www.website.com/webpage.html\"\n \n/persName\n\n\n\n\nInside the speech marks for \nkey\n, include the real full name of the person mentioned \n\n\nIn \nfrom\n and \nto\n, include their birth and death years, using ? for unknown years\n\n\nIn \nrole\n, put the occupation, role or 'claim to fame' for this individual.  \n\n\nIn \nref\n, put the URL (link) to the Dictionary of National Biography, Wikipedia or other biography website where you found this information. If there is a \n in your link, you will need to replace this with \namp;\n\n\n\n\nFor places, surround your text with these\n\n\nplaceName key=\"Sheffield, United Kingdom\" ref=\"http://tools.wmflabs.org/geohack/geohack.php?pagename=Sheffield\nparams=53_23_01_N_1_28_01_W_type:city_region:GB\"\"\n \n/placeName\n\n\n\n\nIn \nkey\n, put the city and country with best information you can find for the modern names for this location\n\n\nIn \nref\n, put a link to the relevant coordinates on Wikipedia GeoHack website (http://tools.wmflabs.org/geohack/) \n\nTo obtain this, go to the Wikipedia page for this city and click on the latitude/longitude coordinates for the location. For large areas, such as entire countries or continents, just use the Wikipedia page URL.\n\n\n\n\nFor claims or arguments, surround your text with these\n\n\ninterp key=\"reason\" n=\"citation\" cert=\"high\" ref=\"http://www.website.com/webpage.html\"\n \n/interp\n\n\n\n\nIn \nkey\n, explain why you believe this claim is true or not\n\n\nIn \nn\n, put a full citation to the relevant source\n\n\nIn \ncert\n (short for certainty), put: high, medium, low or unknown\n\n\nIn \nref\n, put the link to the website where you got the information to assess this claim. \n\n\n\n\nWhen you are happy with your work, hit save your work, give it a useful name, make sure it has .xml as the extension, and save it \nand the .xsl file\n to your repository.\n\n\n\n\nAlex Gill has made \nThe Short and Sweet TEI Handout\n which you might want to explore as well. When you embark on encoding documents for your own research, \nhere are some questions to think about\n to help you decide what kinds of tagging you'll need; these \ntemplates from HisTEI\n might be useful (open the whole project with OxygenXML for full functionality, but you can copy those templates in any editor). \n\n\n\n\nViewing Your Encoded Text\n\n\nTo see your encoded text, make sure your .xml and .xsl file are in the same folder. \nOpen either Internet Explorer or Firefox\n. The following will not work in Chrome because it has different security settings.\n\n\nMaking sure both your (page number).xml file and your 000style.xsl file are in the same folder (or both on your desktop), drag the icon for (page number).xml into your Internet Explorer Browser window.\n\n\nIf you now see a colour-coded version of your text, Congratulations! If you hover over the coloured sections, you should see a pop-up with the additional information you entered.\n\n\nIf your text comes up only in black, with no paragraph divisions or headings, or doesn't come up at all, something has gone wrong. Re-open your .xml file and check that you have:\n\n\nPlaced \np\n at the start of every paragraph, including the start of the page\n\n\nPlaced \n/p\n at the end of every paragraph, including the end of the page\n\n\nMade sure all your \npersName\n, \nplaceName\n and \ninterp\n tags are properly enclosed in \ns\n\n\nMade sure you have both an open \n and close \n\\\n tag for each tag you use\n\n\nMade sure you attribute values are fully enclosed in \n\"\"\n.\n\n\nMade sure you have a space between the \n\"\n of one attribute and the start of the next\n\n\nMade sure you do NOT have a space after the \n=\n of an attribute\n\n\nIf your text still does not appear formatted, you may need to remove the text one paragraph at a time, refreshing your browser window, until it appears. This will help you identify which paragraph (or sentence) has the error within it).\n\n\nIf you still don't see your text\n\n\nIf you do not see the colour-coded version of your text, this might not necessarily mean that you've done something wrong\n\n\n\n\nSome browsers will not perform the transformation, for security reasons.\n\n\n\n\nIn which case, here's what we can do. If you are on a Windows machine using Notepad++, go to 'Plugins' \n Plugin Tools. (If you are on Windows but aren't using Notepad++, Sublime and Atom probably have a similar functionality, but you will have to search to figure it out.) Select 'XML Tools' from the list, and install it. You'll probably have to restart the program to complete the plugin installation. Open up the \n1.xml\n file in Notepad ++. Then, under 'plugins'\n'xml tools\" select 'XSL Transformation settings'. In the popup, click on the elipses: \n...\n to open up the file finder, and select the \n000style.xsl\n stylesheet. Click 'transform'. A new tab will open in Notepad++ \nwith a fully-formed html file displaying your data according to the stylesheet.\n Save this, and then open it in a browser!\n\n\n\n\n\n\nYou can also check 'validate' from the XML Tools menu in Notepad++, which will identify errors in your XML. If you're still having errors, a likely culprit might be the way your geographic URLs are encoded. Compare what you've got with what's in the \n1.xml\n reference document. \n\n\n\n\n\n\nAdvanced\n: If you install a \nWAMP\n or \nMAMP\n server, and put your xml and xsl files in the WWW folder, you \nshould\n be able to see the transformation no problem at \nlocalhost\\myxml.xml\n (for example). (You can also use \nPython's built in webserver if you have Python on your machine\n - all Mac users for instance do.)\n\n\n\n\n\n\n\n\nSo here's the CND.xml, transformed into a csv: \nhttp://shawngraham.github.io/exercise/cnd.xml\n . If you 'view page source', you'll see the original XML again! Save-as the page as whatever-you-want.csv and you can do some data mining on it.\n\n\n\n\nMore on transformations\n\n\nI made a file I've called \nSG_transformer.xsl\n. Open that file in your text editor. What tags would it be looking for in the xml file? What might it do to your markup? What line would you change in your XML file to get it to point to this stylesheet? Write all this down in your open notebook. It is a good habit to get into to keep track of your thoughts when looking at ancillary files like this.\n\n\nIf the nature of your project will involve a lot of transcription, you would be well advised to use an XML editor like \nOxygenXML\n, which has a free 1 month trial. The editor makes it easy to maintain \nconsistency\n in your markup, and also, to quickly create stylesheets for whatever purpose you need. There are also a number of utility programs freely available that will convert XML to CSV or other formats. One such may be \nfound here\n. But the best way to transform these XML files is with XSL.", 
            "title": "Text Encoding"
        }, 
        {
            "location": "/supporting materials/tei/#close-reading-with-tei", 
            "text": "This worksheet, and all related files, are released CC-BY.   By M. H. Beals ; Adapted for HIST3907b by S Graham  You will need the files in  this subfolder  so download that repository as a zip file and unzip it somewhere handy on your machine.  This exercise will explore a historical text and help you create a digital record of your analysis  Vetting a Website  Visit the Recovered Histories Website at  http://www.recoveredhistories.org  Examine the site's layout and read its introduction   What makes you believe this site is a trustworthy provider of historical texts?  What makes you believe this site is NOT a trustworthy provider of historical texts?    Finding a Source  Visit the site's collections via the 'Browse' function  Locate the pamphlet  Negro Slavery  by Zachary Macaulay and open it  This is an abolitionist pamphlet regarding the Atlantic slave trade, presenting and examine evidence of how it is run.  When you approach a primary source like this, it is tempting to read through it from beginning to end, to get an overview of its contents, and then 'mine' or 'cherry-pick' good quotations to include in your assessments.  However, we are going to focus on examining a very small part of the text in a very high level of detail.    Setting Up Your Workspace  Arrange your workspace so that you have the scanned text of the pamphlet easily visible on one side of your screen. Open the 'blanktemplate.txt' file in Sublime Text, Textwrangler or Notepadd++ (or any text editor that understands encoding) and have that on the other side of the screen.  The last lines will be  /body /text /TEI /teiCorpus . Everything you write today should be just above  /body .  Transcribing Your Page  The first thing you will need is go to the tag   biblScope 1 /biblScope  and replace the number one (1) with the page number you are transcribing. Which page should you transcribe? Select a page in the document that you find interesting.  Next, you will need to  very carefully  transcribe your page of text from the image into your document.  Make sure you do not make any changes to the text, even if you think they author has used poor grammar or misspelled a word.  You do not need to worry about line breaks but should start every new paragraph (or heading) with a  p  and end every paragraph (or heading) with a  /p .  Once you have completed your transcription, look away from your computer for 30-45 seconds.  Staring into the distance every 10-20 minutes will keep your eyes from straining.  Also, shake out your hands at the wrists, to prevent repetitive stress injuries to your fingers.    Encoding Your Transcription  You are now going to  encode  or  mark-up  your text.    Re-read your page and highlight / colour the following things:   Any persons mentioned (including any he/she if they refer to a specific person)  Any places mentioned  Any claims, assertions or arguments made\nNow that you have highlighted these, you are going to put proper code around them.   For persons, surround your text with these  persName key=\"Last, First\" from=\"YYYY\" to=\"YYYY\" role=\"Occupation\" ref=\"http://www.website.com/webpage.html\"   /persName   Inside the speech marks for  key , include the real full name of the person mentioned   In  from  and  to , include their birth and death years, using ? for unknown years  In  role , put the occupation, role or 'claim to fame' for this individual.    In  ref , put the URL (link) to the Dictionary of National Biography, Wikipedia or other biography website where you found this information. If there is a   in your link, you will need to replace this with  amp;   For places, surround your text with these  placeName key=\"Sheffield, United Kingdom\" ref=\"http://tools.wmflabs.org/geohack/geohack.php?pagename=Sheffield params=53_23_01_N_1_28_01_W_type:city_region:GB\"\"   /placeName   In  key , put the city and country with best information you can find for the modern names for this location  In  ref , put a link to the relevant coordinates on Wikipedia GeoHack website (http://tools.wmflabs.org/geohack/)  To obtain this, go to the Wikipedia page for this city and click on the latitude/longitude coordinates for the location. For large areas, such as entire countries or continents, just use the Wikipedia page URL.   For claims or arguments, surround your text with these  interp key=\"reason\" n=\"citation\" cert=\"high\" ref=\"http://www.website.com/webpage.html\"   /interp   In  key , explain why you believe this claim is true or not  In  n , put a full citation to the relevant source  In  cert  (short for certainty), put: high, medium, low or unknown  In  ref , put the link to the website where you got the information to assess this claim.    When you are happy with your work, hit save your work, give it a useful name, make sure it has .xml as the extension, and save it  and the .xsl file  to your repository.   Alex Gill has made  The Short and Sweet TEI Handout  which you might want to explore as well. When you embark on encoding documents for your own research,  here are some questions to think about  to help you decide what kinds of tagging you'll need; these  templates from HisTEI  might be useful (open the whole project with OxygenXML for full functionality, but you can copy those templates in any editor).    Viewing Your Encoded Text  To see your encoded text, make sure your .xml and .xsl file are in the same folder.  Open either Internet Explorer or Firefox . The following will not work in Chrome because it has different security settings.  Making sure both your (page number).xml file and your 000style.xsl file are in the same folder (or both on your desktop), drag the icon for (page number).xml into your Internet Explorer Browser window.  If you now see a colour-coded version of your text, Congratulations! If you hover over the coloured sections, you should see a pop-up with the additional information you entered.  If your text comes up only in black, with no paragraph divisions or headings, or doesn't come up at all, something has gone wrong. Re-open your .xml file and check that you have:  Placed  p  at the start of every paragraph, including the start of the page  Placed  /p  at the end of every paragraph, including the end of the page  Made sure all your  persName ,  placeName  and  interp  tags are properly enclosed in  s  Made sure you have both an open   and close  \\  tag for each tag you use  Made sure you attribute values are fully enclosed in  \"\" .  Made sure you have a space between the  \"  of one attribute and the start of the next  Made sure you do NOT have a space after the  =  of an attribute  If your text still does not appear formatted, you may need to remove the text one paragraph at a time, refreshing your browser window, until it appears. This will help you identify which paragraph (or sentence) has the error within it).  If you still don't see your text  If you do not see the colour-coded version of your text, this might not necessarily mean that you've done something wrong   Some browsers will not perform the transformation, for security reasons.   In which case, here's what we can do. If you are on a Windows machine using Notepad++, go to 'Plugins'   Plugin Tools. (If you are on Windows but aren't using Notepad++, Sublime and Atom probably have a similar functionality, but you will have to search to figure it out.) Select 'XML Tools' from the list, and install it. You'll probably have to restart the program to complete the plugin installation. Open up the  1.xml  file in Notepad ++. Then, under 'plugins' 'xml tools\" select 'XSL Transformation settings'. In the popup, click on the elipses:  ...  to open up the file finder, and select the  000style.xsl  stylesheet. Click 'transform'. A new tab will open in Notepad++  with a fully-formed html file displaying your data according to the stylesheet.  Save this, and then open it in a browser!    You can also check 'validate' from the XML Tools menu in Notepad++, which will identify errors in your XML. If you're still having errors, a likely culprit might be the way your geographic URLs are encoded. Compare what you've got with what's in the  1.xml  reference document.     Advanced : If you install a  WAMP  or  MAMP  server, and put your xml and xsl files in the WWW folder, you  should  be able to see the transformation no problem at  localhost\\myxml.xml  (for example). (You can also use  Python's built in webserver if you have Python on your machine  - all Mac users for instance do.)     So here's the CND.xml, transformed into a csv:  http://shawngraham.github.io/exercise/cnd.xml  . If you 'view page source', you'll see the original XML again! Save-as the page as whatever-you-want.csv and you can do some data mining on it.   More on transformations  I made a file I've called  SG_transformer.xsl . Open that file in your text editor. What tags would it be looking for in the xml file? What might it do to your markup? What line would you change in your XML file to get it to point to this stylesheet? Write all this down in your open notebook. It is a good habit to get into to keep track of your thoughts when looking at ancillary files like this.  If the nature of your project will involve a lot of transcription, you would be well advised to use an XML editor like  OxygenXML , which has a free 1 month trial. The editor makes it easy to maintain  consistency  in your markup, and also, to quickly create stylesheets for whatever purpose you need. There are also a number of utility programs freely available that will convert XML to CSV or other formats. One such may be  found here . But the best way to transform these XML files is with XSL.", 
            "title": "Close Reading with TEI"
        }, 
        {
            "location": "/supporting materials/ner/", 
            "text": "Using the Stanford NER to tag a corpus\n\n\nIn our regular expressions example, we were able to extract some of the metadata from the document because it was more or less already formatted in such a way that we could write a pattern to find it. Sometimes however clear-cut patterns are not quite as easy to apply. For instance, what if we were interested in the place names that appear in the documents? What if we suspected that the focus of diplomatic activity shifted over time? This is where \u2018named entity recognition\u2019 can be useful. Named entity recognition covers a broad range of techniques, based on machine learning and statistical models of language to laboriously trained classifiers using dictionaries. One of the easiest to use out-of-the-box is the Stanford Named Entity Recognizer.  In essence, we tell it \u2018here is a block of text \u2013 classify!\u2019 It will then process through the text, looking at the structure of your text and matching it against its statistical models of word use to identify person, organization, and locations. One can also expand that classification to extract time, money, percent, and date. \n\n\nGrab the Stanford NER\n\n\nLet us use the NER to extract person, organization, and locations.  First, download the Stanford NER from \nhttp://nlp.stanford.edu/software/CRF-NER.shtml\n and extract it to your machine. Open the location where you extracted the files. \n+ On a Mac, double-click on the one called \u2018ner-gui.command\u2019. (Mac Users: there is also this excellent tutorial from Michelle Moravec \nyou may wish to consult\n.\n+ On PC, double-click on ner-gui.bat. This opens up a new window (using Java) with \u2018Stanford Named Entity Recognizer\u2019 and also a terminal window. \n\n\nDon\u2019t touch the terminal window for now. (\nPC users, hang on a moment\n \u2013 there is a bit more that you need to know before you can use this tool successfully. You will have to use the command line in order to get the output out).\n\n\nRunning the NER via its GUI\n\n\nIn the \u2018Stanford Named Entity Recognizer\u2019 window there is some default text. Click inside this window and delete the text. Then, click on \u2018File,\u2019 then \u2018Open,\u2019 and select your text for the diplomatic correspondence of the Republic of Texas (that you should still have from earlier modules in this course). Since this text file contains a lot of extraneous information in it \u2013 information which we are not currently interested in, including the publishing information and the index table of letters \u2013 you should open the file in a text editor first and delete that information. We want just the letters for this exercise. Save with a new name and then open it using \u2018File \n open\u2019 in the Stanford NER. The file will open within the window. In the Stanford NER window, click on \u2018classifier\u2019 then \u2018load CRF from file\u2019. Navigate to where you unzipped the Stanford NER folder. Click on the \u2018classifier\u2019 folder. There are a number of files here; the ones that you are interested in end with .gz:\n\n\nenglish.all.3class.distsim.crf.ser.gz\n\n\nenglish.all.4class.distsim.crf.ser.gz\n\n\neglish.muc.7class.distsim.crf.ser.gz\n\n\nThese files correspond to these entities to extract:\n\n\n3class:    Location, Person, Organization\n\n\n4class:    Location, Person, Organization, Misc\n\n\n7class:    Time, Location, Organization, Person, Money, Percent, Date\n\n\nSelect the location, person, and organization classifier (ie, 3class) and then press \u2018Run NER.\u2019 At this point, the program will appear to \u2018hang\u2019 \u2013 nothing much will seem to be happening. However, in the background, the program has started to process your text. Depending on the size of your text, this could take anywhere from a few minutes to a few hours. Be patient! Watch the terminal window \u2013 once the program has results for you, these will start to scroll by in the terminal window. In the main program window, once the entire text has processed, the text will appear with colour-coded highlighting showing which words are location words, which ones are persons, which ones are organizations. You have now classified a text. Note: sometimes your computer may run out of memory \u2013 in that case, you\u2019ll see an error referring to \u201cOut of Heap Space\u201d in your terminal window. That\u2019s OK \u2013 just copy and paste a smaller bit of the document, say the first 10,000 lines or so. Then try again.\n\n\nManipulating that data\n\n\nMac users can grab the data and paste it elsewhere; PC users will have to run the NER from the command line to get usable output.\n\n\nMac Users\n\n\nOn a Mac, you can copy and paste the output from the terminal window into your text editor of choice. It will look something like: \n\n\nLOCATION: Texas\n\nPERSON: Moore\n\nORGANIZATION: Suprema\n\n\nAnd so forth. Once you've pasted it into Textwrangler (or whatever text editor you use), you can now use regular expressions to manipulate the text further. More in a moment.\n\n\nPC Users\n\n\nOn a PC, things are not so simple because the command window only shows a small fraction of the complete output \u2013 you cannot copy and paste it all! What we have to do instead is type in a command at the command prompt, rather than using the graphical interface, and then redirect the output into a new text file. \n\n\n\n\nOpen a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).\n\n\nType the following as a single line:\n\n\n\n\njava -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML \n \u201cmy-ner-output.txt\u201d\n\n\n\n\nThe first bit, \njava \u2013mx500m\n says how much memory to use. If you have 1gb of memory available, you can type \njava \u2013mx 1g\n (or 2g, or 3g, etc). The next part of the command calls the NER programme itself.  You can set which classifier to use after the \n\u2013loadClassifier classifiers/\n by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier). At \n\u2013textFile\n you give it the name of your input file (on our machine, called \ntexas-letters.txt\n, and then specify the outputFormat.  The \n character sends the output to a new text file, here called \nmy-ner-output.txt\n.  Hit enter, and a few moments later the programme will tell you something along the lines of\n\n\nCRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second\n\n\nOpen the text file in Notepad++, and you\u2019ll see output like this:\n\n\nIn the name of the \nLOCATION\nRepublic of Texas\n/LOCATION\n, Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I \nPERSON\nSam Houston\n/PERSON\n President thereof send Greeting\n\n\nCongratulations \u2013 you\u2019ve successfully tagged a document using a named entity recognizer!\n\n\nNow you need to do a bit more data-munging before you can do anything useful. \nImagine you wanted to eventually visualize this as a network\n. You will need your regex skills again...", 
            "title": "NER"
        }, 
        {
            "location": "/supporting materials/ner/#using-the-stanford-ner-to-tag-a-corpus", 
            "text": "In our regular expressions example, we were able to extract some of the metadata from the document because it was more or less already formatted in such a way that we could write a pattern to find it. Sometimes however clear-cut patterns are not quite as easy to apply. For instance, what if we were interested in the place names that appear in the documents? What if we suspected that the focus of diplomatic activity shifted over time? This is where \u2018named entity recognition\u2019 can be useful. Named entity recognition covers a broad range of techniques, based on machine learning and statistical models of language to laboriously trained classifiers using dictionaries. One of the easiest to use out-of-the-box is the Stanford Named Entity Recognizer.  In essence, we tell it \u2018here is a block of text \u2013 classify!\u2019 It will then process through the text, looking at the structure of your text and matching it against its statistical models of word use to identify person, organization, and locations. One can also expand that classification to extract time, money, percent, and date.", 
            "title": "Using the Stanford NER to tag a corpus"
        }, 
        {
            "location": "/supporting materials/ner/#grab-the-stanford-ner", 
            "text": "Let us use the NER to extract person, organization, and locations.  First, download the Stanford NER from  http://nlp.stanford.edu/software/CRF-NER.shtml  and extract it to your machine. Open the location where you extracted the files. \n+ On a Mac, double-click on the one called \u2018ner-gui.command\u2019. (Mac Users: there is also this excellent tutorial from Michelle Moravec  you may wish to consult .\n+ On PC, double-click on ner-gui.bat. This opens up a new window (using Java) with \u2018Stanford Named Entity Recognizer\u2019 and also a terminal window.   Don\u2019t touch the terminal window for now. ( PC users, hang on a moment  \u2013 there is a bit more that you need to know before you can use this tool successfully. You will have to use the command line in order to get the output out).", 
            "title": "Grab the Stanford NER"
        }, 
        {
            "location": "/supporting materials/ner/#running-the-ner-via-its-gui", 
            "text": "In the \u2018Stanford Named Entity Recognizer\u2019 window there is some default text. Click inside this window and delete the text. Then, click on \u2018File,\u2019 then \u2018Open,\u2019 and select your text for the diplomatic correspondence of the Republic of Texas (that you should still have from earlier modules in this course). Since this text file contains a lot of extraneous information in it \u2013 information which we are not currently interested in, including the publishing information and the index table of letters \u2013 you should open the file in a text editor first and delete that information. We want just the letters for this exercise. Save with a new name and then open it using \u2018File   open\u2019 in the Stanford NER. The file will open within the window. In the Stanford NER window, click on \u2018classifier\u2019 then \u2018load CRF from file\u2019. Navigate to where you unzipped the Stanford NER folder. Click on the \u2018classifier\u2019 folder. There are a number of files here; the ones that you are interested in end with .gz:  english.all.3class.distsim.crf.ser.gz  english.all.4class.distsim.crf.ser.gz  eglish.muc.7class.distsim.crf.ser.gz  These files correspond to these entities to extract:  3class:    Location, Person, Organization  4class:    Location, Person, Organization, Misc  7class:    Time, Location, Organization, Person, Money, Percent, Date  Select the location, person, and organization classifier (ie, 3class) and then press \u2018Run NER.\u2019 At this point, the program will appear to \u2018hang\u2019 \u2013 nothing much will seem to be happening. However, in the background, the program has started to process your text. Depending on the size of your text, this could take anywhere from a few minutes to a few hours. Be patient! Watch the terminal window \u2013 once the program has results for you, these will start to scroll by in the terminal window. In the main program window, once the entire text has processed, the text will appear with colour-coded highlighting showing which words are location words, which ones are persons, which ones are organizations. You have now classified a text. Note: sometimes your computer may run out of memory \u2013 in that case, you\u2019ll see an error referring to \u201cOut of Heap Space\u201d in your terminal window. That\u2019s OK \u2013 just copy and paste a smaller bit of the document, say the first 10,000 lines or so. Then try again.", 
            "title": "Running the NER via its GUI"
        }, 
        {
            "location": "/supporting materials/ner/#manipulating-that-data", 
            "text": "Mac users can grab the data and paste it elsewhere; PC users will have to run the NER from the command line to get usable output.  Mac Users  On a Mac, you can copy and paste the output from the terminal window into your text editor of choice. It will look something like:   LOCATION: Texas \nPERSON: Moore \nORGANIZATION: Suprema  And so forth. Once you've pasted it into Textwrangler (or whatever text editor you use), you can now use regular expressions to manipulate the text further. More in a moment.  PC Users  On a PC, things are not so simple because the command window only shows a small fraction of the complete output \u2013 you cannot copy and paste it all! What we have to do instead is type in a command at the command prompt, rather than using the graphical interface, and then redirect the output into a new text file.    Open a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).  Type the following as a single line:   java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML   \u201cmy-ner-output.txt\u201d  The first bit,  java \u2013mx500m  says how much memory to use. If you have 1gb of memory available, you can type  java \u2013mx 1g  (or 2g, or 3g, etc). The next part of the command calls the NER programme itself.  You can set which classifier to use after the  \u2013loadClassifier classifiers/  by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier). At  \u2013textFile  you give it the name of your input file (on our machine, called  texas-letters.txt , and then specify the outputFormat.  The   character sends the output to a new text file, here called  my-ner-output.txt .  Hit enter, and a few moments later the programme will tell you something along the lines of  CRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second  Open the text file in Notepad++, and you\u2019ll see output like this:  In the name of the  LOCATION Republic of Texas /LOCATION , Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I  PERSON Sam Houston /PERSON  President thereof send Greeting  Congratulations \u2013 you\u2019ve successfully tagged a document using a named entity recognizer!  Now you need to do a bit more data-munging before you can do anything useful.  Imagine you wanted to eventually visualize this as a network . You will need your regex skills again...", 
            "title": "Manipulating that data"
        }, 
        {
            "location": "/supporting materials/open-refine/", 
            "text": "An Introduction to Open Refine\n\n\nThis text was adopted from the first drafts of The Macroscope\n\n\n(An alternative Open Refine exercise is offered by \nThomas Padilla\n and you may wish to give it a try instead/too.) \n\n\nInstall Open Refine\n\n\n\n\nOpenRefine (formerly Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; extending it with web services; and linking it to databases like Freebase.\n\n\n\n\nIn this exercise, we are going to use a tool that originated with Google. Since 2012, it has been open-sourced and freely available on the net. Using it takes a bit of getting used to, however. Visit the \nOpen Refine\n home page and watch the three videos. Then, \ndownload it to your machine\n\n\nFollow the installation instructions. Start Open Refine by double clicking on its icon. This will open a new browser window, pointing to http://127.0.0.1:3333. This location is your own computer, so even though it looks like it\u2019s running on the internet, it isn\u2019t. The \u20183333\u2019 is a \u2018port\u2019, meaning that Open Refine is running much like a server, serving up a webpage via that port to the browser.\n\n\nStart Cleaning our Texan Correspondence.\n\n\n\n\n\n\nStart a new project by clicking on the \u2018create project\u2019 tab on the left hand side of the screen. Click on \u2018choose files\u2019 and select the CSV file created in exercise 2. This will give you a preview of your data. Name your project in the box on the top right side (eg., 'm3-exercise3' or similar) and then click \u2018create project\u2019. It may take a few minutes.\n\n\n\n\n\n\nOnce your project has started, one of the columns that should be visible in your data is \u2018sender\u2019. Click on the arrow to the left of \"Sender\" in OpenRefine and select Facet-\nText Facet. Do the same with the arrow next to \"Recipient\". A box will appear on the left side of the browser showing all 189 names listed as senders in the spreadsheet. The spreadsheet itself is nearly a thousand rows, so immediately we see that, in this correspondence collection, some names are used multiple times. You may also have noticed that many of the names suffered from errors in the text scan (OCR or Optical Character Recognition errors), rendering some identical names from the book as similar, but not the same, in the spreadsheet. For example the recipient \"Juan de Dios Cafiedo\" is occasionally listed as \"Juan de Dios CaAedo\". Any subsequent analysis will need these errors to be cleared up, and OpenRefine will help fix them.\n\n\n\n\n\n\nWithin the \"Sender\" facet box on the left-hand side, click on the button labeled \"Cluster\". This feature presents various automatic ways of merging values that appear to be the same.   Play with the values in the drop-down boxes and notice how the number of clusters found change depending on which method is used. Because the methods are slightly different, each will present different matches that may or may not be useful. If you see two values which should be merged, e.g. \"Ashbel Smith\" and \". Ashbel Smith\", check the box to the right in the 'Merge' column and click the 'Merge Selected \n Re-Cluster' button below.\n\n\n\n\n\n\nGo through the various cluster methods one-at-a-time, including changing number values, and merge the values which appear to be the same. \"Juan de Dios CaAedo\" clearly should be merged with \"Juan de Dios Cafiedo\", however \"Correspondent in Tampico\" probably should not be merged with \"Correspondent at Vera Cruz.\" Since we are not experts, we will have to use our best judgement in these cases - or get cracking on some more research to help us make the call. By the end, you should have reduced the number of unique Senders from 189 to around 150. Repeat these steps with Recipients, reducing unique Recipients from 192 to about 160. To finish the automatic cleaning of the data, click the arrow next to \"Sender\" and select 'Edit Cells-\nCommon transforms-\nTrim leading and trailing whitespace'. Repeat for \"Recipient\". The resulting spreadsheet will not be perfect, but it will be much easier to clean by hand than it would have been before taking this step. Click on \u2018export\u2019 at the top right of the window to get your data back out as a .csv file.\n\n\n\n\n\n\nNow what?\n\n\nThe text you've just cleaned could now be loaded into something like \nPalladio\n or \nGephi\n for network analysis! However, every network analysis program has its own idiosyncracies. Gephi, for instance, can import lists of relationships if the CSV file has columns labelled 'source' and 'target'. So let's assume that's where we want to visualize \n analyze this data:\n\n\n\n\nIn order to get this correspondence data into Gephi, we will have to rename the \"Sender\" column to \"source\" and the \"Recipient\" column to \"target\". You could do this in a spreadsheet, of course. But since you have Open Refine running.... In the arrow to the left of Sender in the main OpenRefine window, select Edit column-\nRename this column, and rename the column \"source\". Now in the top right of the window, select 'Export-\nCustom tabular exporter'. Notice that \"source\", \"target\", and \"Date\" are checked in the content tab; uncheck \"Date\", as it will not be used in Gephi (networks where the nodes have different dates, ie dynamic networks, are fiddly and we will go into them in more detail later). Go to the download tab and change the download option from 'Tab-separated values (TSV)' to 'Comma-separated values (CSV)' and press download.  The file will likely download to your automatic download directory. We will revisit this file later. You can drop it into the Palladio interface right now though! Go ahead and do that. Do you see any interesting patterns? Make a note!\n\n\n\n\nOPTIONAL: Going further with Open Refine: Named Entity Extraction\n\n\nSay we wanted, instead of the correspondence network, a visualization of all the places named in this body of letters. It might be interesting to visualize on a map the changing focus of Texas' diplomatic attention over time. There is a plugin for Open Refine that does what is called \nNamed Entity Extraction\n. The plugin, and how to install \n use it, is available \nhere\n.\n+ Use regex on your original document containing the letters to clean up the data so that you have one letter per line.\n+ Import the file into Open Refine\n+ Extract Named Entities\n+ Visualize the results in a spreadsheet\n+ Write up a 'how to' in your notebook explaining these steps in detail.\n\n\nAn interesting use case is discussed \nhere\n and \nhere\n\n\n\n\nFurther Help * \nsee this page by Kalani Craig\n\n\n\n\nOptional: Exploring Other Named Entity Extraction tools\n\n\nVoyant Tools RezoViz\n\n\nVoyant-Tools\n has something called 'RezoViz', which will extract entities and tie them together into a network based on appearing in the same document. Upload your corpus to Voyant-Tools. In the top right, there's a 'save' icon. Select 'url for a different tool/skin'. Select 'RezoViz' from the tools list that pops up. A new URL will appear in the box. Copy, paste into a new browser window. Works best on Chrome.\n\n\nStanford NER\n\n\nStanford NER\n download.\n\n\n\n\n\n\nMac instructions for use\n. The link is to Michelle Moravec's instructions, for Mac. \n\n\n\n\n\n\nWindows: If you're on windows and want to do this, things are a bit more complicated. Download and unzip the NER package. \n\n\n\n\n\n\nOpen a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).\n\n\n\n\n\n\nType the following as a single line (highlight the text with your mouse - it scrolls to the right beyond the page, and then copy it):\n\n\n\n\n\n\njava -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML \n \u201cmy-ner-output.txt\u201d\n\n\n\n\nThe first bit, java \u2013mx500m says how much memory to use. If you have 1gb of memory available, you can type java \u2013mx 1g (or 2g, or 3g, etc). The next part of the command calls the NER programme itself. You can set which classifier to use after the \u2013loadClassifier classifiers/ by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier). At \u2013textFile you give it the name of your input file (on our machine, called \u2018texas-letters.txt\u2019, and then specify the outputFormat. The \n character sends the output to a new text file, here called \u201cmy-ner-output.txt\u201d. Hit enter, and a few moments later the programme will tell you something along the lines of\n\n\n\n\nCRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second\n\n\n\n\nOpen the text file in your text editor, and you\u2019ll see output like this:\n\n\n\n\nIn the name of the \nLOCATION\nRepublic of Texas\n/LOCATION\n, Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I \nPERSON\nSam Houston\n/PERSON\n President thereof send Greeting\n\n\n\n\nCongratulations! You've tagged a body of letters. What next? You could organize this into XML, you could visualize, you could regex to find and extract all of your locations, or persons, or...", 
            "title": "Open Refine"
        }, 
        {
            "location": "/supporting materials/open-refine/#an-introduction-to-open-refine", 
            "text": "This text was adopted from the first drafts of The Macroscope  (An alternative Open Refine exercise is offered by  Thomas Padilla  and you may wish to give it a try instead/too.)", 
            "title": "An Introduction to Open Refine"
        }, 
        {
            "location": "/supporting materials/open-refine/#install-open-refine", 
            "text": "OpenRefine (formerly Google Refine) is a powerful tool for working with messy data: cleaning it; transforming it from one format into another; extending it with web services; and linking it to databases like Freebase.   In this exercise, we are going to use a tool that originated with Google. Since 2012, it has been open-sourced and freely available on the net. Using it takes a bit of getting used to, however. Visit the  Open Refine  home page and watch the three videos. Then,  download it to your machine  Follow the installation instructions. Start Open Refine by double clicking on its icon. This will open a new browser window, pointing to http://127.0.0.1:3333. This location is your own computer, so even though it looks like it\u2019s running on the internet, it isn\u2019t. The \u20183333\u2019 is a \u2018port\u2019, meaning that Open Refine is running much like a server, serving up a webpage via that port to the browser.", 
            "title": "Install Open Refine"
        }, 
        {
            "location": "/supporting materials/open-refine/#start-cleaning-our-texan-correspondence", 
            "text": "Start a new project by clicking on the \u2018create project\u2019 tab on the left hand side of the screen. Click on \u2018choose files\u2019 and select the CSV file created in exercise 2. This will give you a preview of your data. Name your project in the box on the top right side (eg., 'm3-exercise3' or similar) and then click \u2018create project\u2019. It may take a few minutes.    Once your project has started, one of the columns that should be visible in your data is \u2018sender\u2019. Click on the arrow to the left of \"Sender\" in OpenRefine and select Facet- Text Facet. Do the same with the arrow next to \"Recipient\". A box will appear on the left side of the browser showing all 189 names listed as senders in the spreadsheet. The spreadsheet itself is nearly a thousand rows, so immediately we see that, in this correspondence collection, some names are used multiple times. You may also have noticed that many of the names suffered from errors in the text scan (OCR or Optical Character Recognition errors), rendering some identical names from the book as similar, but not the same, in the spreadsheet. For example the recipient \"Juan de Dios Cafiedo\" is occasionally listed as \"Juan de Dios CaAedo\". Any subsequent analysis will need these errors to be cleared up, and OpenRefine will help fix them.    Within the \"Sender\" facet box on the left-hand side, click on the button labeled \"Cluster\". This feature presents various automatic ways of merging values that appear to be the same.   Play with the values in the drop-down boxes and notice how the number of clusters found change depending on which method is used. Because the methods are slightly different, each will present different matches that may or may not be useful. If you see two values which should be merged, e.g. \"Ashbel Smith\" and \". Ashbel Smith\", check the box to the right in the 'Merge' column and click the 'Merge Selected   Re-Cluster' button below.    Go through the various cluster methods one-at-a-time, including changing number values, and merge the values which appear to be the same. \"Juan de Dios CaAedo\" clearly should be merged with \"Juan de Dios Cafiedo\", however \"Correspondent in Tampico\" probably should not be merged with \"Correspondent at Vera Cruz.\" Since we are not experts, we will have to use our best judgement in these cases - or get cracking on some more research to help us make the call. By the end, you should have reduced the number of unique Senders from 189 to around 150. Repeat these steps with Recipients, reducing unique Recipients from 192 to about 160. To finish the automatic cleaning of the data, click the arrow next to \"Sender\" and select 'Edit Cells- Common transforms- Trim leading and trailing whitespace'. Repeat for \"Recipient\". The resulting spreadsheet will not be perfect, but it will be much easier to clean by hand than it would have been before taking this step. Click on \u2018export\u2019 at the top right of the window to get your data back out as a .csv file.", 
            "title": "Start Cleaning our Texan Correspondence."
        }, 
        {
            "location": "/supporting materials/open-refine/#now-what", 
            "text": "The text you've just cleaned could now be loaded into something like  Palladio  or  Gephi  for network analysis! However, every network analysis program has its own idiosyncracies. Gephi, for instance, can import lists of relationships if the CSV file has columns labelled 'source' and 'target'. So let's assume that's where we want to visualize   analyze this data:   In order to get this correspondence data into Gephi, we will have to rename the \"Sender\" column to \"source\" and the \"Recipient\" column to \"target\". You could do this in a spreadsheet, of course. But since you have Open Refine running.... In the arrow to the left of Sender in the main OpenRefine window, select Edit column- Rename this column, and rename the column \"source\". Now in the top right of the window, select 'Export- Custom tabular exporter'. Notice that \"source\", \"target\", and \"Date\" are checked in the content tab; uncheck \"Date\", as it will not be used in Gephi (networks where the nodes have different dates, ie dynamic networks, are fiddly and we will go into them in more detail later). Go to the download tab and change the download option from 'Tab-separated values (TSV)' to 'Comma-separated values (CSV)' and press download.  The file will likely download to your automatic download directory. We will revisit this file later. You can drop it into the Palladio interface right now though! Go ahead and do that. Do you see any interesting patterns? Make a note!   OPTIONAL: Going further with Open Refine: Named Entity Extraction  Say we wanted, instead of the correspondence network, a visualization of all the places named in this body of letters. It might be interesting to visualize on a map the changing focus of Texas' diplomatic attention over time. There is a plugin for Open Refine that does what is called  Named Entity Extraction . The plugin, and how to install   use it, is available  here .\n+ Use regex on your original document containing the letters to clean up the data so that you have one letter per line.\n+ Import the file into Open Refine\n+ Extract Named Entities\n+ Visualize the results in a spreadsheet\n+ Write up a 'how to' in your notebook explaining these steps in detail.  An interesting use case is discussed  here  and  here   Further Help *  see this page by Kalani Craig   Optional: Exploring Other Named Entity Extraction tools  Voyant Tools RezoViz  Voyant-Tools  has something called 'RezoViz', which will extract entities and tie them together into a network based on appearing in the same document. Upload your corpus to Voyant-Tools. In the top right, there's a 'save' icon. Select 'url for a different tool/skin'. Select 'RezoViz' from the tools list that pops up. A new URL will appear in the box. Copy, paste into a new browser window. Works best on Chrome.  Stanford NER  Stanford NER  download.    Mac instructions for use . The link is to Michelle Moravec's instructions, for Mac.     Windows: If you're on windows and want to do this, things are a bit more complicated. Download and unzip the NER package.     Open a command prompt in the Stanford NER folder on your Windows machine (you can right-click on the folder in your windows explorer, and select \u2018open command prompt here\u2019).    Type the following as a single line (highlight the text with your mouse - it scrolls to the right beyond the page, and then copy it):    java -mx500m -cp stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier -loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz -textFile texas-letters.txt -outputFormat inlineXML   \u201cmy-ner-output.txt\u201d  The first bit, java \u2013mx500m says how much memory to use. If you have 1gb of memory available, you can type java \u2013mx 1g (or 2g, or 3g, etc). The next part of the command calls the NER programme itself. You can set which classifier to use after the \u2013loadClassifier classifiers/ by typing in the exact file name for the classifier you wish to use (you are telling \u2018loadClassifier\u2019 the exact path to the classifier). At \u2013textFile you give it the name of your input file (on our machine, called \u2018texas-letters.txt\u2019, and then specify the outputFormat. The   character sends the output to a new text file, here called \u201cmy-ner-output.txt\u201d. Hit enter, and a few moments later the programme will tell you something along the lines of   CRFCLassifier tagged 375281 words in 13745 documents at 10833.43 words per second   Open the text file in your text editor, and you\u2019ll see output like this:   In the name of the  LOCATION Republic of Texas /LOCATION , Free, Sovereign and Independent. To all whom these Presents shall come or may in any wise concern. I  PERSON Sam Houston /PERSON  President thereof send Greeting   Congratulations! You've tagged a body of letters. What next? You could organize this into XML, you could visualize, you could regex to find and extract all of your locations, or persons, or...", 
            "title": "Now what?"
        }, 
        {
            "location": "/supporting materials/regex/", 
            "text": "A gentle introduction to Regular Expressions\n\n\nthis text is adopted from the first drafts of\n The Macroscope \nwhich is currently in-press with Imperial College Press. Users should consult that version once it's published.\n\n\nIntroduction\n\n\nA regular expression (also called regex) is a powerful tool for finding and manipulating text.  At its simplest, a regular expression is just a way of looking through texts to locate patterns. A regular expression can help you find every line that begins with a number, or every instance of an email address, or whenever a word is used even if there are slight variations in how it's spelled. As long as you can describe the pattern you're looking for, regular expressions can help you find it. Once you've found your patterns, they can then help you manipulate your text so that it fits just what you need. \n\n\nRegular expressions can look pretty complex, but once you know the basic syntax and vocabulary, simple \u2018regexes\u2019 will be easy. Regular expressions can often be used right inside the 'Find and Replace' box in many text and document editors, such as Notepad++ on Windows, or TextWrangler on OS X. Do not use Microsoft Word, however! To find these text editors, you can find \nNotepad++ here\n or \nTextWrangler here\n.\n\n\n\n\nNB in Notepad++ when you do a search, to use regular expressions in your search you must tick off the checkbox enabling them. Otherwise, Notepad++ will treat your search literally, looking for that exact \ntext\n rather than the \npattern\n. Similarly in Textwrangler, you need to tick off the box marked 'grep' when you bring up the search dialogue panel. In Sublime Text, you need to tick the box that has \n.*\n in the search panel to enable regular expression searches. Please also note that while this tutorial was initially written with the text editors Notepad++ and Textwrangler in mind, all that follows applies equally to other text editors that can work with regular expressions.\n\n\n\n\nSome basic principles\n\n\n\n\nprotip: there are libraries of regular expressions, online. For example, if you want to find all postal codes, you can search \u201cregular expression Canadian postal code\u201d and learn what \u2018formula\u2019 to search for to find them\n\n\n\n\nLet's say you're looking for all the instances of \"cat\" or \"dog\" in your document. When you type the vertical bar on your keyboard (it looks like \n|\n, shift+backslash on windows keyboards), that means 'or' in regular expressions. So, if your query is dog|cat and you press 'find', it will show you the first time either dog or cat appears in your text.\n\n\nIf you want to replace every instance of either \"cat\" or \"dog\" in your document with the world \"animal\", you would open your find-and-replace box, put \ndog|cat\n in the search query, put animal in the 'replace' box, hit 'replace all', and watch your entire document fill up with references to animals instead of dogs and cats.\n\n\nThe astute reader will have noticed a problem with the instructions above; simply replacing every instance of \"dog\" or \"cat\" with \"animal\" is bound to create problems. Simple searches don't differentiate between letters and spaces, so every time \"cat\" or \"dog\" appear within words, they'll also be replaced with \"animal\". \"catch\" will become \"animalch\"; \"dogma\" will become \"animalma\"; \"certificate\" will become \"certifianimale\". In this case, the solution appears simple; put a space before and after your search query, so now it reads: \n\n\ndog | cat\n  \n\n\nWith the spaces, \"animal\" replace \"dog\" or \"cat\" only in those instances where they're definitely complete words; that is, when they're separated by spaces.\n\n\nThe even more astute reader will notice that this still does not solve our problem of replacing every instance of \"dog\" or \"cat\". What if the word comes at the beginning of a line, so it is not in front of a space? What if the word is at the end of a sentence or a clause, and thus followed by a punctuation? Luckily, in the language of regex, you can represent the beginning or end of a word using special characters. \n\n\n\\\n \n\n\nmeans the beginning of a word. In some programs, like TextWrangler, this is used instead:\n\n\n\\b\n\n\nso if you search for \n\\\ncat\n , (or, in TextWrangler, \n\\bcat\n )it will find \"cat\", \"catch\", and \"catsup\", but not \"copycat\", because your query searched for words beginning with \"cat\". For patterns at the end of the line, you would use:\n\n\n\\\n \n\n\nor in TextWrangler,\n\n\n\\b\n\n\nagain.  The remainder of this walk-through imagines that you are using Notepad++, but if you\u2019re using Textwrangler, keep this quirk in mind. If you search for \n\n\ncat\\\n \n\n\nit will find \"cat\" and \"copycat\", but not \"catch,\" because your query searched for words ending with -\"cat\".\n\n\nRegular expressions can be mixed, so if you wanted to find words only matching \"cat\", no matter where in the sentence, you'd search for \n\n\n\\\ncat\\\n\n\nwhich would find every instance. And, because all regular expressions can be mixed, if you searched for (in Notepad++; what would you change, if you were using TextWrangler?)\n\n\n\\\ncat|dog\\\n\n\nand replaced all with \"animal\", you would have a document that replaced all instances of \"dog\" or \"cat\" with \"animal\", no matter where in the sentence they appear. You can also search for variations within a single word using parentheses. For example if you were looking for instances of \"gray\" or \"grey\", instead of the search query\n\n\ngray|grey\n\n\nyou could type \n\n\ngr(a|e)y\n\n\ninstead. The parentheses signify a group, and like the order of operations in arithmetic, regular expressions read the parentheses before anything else. Similarly, if you wanted to find instances of either \"that dog\" or \"that cat\", you would search for: \n\n\n(that dog)|(that cat)\n\n\nNotice that the vertical bar | can appear either inside or outside the parentheses, depending on what you want to search for.\n\n\nThe period character . in regular expressions directs the search to just find any character at all. For example, if we searched for:\n\n\nd.g\n \n\n\nthe search would return \"dig\", \"dog\", \"dug\", and so forth. \n\n\nAnother special character from our cheat sheet, the plus + instructs the program to find any number of the previous character. If we search for \n\n\ndo+g\n\n\nit would return any words that looked like \"dog\", \"doog\", \"dooog\", and so forth. Adding parentheses before the plus would make a search for repetitions of whatever is in the parentheses, for example querying \n\n\n(do)+g\n\n\nwould return \"dog\", \"dodog\", \"dododog\", and so forth.\n\n\nCombining the plus '+' and period '.' characters can be particularly powerful in regular expressions, instructing the program to find any amount of any characters within your search. A search for \n\n\nd.+g\n\n\nfor example, might return \"dried fruits are g\", because the string begins with \"d\" and ends with \"g\", and has various characters in the middle. Searching for simply \".+\" will yield query results that are entire lines of text, because you are searching for any character, and any amount of them.\n\n\nParentheses in regular expressions are also very useful when replacing text. The text within a regular expression forms what's called a group, and the software you use to search remembers which groups you queried in order of their appearance. For example, if you search for \n\n\n(dogs)( and )(cats)\n\n\nwhich would find all instances of \"dogs and cats\" in your document, your program would remember \"dogs\" is group 1, \" and \" is group 2, and \"cats\" is group 3. Notepad++ remembers them as \n\"\\1\"\n, \n\"\\2\"\n, and \n\"\\3\"\n for each group respectively.\n\n\nIf you wanted to switch the order of \"dogs\" and \"cats\" every time the phrase \"dogs and cats\" appeared in your document, you would type \n\n\n(dogs)( and )(cats)\n\n\nin the 'find' box, and \n\n\n\\3\\2\\1\n\n\nin the 'replace' box. That would replace the entire string with group 3 (\"cats\") in the first spot, group 2 (\" and \") in the second spot, and group 1 (\"dogs\") in the last spot, thus changing the result to \"cats and dogs\".\n\n\nThe vocabulary of regular expressions is pretty large, but there are many cheat sheets for regex online (one that I sometimes use is \nhttp://regexlib.com/CheatSheet.aspx\n. Another good one is at \nhttp://docs.activestate.com/komodo/4.4/regex-intro.html\n)\n\n\nNow, continue on to the \nmain exercise", 
            "title": "Regular Expressions (regex)"
        }, 
        {
            "location": "/supporting materials/regex/#a-gentle-introduction-to-regular-expressions", 
            "text": "this text is adopted from the first drafts of  The Macroscope  which is currently in-press with Imperial College Press. Users should consult that version once it's published.", 
            "title": "A gentle introduction to Regular Expressions"
        }, 
        {
            "location": "/supporting materials/regex/#introduction", 
            "text": "A regular expression (also called regex) is a powerful tool for finding and manipulating text.  At its simplest, a regular expression is just a way of looking through texts to locate patterns. A regular expression can help you find every line that begins with a number, or every instance of an email address, or whenever a word is used even if there are slight variations in how it's spelled. As long as you can describe the pattern you're looking for, regular expressions can help you find it. Once you've found your patterns, they can then help you manipulate your text so that it fits just what you need.   Regular expressions can look pretty complex, but once you know the basic syntax and vocabulary, simple \u2018regexes\u2019 will be easy. Regular expressions can often be used right inside the 'Find and Replace' box in many text and document editors, such as Notepad++ on Windows, or TextWrangler on OS X. Do not use Microsoft Word, however! To find these text editors, you can find  Notepad++ here  or  TextWrangler here .   NB in Notepad++ when you do a search, to use regular expressions in your search you must tick off the checkbox enabling them. Otherwise, Notepad++ will treat your search literally, looking for that exact  text  rather than the  pattern . Similarly in Textwrangler, you need to tick off the box marked 'grep' when you bring up the search dialogue panel. In Sublime Text, you need to tick the box that has  .*  in the search panel to enable regular expression searches. Please also note that while this tutorial was initially written with the text editors Notepad++ and Textwrangler in mind, all that follows applies equally to other text editors that can work with regular expressions.", 
            "title": "Introduction"
        }, 
        {
            "location": "/supporting materials/regex/#some-basic-principles", 
            "text": "protip: there are libraries of regular expressions, online. For example, if you want to find all postal codes, you can search \u201cregular expression Canadian postal code\u201d and learn what \u2018formula\u2019 to search for to find them   Let's say you're looking for all the instances of \"cat\" or \"dog\" in your document. When you type the vertical bar on your keyboard (it looks like  | , shift+backslash on windows keyboards), that means 'or' in regular expressions. So, if your query is dog|cat and you press 'find', it will show you the first time either dog or cat appears in your text.  If you want to replace every instance of either \"cat\" or \"dog\" in your document with the world \"animal\", you would open your find-and-replace box, put  dog|cat  in the search query, put animal in the 'replace' box, hit 'replace all', and watch your entire document fill up with references to animals instead of dogs and cats.  The astute reader will have noticed a problem with the instructions above; simply replacing every instance of \"dog\" or \"cat\" with \"animal\" is bound to create problems. Simple searches don't differentiate between letters and spaces, so every time \"cat\" or \"dog\" appear within words, they'll also be replaced with \"animal\". \"catch\" will become \"animalch\"; \"dogma\" will become \"animalma\"; \"certificate\" will become \"certifianimale\". In this case, the solution appears simple; put a space before and after your search query, so now it reads:   dog | cat     With the spaces, \"animal\" replace \"dog\" or \"cat\" only in those instances where they're definitely complete words; that is, when they're separated by spaces.  The even more astute reader will notice that this still does not solve our problem of replacing every instance of \"dog\" or \"cat\". What if the word comes at the beginning of a line, so it is not in front of a space? What if the word is at the end of a sentence or a clause, and thus followed by a punctuation? Luckily, in the language of regex, you can represent the beginning or end of a word using special characters.   \\    means the beginning of a word. In some programs, like TextWrangler, this is used instead:  \\b  so if you search for  \\ cat  , (or, in TextWrangler,  \\bcat  )it will find \"cat\", \"catch\", and \"catsup\", but not \"copycat\", because your query searched for words beginning with \"cat\". For patterns at the end of the line, you would use:  \\    or in TextWrangler,  \\b  again.  The remainder of this walk-through imagines that you are using Notepad++, but if you\u2019re using Textwrangler, keep this quirk in mind. If you search for   cat\\    it will find \"cat\" and \"copycat\", but not \"catch,\" because your query searched for words ending with -\"cat\".  Regular expressions can be mixed, so if you wanted to find words only matching \"cat\", no matter where in the sentence, you'd search for   \\ cat\\  which would find every instance. And, because all regular expressions can be mixed, if you searched for (in Notepad++; what would you change, if you were using TextWrangler?)  \\ cat|dog\\  and replaced all with \"animal\", you would have a document that replaced all instances of \"dog\" or \"cat\" with \"animal\", no matter where in the sentence they appear. You can also search for variations within a single word using parentheses. For example if you were looking for instances of \"gray\" or \"grey\", instead of the search query  gray|grey  you could type   gr(a|e)y  instead. The parentheses signify a group, and like the order of operations in arithmetic, regular expressions read the parentheses before anything else. Similarly, if you wanted to find instances of either \"that dog\" or \"that cat\", you would search for:   (that dog)|(that cat)  Notice that the vertical bar | can appear either inside or outside the parentheses, depending on what you want to search for.  The period character . in regular expressions directs the search to just find any character at all. For example, if we searched for:  d.g    the search would return \"dig\", \"dog\", \"dug\", and so forth.   Another special character from our cheat sheet, the plus + instructs the program to find any number of the previous character. If we search for   do+g  it would return any words that looked like \"dog\", \"doog\", \"dooog\", and so forth. Adding parentheses before the plus would make a search for repetitions of whatever is in the parentheses, for example querying   (do)+g  would return \"dog\", \"dodog\", \"dododog\", and so forth.  Combining the plus '+' and period '.' characters can be particularly powerful in regular expressions, instructing the program to find any amount of any characters within your search. A search for   d.+g  for example, might return \"dried fruits are g\", because the string begins with \"d\" and ends with \"g\", and has various characters in the middle. Searching for simply \".+\" will yield query results that are entire lines of text, because you are searching for any character, and any amount of them.  Parentheses in regular expressions are also very useful when replacing text. The text within a regular expression forms what's called a group, and the software you use to search remembers which groups you queried in order of their appearance. For example, if you search for   (dogs)( and )(cats)  which would find all instances of \"dogs and cats\" in your document, your program would remember \"dogs\" is group 1, \" and \" is group 2, and \"cats\" is group 3. Notepad++ remembers them as  \"\\1\" ,  \"\\2\" , and  \"\\3\"  for each group respectively.  If you wanted to switch the order of \"dogs\" and \"cats\" every time the phrase \"dogs and cats\" appeared in your document, you would type   (dogs)( and )(cats)  in the 'find' box, and   \\3\\2\\1  in the 'replace' box. That would replace the entire string with group 3 (\"cats\") in the first spot, group 2 (\" and \") in the second spot, and group 1 (\"dogs\") in the last spot, thus changing the result to \"cats and dogs\".  The vocabulary of regular expressions is pretty large, but there are many cheat sheets for regex online (one that I sometimes use is  http://regexlib.com/CheatSheet.aspx . Another good one is at  http://docs.activestate.com/komodo/4.4/regex-intro.html )", 
            "title": "Some basic principles"
        }, 
        {
            "location": "/supporting materials/regex/#now-continue-on-to-the-main-exercise", 
            "text": "", 
            "title": "Now, continue on to the main exercise"
        }, 
        {
            "location": "/supporting materials/regex-ner/", 
            "text": "Further Munging the output of NER\n\n\nSo, you've learned how to tag a corpus using the Stanford NER (\nhere is that exercise again, as a reminder\n). There are several ways we might like to visualize that output.\n\n\nUnfortunately, depending on what you want to \ndo\n with that data, you are going to have to go back to the data-munging cycle. \n\n\nAs a network\n\n\nLet us imagine that we wanted to visualize the locations mentioned in the letters as a kind of network.\n\n\nWe will use regular expressions to further manipulate the data into a useful source -\n target list.\n\n\nREGEX on Mac to Useful Output (PC, please skip to next section):\n\n\nWe need to organize those locations so that locations mentioned in a letter are connected to each other. To begin, we trim away any line that does not start with LOCATION. We do this by using our regular expression skills from module 3, and adding in a few more commands:\n\n\nFIND: \n^(?!.*LOCATION).*$\n\n\nand replace with nothing. We had a few new commands in there: the \n?!\n tells it to begin looking ahead for the literal phrase LOCATION, and then the dot and dollar sign let us know to end at the end of the line. In any case, this will delete everything that doesn't have the location tag in front. Now, let us also mark those blank lines we noticed as the start of a new letter. \n\n\nFIND: \n^\\s*$\n\n\nwhere:\n\n^\n marks the beginning of a line\n\n\n$\n marks the end of a line\n\n\n\\s\n indicates \u2018whitespace\u2019\n\n\n\\*\n indicates a zero or more repetition of the thing in front of it (in this case, whitespace)\n\n\nand we replace with the phrase, \u201cblankspace\u201d. Because there might be a few blank lines, you might see \u2018blankspace\u2019 in places. Where you\u2019ve got \u2018blankspaceblankspace\u2019, that\u2019s showing us the spaces between the original documents (whereas one \u2018blankspace\u2019 was where an ORGANIZATION or PERSON tag was removed).\n\n\nAt this point, we might want to remove spaces between place names and use underscores instead, so that when we finally get this into a comma separated format, places like \u2018United States\u2019 will be \u2018United_States\u2019 rather than \u2018united, states\u2019.\n\n\nFind:  \n-a single space\n\nReplace: \n\\_\n\n\nNow, we want to reintroduce the space after \u2018LOCATION:\u2019, so\n\n\nFind: \n:_\n       \n-this is a colon, followed by an underscore\n\nReplace: \n:\n \n-this is a colon, followed by a space\n\n\nNow we want to get all of the locations for a single letter into a single line. So we want to get rid of new lines and LOCATION:\n\n\nFind: \n\\n(LOCATION:)\n\nReplace: \n\n\nIt is beginning to look like a list. Let\u2019s replace \u2018blankspaceblankspace\u2019 with \u2018new-document\u2019.\n\n\nFind: \nblankspaceblankspace\n\nReplace: new-document\n\n\nAnd now let\u2019s get those single blankspace lines excised:\n\n\nFind \n\\n(blankspace)\n\nReplace: \n\n\nNow you have something that looks like this:\n\n\n\nnew-document Houston Republic_of_Texas United_States Texas\n\nnew-document Texas\n\nnew-document United_States Town_of_Columbia\n\nnew-document United_States Texas\n\nnew-document United_States United_States_of_ Republic_of_Texas\n\nnew-document New_Orleans United_States_Govern\n\nnew-document United_States Houston United_States Texas united_states\n\nnew-document United_States New_Orleans United_States\n\nnew-document Houston United_States Texas United_States\n\nnew-document New_Orleans\n\n\n\n\n\nWhy not leave \u2018new-document\u2019 in the file? It\u2019ll make a handy marker. Let\u2019s replace the spaces with commas:\n\n\nFind: \n     \n-a single space\n\nReplace: \n,\n \n\n\nSave your work as a *.csv file. You now have a file that you can load into a variety of other platforms or tools to perform your analysis. Of course, NER is not perfect, especially when dealing with names like \u2018Houston\u2019 that were a personal name long before they were a place name.\n\n\nREGEX on PC to Useful Output:\n\n\nOpen your tagged file in Notepad++.  There are a lot of carriage returns, line breaks, and white spaces in this document that will make our regex very complicated and will in fact break it periodically, if we try to work with it as it is. Instead, let us remove all new lines and whitespaces and the outset, and use regex to put structure back. To begin, we want to get the entire document into a single line:\n\n\nFind: \n\\n\n\nReplace with nothing.\n\n\nNotepad++ reports the number of lines in the document at the bottom of the window. It should now report lines: 1.\n\n\nLet\u2019s introduce line breaks to correspond with the original letters (ie, a line break signals a new letter), since we want to visualize the network of locations mentioned where we join places together on the basis of being mentioned in the same letter. If we examine the original document, one candidate for something to search for, to signal a new letter, could be  \nPERSON\n to \nPERSON\n but the named entity recognizer sometimes confuses persons for places or organizations; the object character recognition also makes things complicated.  Since the letters are organized chronologically, and we can see \u2018Digitized by Google\u2019 at the bottom of every page (and since no writer in the 1840s is going to use the word digitized) let\u2019s use that as our page break marker. For the most part, one letter corresponds to one page in the book. It\u2019s not ideal, but this is something that the historian will have to discuss in her methods and conclusions. Perhaps, over the seven hundred odd letters in this collection, it doesn\u2019t actually make much difference.\n\n\nFind: \n(digitized)\n\nReplace:\n\\n\\1\n\n\nYou now have on the order of 700 odd lines in the document. \n\n\nLet\u2019s find the locations and put them on individual lines, so that we can strip out everything else. \n\n\nFind: \n(LOCATION)(.\\*?)(LOCATION)\n\nReplace: \n\\n\\2\n\n\nIn the search string above, the \n.\\*\n would look for everything between the first \nlocation\n on the line and the last \nlocation\nin the line, so we would get a lot of junk. By using \n.\\*?\n we just get the text between \nlocation\nand the next \nlocation\n tag.\nW\ne need to replace the \n and \n from the location tags now, as those are characters with special meanings in our regular expressions. Turn off the regular expressions in notepad ++ by unticking the \u2018regular expression\u2019 box. Now search for \n, \n, and \\ in turn, replacing them with tildes:\n\n\nLOCATION\nTexas\n/LOCATION\n becomes \n~Texas~~~\n\n\nNow we want to delete in each line everything that isn\u2019t a location. Our locations start the line and are trailed by three tildes, so we just need to find those three tildes and everything that comes after, and delete. Turn regular expressions back on in the search window.\n\n\nFind: \n(~~~)(.*)\n\nReplace with nothing.\n\n\nOur marker for a new page in this book was the line that began with \u2018Digitized by\u2019. We need to delete that line in its entirety, leaving a blank line.\n\n\nFind: \n(Digitized)(.\\*)\n\nReplace: \n\\n\n\n\nNow we want to get those locations all in the same line again. We need to find the new line character followed by the tilde, and then delete that new line, replacing it with a comma:\n\n\nFind: \n(\\n)(~)\n\nReplace: \n,\n\n\nNow we remove extraneous blank lines.\n\n\nFind: \n\\s*$\n\nReplace with nothing. \n\n\nWe\u2019re almost there. Let\u2019s remove the comma that starts each line by searching for \n\n^,\n (remembering that the carat character indicates the start of a line) and replacing with nothing. Save this as 'cleaned-locations.csv' Congratulations! You\u2019ve taken quite complicated output and cleaned it so that every place mentioned on a single page in the original publication is now on its own line, which means you can import it into a network visualization package, a spreadsheet, or some other tool!\n\n\nNow you can upload this csv to Gephi.", 
            "title": "Going further with regex"
        }, 
        {
            "location": "/supporting materials/regex-ner/#further-munging-the-output-of-ner", 
            "text": "So, you've learned how to tag a corpus using the Stanford NER ( here is that exercise again, as a reminder ). There are several ways we might like to visualize that output.  Unfortunately, depending on what you want to  do  with that data, you are going to have to go back to the data-munging cycle.", 
            "title": "Further Munging the output of NER"
        }, 
        {
            "location": "/supporting materials/regex-ner/#as-a-network", 
            "text": "Let us imagine that we wanted to visualize the locations mentioned in the letters as a kind of network.  We will use regular expressions to further manipulate the data into a useful source -  target list.  REGEX on Mac to Useful Output (PC, please skip to next section):  We need to organize those locations so that locations mentioned in a letter are connected to each other. To begin, we trim away any line that does not start with LOCATION. We do this by using our regular expression skills from module 3, and adding in a few more commands:  FIND:  ^(?!.*LOCATION).*$  and replace with nothing. We had a few new commands in there: the  ?!  tells it to begin looking ahead for the literal phrase LOCATION, and then the dot and dollar sign let us know to end at the end of the line. In any case, this will delete everything that doesn't have the location tag in front. Now, let us also mark those blank lines we noticed as the start of a new letter.   FIND:  ^\\s*$  where: ^  marks the beginning of a line  $  marks the end of a line  \\s  indicates \u2018whitespace\u2019  \\*  indicates a zero or more repetition of the thing in front of it (in this case, whitespace)  and we replace with the phrase, \u201cblankspace\u201d. Because there might be a few blank lines, you might see \u2018blankspace\u2019 in places. Where you\u2019ve got \u2018blankspaceblankspace\u2019, that\u2019s showing us the spaces between the original documents (whereas one \u2018blankspace\u2019 was where an ORGANIZATION or PERSON tag was removed).  At this point, we might want to remove spaces between place names and use underscores instead, so that when we finally get this into a comma separated format, places like \u2018United States\u2019 will be \u2018United_States\u2019 rather than \u2018united, states\u2019.  Find:   -a single space \nReplace:  \\_  Now, we want to reintroduce the space after \u2018LOCATION:\u2019, so  Find:  :_         -this is a colon, followed by an underscore \nReplace:  :   -this is a colon, followed by a space  Now we want to get all of the locations for a single letter into a single line. So we want to get rid of new lines and LOCATION:  Find:  \\n(LOCATION:) \nReplace:   It is beginning to look like a list. Let\u2019s replace \u2018blankspaceblankspace\u2019 with \u2018new-document\u2019.  Find:  blankspaceblankspace \nReplace: new-document  And now let\u2019s get those single blankspace lines excised:  Find  \\n(blankspace) \nReplace:   Now you have something that looks like this:  \nnew-document Houston Republic_of_Texas United_States Texas \nnew-document Texas \nnew-document United_States Town_of_Columbia \nnew-document United_States Texas \nnew-document United_States United_States_of_ Republic_of_Texas \nnew-document New_Orleans United_States_Govern \nnew-document United_States Houston United_States Texas united_states \nnew-document United_States New_Orleans United_States \nnew-document Houston United_States Texas United_States \nnew-document New_Orleans   Why not leave \u2018new-document\u2019 in the file? It\u2019ll make a handy marker. Let\u2019s replace the spaces with commas:  Find:        -a single space \nReplace:  ,    Save your work as a *.csv file. You now have a file that you can load into a variety of other platforms or tools to perform your analysis. Of course, NER is not perfect, especially when dealing with names like \u2018Houston\u2019 that were a personal name long before they were a place name.  REGEX on PC to Useful Output:  Open your tagged file in Notepad++.  There are a lot of carriage returns, line breaks, and white spaces in this document that will make our regex very complicated and will in fact break it periodically, if we try to work with it as it is. Instead, let us remove all new lines and whitespaces and the outset, and use regex to put structure back. To begin, we want to get the entire document into a single line:  Find:  \\n \nReplace with nothing.  Notepad++ reports the number of lines in the document at the bottom of the window. It should now report lines: 1.  Let\u2019s introduce line breaks to correspond with the original letters (ie, a line break signals a new letter), since we want to visualize the network of locations mentioned where we join places together on the basis of being mentioned in the same letter. If we examine the original document, one candidate for something to search for, to signal a new letter, could be   PERSON  to  PERSON  but the named entity recognizer sometimes confuses persons for places or organizations; the object character recognition also makes things complicated.  Since the letters are organized chronologically, and we can see \u2018Digitized by Google\u2019 at the bottom of every page (and since no writer in the 1840s is going to use the word digitized) let\u2019s use that as our page break marker. For the most part, one letter corresponds to one page in the book. It\u2019s not ideal, but this is something that the historian will have to discuss in her methods and conclusions. Perhaps, over the seven hundred odd letters in this collection, it doesn\u2019t actually make much difference.  Find:  (digitized) \nReplace: \\n\\1  You now have on the order of 700 odd lines in the document.   Let\u2019s find the locations and put them on individual lines, so that we can strip out everything else.   Find:  (LOCATION)(.\\*?)(LOCATION) \nReplace:  \\n\\2  In the search string above, the  .\\*  would look for everything between the first  location  on the line and the last  location in the line, so we would get a lot of junk. By using  .\\*?  we just get the text between  location and the next  location  tag.\nW\ne need to replace the   and   from the location tags now, as those are characters with special meanings in our regular expressions. Turn off the regular expressions in notepad ++ by unticking the \u2018regular expression\u2019 box. Now search for  ,  , and \\ in turn, replacing them with tildes:  LOCATION Texas /LOCATION  becomes  ~Texas~~~  Now we want to delete in each line everything that isn\u2019t a location. Our locations start the line and are trailed by three tildes, so we just need to find those three tildes and everything that comes after, and delete. Turn regular expressions back on in the search window.  Find:  (~~~)(.*) \nReplace with nothing.  Our marker for a new page in this book was the line that began with \u2018Digitized by\u2019. We need to delete that line in its entirety, leaving a blank line.  Find:  (Digitized)(.\\*) \nReplace:  \\n  Now we want to get those locations all in the same line again. We need to find the new line character followed by the tilde, and then delete that new line, replacing it with a comma:  Find:  (\\n)(~) \nReplace:  ,  Now we remove extraneous blank lines.  Find:  \\s*$ \nReplace with nothing.   We\u2019re almost there. Let\u2019s remove the comma that starts each line by searching for  ^,  (remembering that the carat character indicates the start of a line) and replacing with nothing. Save this as 'cleaned-locations.csv' Congratulations! You\u2019ve taken quite complicated output and cleaned it so that every place mentioned on a single page in the original publication is now on its own line, which means you can import it into a network visualization package, a spreadsheet, or some other tool!  Now you can upload this csv to Gephi.", 
            "title": "As a network"
        }, 
        {
            "location": "/supporting materials/regexex/", 
            "text": "REGEX and the Republic of Texas\n\n\nThe correspondence of the Republic of Texas was collated into a single volume and published with a helpful index in 1911. It was scanned and OCR'd by Google, and is now available as a text file from the Internet Archive. You can see the OCR'd text at \nthis page\n. We are going to grab the index from that file, and transform it using regex. \n\n\nThere are several hundred entries in that index. You could clean them up by hand, deleting and cutting and pasting, but with the power of regex, we'll go from this:\n\n\nSam Houston to A. B. Roman, September 12, 1842 101\nSam Houston to A. B. Roman, October 29, 1842 101\nCorrespondence for 1843-1846 \u2014\nIsaac Van Zandt to Anson Jones, January 11, 1843 103\n\n\n\n\n...to nicely CSV-formatted table like this:\n\n\nSam Houston, A. B. Roman, September 12 1842\nSam Houston, A. B. Roman, October 29 1842\nIsaac Van Zandt, Anson Jones, January 11 1843\n\n\n\n\nThe change doesn't look like much, and you might think to yourself, 'hey, I could just do that by hand'. You could; but it'd take you ages, and if you made a mistake somewhere - are you sure you could do this consisently, for a couple of hours at a time? Probably not. Your time is better spent figuring out the search and replace patterns, and then setting your machine loose to implement it.\n\n\nData formatted like this could be fed into a network analysis program, for instance, or otherwise visualized and analyzed. Regex as we are going to use in this tutorial allows us to go from unstructured to structured data. \nThere is a video at the end of this document that captures someone working through this exercise.\n\n\nGetting started\n\n\nIn the previous module, we learned how to automatically grab text from sites like the Internet Archive. In this particular exercise today, we'll just copy and paste from the OCR'd text page (as we're working with a single document).\n\n\n\n\nCopy the OCR'd text from this file: \nhttp://archive.org/stream/diplomaticcorre33statgoog/diplomaticcorre33statgoog_djvu.txt\n\n\n\n\nand paste it into a text file (using your text editor). Save it as \ntexan-correspondence.txt\n. Remember to save a spare copy of your file before you begin - this is very important, because you're going to make mistakes that you won't be sure how to fix. \n\n\n\n\nDelete everything in your working copy of the file except for the index of the list of letters. \n\n\n\n\nThat is, you\u2019re looking for the table of letters, starting with \u2018Sam Houston to J. Pinckney Henderson, December 31, 1836 51\u2019 and ending with \u2018Wm. Henry Daingerfield to Ebenezer Allen, February 2, 1846 1582\u2019. Your file will now have approximately 2000 lines in it.\n\n\nNotice that there is a lot of text that we are not interested in at the moment: page numbers, headers, footers, or categories. We're going to use regular expressions to get rid of them. What we want to end up with is a spreadsheet that is arranged in three columns:\n\n\nSender, Recipient, Date \n\n\n\n\nScroll down through the text; notice there are many lines which don't include a letter, because they're either header info, or blank, or some other extraneous text. We're going to get rid of all of those lines. We want to keep every line that has this information in it: Sender to Recipient, Month, Date, Year, Page\n\n\nThe workflow\n\n\nWe start by finding every line that looks like a reference to a letter, and put a tilde (a \n~\n symbol) at the beginning of it so we know to save it for later. Next, we get rid of all the lines that don't start with tildes, so that we're left with only the relevant text. After this is done, we format the remaining text by putting commas in appropriate places, so we can import it into a spreadsheet and do further edits there.\n\n\nStep One\n\n\nIdentifying Lines that have Correspondence Senders and Receivers in them\n\n\nDiscussion\n Read in full before doing any manipulation of your text!\n\n\nIn Notepad++, press ctrl-f or search-\nfind to open the find dialogue box.  In that box, go to the 'Replace' tab, and check the radio box for 'Regular expression' at the bottom of the search box. In TextWrangler, hit command+f to open the find and replace dialogue box.  Tick off the \u2018grep\u2019 radio button (which tells TextWrangler that we want to do a regex search) and the \u2018wraparound\u2019 button (which tells TextWrangler to search everywhere). In Sublime text, command+f opens the 'find' box (and shift+command+f opens find \nand\n replace). Tick the '.*' button to tell Sublime we're working with regular expressions.\n\n\nRemember from our basic introduction that there's a way to see if the word \"to\" appears in full. Type\n\n\n\\\nto\\\n\n\n\n\n\nin the search bar.  Recall in TextWrangler we would look for \n\\bto\\b\n instead. This will find every instance of the word \"to\" (and not, for instance, also \u2018potato\u2019 or \u2018tomorrow\u2019). (If you find your text editor doesn't seem to recognize some of these flags, try the others. Sadly, not every text editor implements regular expressions in quite the same way.) \n\n\nWe don't just want to find \"to\", but the entire line that contains it. We assume that every line that contains the word \u201cto\u201d in full is a line that has relevant letter information, and every line that does not is one we do not need. You learned earlier that the query \n.+\n returns any amount of text, no matter what it says. Assuming you are using Notepad++, if your query is \n\n\n.+\\\nto\\\n.+\n \n\n\nyour search will return every line which includes the word \"to\" in full, no matter what comes before or after it, and none of the lines which don't. In TextWrangler, your query would be \n.+\\bto\\b.+.\n \n\n\nAs mentioned earlier, we want to add a tilde ~ before each of the lines that look like letters, so we can save them for later. This involves the find-and-replace function, and a query identical to the one before, but with parentheses around it, so it looks like \n\n\n(.+\\\nto\\\n)\n\n\nand the entire line is placed within a parenthetical group. In the 'replace' box, enter \n\n\n~\\1\n\n\nwhich just means replace the line with itself (group 1), placing a tilde before it. \n\n\n\n\nCopy and past some of your text into \nRegExr.com\n. Write your regular expression (ie, what you're trying to find), and your substitution (ie, what you're replace with) in the RegExr interface. Once you're satisfied that you've got it right, run this on your working copy of your text. You might want to save with a new name at each step of this process, so that if something goes wrong, you can recover easily!\n\n\n\n\nStep Two\n\n\nRemoving Lines that Aren\u2019t Relevant\n\n\nDiscussion\n\n\nAfter running the find-and-replace, you should note your document now has most of the lines with tildes in front of it, and a few which do not. The next step is to remove all the lines that do not include a tilde. The search string to find all lines which don't begin with tildes is \n\n\\n[^~].+\n\n\nA \n\\n\n at the beginning of a query searches for a new line, which means it's going to start searching at the first character of each new line.  \n\n\nHowever, given the evolution of computing, it may well be that this won\u2019t quite work on your system\n. \n\n\nLinux based systems use \n\\n\n for a new line, while Windows often uses \n\\r\\n\n, and older Macs just use ``` \\r ````. These are the sorts of things that can drive us crazy, and so we digital historians need to keep in mind! Since this will likely cause much frustration, your safest bet will be to save a copy of what you are working on, and then experiment to see what gives you the best result. In most cases, this will be:\n\n\n\\r\\n[^~].+\n\n\nWithin a set of square brackets \n[]\n the carrot \n^\n means search for anything that isn't within these brackets; in this case, the tilde \n~\n. The  \n.+\n as before means search for all the rest of the characters in the line as well. All together, the query returns any full line which does not begin with a tilde; that is, the lines we did not mark as looking like letters. \n\n\nBy finding all \n\\r\\n[^~].+\nand replacing it with nothing, you effectively delete all the lines that don't look like letters. What you're left with is a series of letters, and a series of blank lines. \n\n\n\n\nDevise the regular expression that works on your machine, and replace with nothing. Test on RegExr or on a smaller chunk of your text to make sure it works.\n\n\n\n\nStep Three\n\n\nRemoving the Blank Lines\n\n\nDiscussion\n\n\nGiven what we know about new lines, what do you think a regex would be for a blank line? Try that out in RegExr now. In Notepad++, it'll most likely be \n\\n\\r\n ; in textwrangler \n^\\r\n. Do you see what the difference is? Examine the cheat sheets and documentation at RegExr and make a note in your notebook on the difference.\n\n\n\n\nDevise the regular expression to find the blank lines, and replace with nothing. Are you still saving at each successful step?\n\n\n\n\nStep Four\n\n\nTransforming into CSV format\n\n\nDiscussion\n\n\nTo turn this text file into a spreadsheet, we'll want to separate it out into one column for sender, one for recipient, and one for date, each separated by a single comma. Notice that most lines have extraneous page numbers attached to them; we can get rid of those with regular expressions. There's also usually a comma separating the month-date and the year, which we'll get rid of as well. In the end, the first line should go from looking like:\n\n\n~Sam Houston to J. Pinckney Henderson, December 31, 1836 51\n\n\nto\n\n\nSam Houston, J. Pinckney Henderson, December 31 1836\n\n\nsuch that each data point is in its own column.\n\n\nYou will start by removing the page number after the year and the comma between the year and the month-date. To do this, first locate the year on each line by using the regex:\n\n\n[0-9]{4}\n\n\nAs a cheat sheet shows, \n[0-9]\n finds any digit between 0 and 9, and \n{4}\nwill find four of them together. Now extend that search out by appending \n.+\n to the end of the query; as seen before, it will capture the entire rest of the line. The query \n\n\n[0-9]{4}.+\n\n\nwill return, for example, \"1836 51\", \"1839 52\", and \"1839 53\" from the first three lines of the text. We also want to capture the comma preceding the year, so add a comma and a space before the query, resulting in \n\n\n, [0-9]{4}.+\n\n\nwhich will return \", 1836 51\", \", 1839 52\", etc.    \n\n\nThe next step is making the parenthetical groups which will be used to remove parts of the text with find-and-replace. In this case, we want to remove the comma and everything after year, but not the year or the space before it. Thus our query will look like:\n\n\n(,)( [0-9]{4})(.+)\n\n\nwith the comma as the first group \n\"\\1\"\n, the space and the year as the second \n\"\\2\"\n, and the rest of the line as the third \n\"\\3\"\n.  Given that all we care about retaining is the second group (we want to keep the year, but not the comma or the page number), what will the \nreplace\n look like?\n\n\n\n\nFind the dates using a regex, and replace so that only the \nsecond\n group in the expression is kept. You might want to consult the introduction to regex again before you execute this one.\n\n\n\n\nStep Five\n\n\nRemoving the tildes\n\n\n\n\nFind the tildes that we used to mark off our text of interest, and replace them with nothing to delete them.\n\n\n\n\nStep Six\n\n\nSeparating Senders and Receivers\n\n\nDiscussion\n\n\nFinally, to separate the sender and recipient by a comma, we find all instances of the word \"to\" and replace it with a comma. Although we used \n\\\n and \n\\\n(in TextWrangler, \n\\b\n) to denote the beginning and end of a word earlier in the lesson, we don't exactly do that here. We include the space preceding \u201cto\u201d in the regular expression, as well as the \n\\\n (\n\\b\n in TextWrangler) to denote the word ending. Once we find instances of the word and the space preceding it, \nto\\\n (nb, remember the space!!!) we replace it with a comma \n,\n.\n\n\n\n\nDevise the regex to find the word, and replace with a comma.\n\n\n\n\nStep Seven\n\n\nCleaning up\n\n\nDiscussion\n\n\nYou may notice that some lines still do not fit our criteria. Line 22, for example, reads \"Abner S. Lipscomb, James Hamilton and A. T. Bumley, AugUHt 15, \". It has an incomplete date; these we don't need to worry about for our purposes. More worrisome are lines, like 61 \"Copy and summary of instructions United States Department of State, \" which include none of the information we want. We can get rid of these lines later in a spreadsheet.  \n\n\nThe only non-standard lines we need to worry about with regular expressions are the ones with more than 2 commas, like line 178, \"A. J. Donelson, Secretary of State [Allen,. arf interim], December 10 1844\". Notice that our second column, the name of the recipient, has a comma inside of it. If you were to import this directly into a spreadsheet, you would get four columns, one for sender, two for recipient, and one for date, which would break any analysis you would then like to run. Unfortunately these lines need to be fixed by hand, but happily regular expressions make finding them easy. The query:\n\n\n.+,.+,.+,\n\n\nwill show you every line with more than 2 commas, because it finds any line that has any set of characters, then a comma, then any other set, then another comma, and so forth.\n\n\n\n\n\n\nDevise a regex to find these lines with more than 2 commas in them. Fix these lines manually, and then click 'find next' to grab the next one.\n\n\n\n\n\n\nAt the top of the file, add a new line that simply reads \"Sender, Recipient, Date\". These will be the column headers. Save as \ncleaned-correspondence.csv\n.\n\n\n\n\n\n\nCongratulations!\n\n\nYou've now used regex to extract, transform, and clean historical text. As a csv file, you could now load this data into a network analysis program such as \nGephi\n to explore the ramifications of this correspondence network. Upload your file to your repository, and make a note of the original location of the file, the transformations that you've done, and the date/time. You will be using your \ncleaned-correspondence.csv\n file in the next exercise using \nOpen Refine\n, where we'll sort out some of the messy OCR (fixing names, and so on). \n\n\nVideo", 
            "title": "Regex & the Republic of Texas"
        }, 
        {
            "location": "/supporting materials/regexex/#regex-and-the-republic-of-texas", 
            "text": "The correspondence of the Republic of Texas was collated into a single volume and published with a helpful index in 1911. It was scanned and OCR'd by Google, and is now available as a text file from the Internet Archive. You can see the OCR'd text at  this page . We are going to grab the index from that file, and transform it using regex.   There are several hundred entries in that index. You could clean them up by hand, deleting and cutting and pasting, but with the power of regex, we'll go from this:  Sam Houston to A. B. Roman, September 12, 1842 101\nSam Houston to A. B. Roman, October 29, 1842 101\nCorrespondence for 1843-1846 \u2014\nIsaac Van Zandt to Anson Jones, January 11, 1843 103  ...to nicely CSV-formatted table like this:  Sam Houston, A. B. Roman, September 12 1842\nSam Houston, A. B. Roman, October 29 1842\nIsaac Van Zandt, Anson Jones, January 11 1843  The change doesn't look like much, and you might think to yourself, 'hey, I could just do that by hand'. You could; but it'd take you ages, and if you made a mistake somewhere - are you sure you could do this consisently, for a couple of hours at a time? Probably not. Your time is better spent figuring out the search and replace patterns, and then setting your machine loose to implement it.  Data formatted like this could be fed into a network analysis program, for instance, or otherwise visualized and analyzed. Regex as we are going to use in this tutorial allows us to go from unstructured to structured data.  There is a video at the end of this document that captures someone working through this exercise.", 
            "title": "REGEX and the Republic of Texas"
        }, 
        {
            "location": "/supporting materials/regexex/#getting-started", 
            "text": "In the previous module, we learned how to automatically grab text from sites like the Internet Archive. In this particular exercise today, we'll just copy and paste from the OCR'd text page (as we're working with a single document).   Copy the OCR'd text from this file:  http://archive.org/stream/diplomaticcorre33statgoog/diplomaticcorre33statgoog_djvu.txt   and paste it into a text file (using your text editor). Save it as  texan-correspondence.txt . Remember to save a spare copy of your file before you begin - this is very important, because you're going to make mistakes that you won't be sure how to fix.    Delete everything in your working copy of the file except for the index of the list of letters.    That is, you\u2019re looking for the table of letters, starting with \u2018Sam Houston to J. Pinckney Henderson, December 31, 1836 51\u2019 and ending with \u2018Wm. Henry Daingerfield to Ebenezer Allen, February 2, 1846 1582\u2019. Your file will now have approximately 2000 lines in it.  Notice that there is a lot of text that we are not interested in at the moment: page numbers, headers, footers, or categories. We're going to use regular expressions to get rid of them. What we want to end up with is a spreadsheet that is arranged in three columns:  Sender, Recipient, Date   Scroll down through the text; notice there are many lines which don't include a letter, because they're either header info, or blank, or some other extraneous text. We're going to get rid of all of those lines. We want to keep every line that has this information in it: Sender to Recipient, Month, Date, Year, Page", 
            "title": "Getting started"
        }, 
        {
            "location": "/supporting materials/regexex/#the-workflow", 
            "text": "We start by finding every line that looks like a reference to a letter, and put a tilde (a  ~  symbol) at the beginning of it so we know to save it for later. Next, we get rid of all the lines that don't start with tildes, so that we're left with only the relevant text. After this is done, we format the remaining text by putting commas in appropriate places, so we can import it into a spreadsheet and do further edits there.  Step One  Identifying Lines that have Correspondence Senders and Receivers in them  Discussion  Read in full before doing any manipulation of your text!  In Notepad++, press ctrl-f or search- find to open the find dialogue box.  In that box, go to the 'Replace' tab, and check the radio box for 'Regular expression' at the bottom of the search box. In TextWrangler, hit command+f to open the find and replace dialogue box.  Tick off the \u2018grep\u2019 radio button (which tells TextWrangler that we want to do a regex search) and the \u2018wraparound\u2019 button (which tells TextWrangler to search everywhere). In Sublime text, command+f opens the 'find' box (and shift+command+f opens find  and  replace). Tick the '.*' button to tell Sublime we're working with regular expressions.  Remember from our basic introduction that there's a way to see if the word \"to\" appears in full. Type  \\ to\\   in the search bar.  Recall in TextWrangler we would look for  \\bto\\b  instead. This will find every instance of the word \"to\" (and not, for instance, also \u2018potato\u2019 or \u2018tomorrow\u2019). (If you find your text editor doesn't seem to recognize some of these flags, try the others. Sadly, not every text editor implements regular expressions in quite the same way.)   We don't just want to find \"to\", but the entire line that contains it. We assume that every line that contains the word \u201cto\u201d in full is a line that has relevant letter information, and every line that does not is one we do not need. You learned earlier that the query  .+  returns any amount of text, no matter what it says. Assuming you are using Notepad++, if your query is   .+\\ to\\ .+    your search will return every line which includes the word \"to\" in full, no matter what comes before or after it, and none of the lines which don't. In TextWrangler, your query would be  .+\\bto\\b.+.    As mentioned earlier, we want to add a tilde ~ before each of the lines that look like letters, so we can save them for later. This involves the find-and-replace function, and a query identical to the one before, but with parentheses around it, so it looks like   (.+\\ to\\ )  and the entire line is placed within a parenthetical group. In the 'replace' box, enter   ~\\1  which just means replace the line with itself (group 1), placing a tilde before it.    Copy and past some of your text into  RegExr.com . Write your regular expression (ie, what you're trying to find), and your substitution (ie, what you're replace with) in the RegExr interface. Once you're satisfied that you've got it right, run this on your working copy of your text. You might want to save with a new name at each step of this process, so that if something goes wrong, you can recover easily!   Step Two  Removing Lines that Aren\u2019t Relevant  Discussion  After running the find-and-replace, you should note your document now has most of the lines with tildes in front of it, and a few which do not. The next step is to remove all the lines that do not include a tilde. The search string to find all lines which don't begin with tildes is  \\n[^~].+  A  \\n  at the beginning of a query searches for a new line, which means it's going to start searching at the first character of each new line.    However, given the evolution of computing, it may well be that this won\u2019t quite work on your system .   Linux based systems use  \\n  for a new line, while Windows often uses  \\r\\n , and older Macs just use ``` \\r ````. These are the sorts of things that can drive us crazy, and so we digital historians need to keep in mind! Since this will likely cause much frustration, your safest bet will be to save a copy of what you are working on, and then experiment to see what gives you the best result. In most cases, this will be:  \\r\\n[^~].+  Within a set of square brackets  []  the carrot  ^  means search for anything that isn't within these brackets; in this case, the tilde  ~ . The   .+  as before means search for all the rest of the characters in the line as well. All together, the query returns any full line which does not begin with a tilde; that is, the lines we did not mark as looking like letters.   By finding all  \\r\\n[^~].+ and replacing it with nothing, you effectively delete all the lines that don't look like letters. What you're left with is a series of letters, and a series of blank lines.    Devise the regular expression that works on your machine, and replace with nothing. Test on RegExr or on a smaller chunk of your text to make sure it works.   Step Three  Removing the Blank Lines  Discussion  Given what we know about new lines, what do you think a regex would be for a blank line? Try that out in RegExr now. In Notepad++, it'll most likely be  \\n\\r  ; in textwrangler  ^\\r . Do you see what the difference is? Examine the cheat sheets and documentation at RegExr and make a note in your notebook on the difference.   Devise the regular expression to find the blank lines, and replace with nothing. Are you still saving at each successful step?   Step Four  Transforming into CSV format  Discussion  To turn this text file into a spreadsheet, we'll want to separate it out into one column for sender, one for recipient, and one for date, each separated by a single comma. Notice that most lines have extraneous page numbers attached to them; we can get rid of those with regular expressions. There's also usually a comma separating the month-date and the year, which we'll get rid of as well. In the end, the first line should go from looking like:  ~Sam Houston to J. Pinckney Henderson, December 31, 1836 51  to  Sam Houston, J. Pinckney Henderson, December 31 1836  such that each data point is in its own column.  You will start by removing the page number after the year and the comma between the year and the month-date. To do this, first locate the year on each line by using the regex:  [0-9]{4}  As a cheat sheet shows,  [0-9]  finds any digit between 0 and 9, and  {4} will find four of them together. Now extend that search out by appending  .+  to the end of the query; as seen before, it will capture the entire rest of the line. The query   [0-9]{4}.+  will return, for example, \"1836 51\", \"1839 52\", and \"1839 53\" from the first three lines of the text. We also want to capture the comma preceding the year, so add a comma and a space before the query, resulting in   , [0-9]{4}.+  which will return \", 1836 51\", \", 1839 52\", etc.      The next step is making the parenthetical groups which will be used to remove parts of the text with find-and-replace. In this case, we want to remove the comma and everything after year, but not the year or the space before it. Thus our query will look like:  (,)( [0-9]{4})(.+)  with the comma as the first group  \"\\1\" , the space and the year as the second  \"\\2\" , and the rest of the line as the third  \"\\3\" .  Given that all we care about retaining is the second group (we want to keep the year, but not the comma or the page number), what will the  replace  look like?   Find the dates using a regex, and replace so that only the  second  group in the expression is kept. You might want to consult the introduction to regex again before you execute this one.   Step Five  Removing the tildes   Find the tildes that we used to mark off our text of interest, and replace them with nothing to delete them.   Step Six  Separating Senders and Receivers  Discussion  Finally, to separate the sender and recipient by a comma, we find all instances of the word \"to\" and replace it with a comma. Although we used  \\  and  \\ (in TextWrangler,  \\b ) to denote the beginning and end of a word earlier in the lesson, we don't exactly do that here. We include the space preceding \u201cto\u201d in the regular expression, as well as the  \\  ( \\b  in TextWrangler) to denote the word ending. Once we find instances of the word and the space preceding it,  to\\  (nb, remember the space!!!) we replace it with a comma  , .   Devise the regex to find the word, and replace with a comma.   Step Seven  Cleaning up  Discussion  You may notice that some lines still do not fit our criteria. Line 22, for example, reads \"Abner S. Lipscomb, James Hamilton and A. T. Bumley, AugUHt 15, \". It has an incomplete date; these we don't need to worry about for our purposes. More worrisome are lines, like 61 \"Copy and summary of instructions United States Department of State, \" which include none of the information we want. We can get rid of these lines later in a spreadsheet.    The only non-standard lines we need to worry about with regular expressions are the ones with more than 2 commas, like line 178, \"A. J. Donelson, Secretary of State [Allen,. arf interim], December 10 1844\". Notice that our second column, the name of the recipient, has a comma inside of it. If you were to import this directly into a spreadsheet, you would get four columns, one for sender, two for recipient, and one for date, which would break any analysis you would then like to run. Unfortunately these lines need to be fixed by hand, but happily regular expressions make finding them easy. The query:  .+,.+,.+,  will show you every line with more than 2 commas, because it finds any line that has any set of characters, then a comma, then any other set, then another comma, and so forth.    Devise a regex to find these lines with more than 2 commas in them. Fix these lines manually, and then click 'find next' to grab the next one.    At the top of the file, add a new line that simply reads \"Sender, Recipient, Date\". These will be the column headers. Save as  cleaned-correspondence.csv .    Congratulations!  You've now used regex to extract, transform, and clean historical text. As a csv file, you could now load this data into a network analysis program such as  Gephi  to explore the ramifications of this correspondence network. Upload your file to your repository, and make a note of the original location of the file, the transformations that you've done, and the date/time. You will be using your  cleaned-correspondence.csv  file in the next exercise using  Open Refine , where we'll sort out some of the messy OCR (fixing names, and so on).   Video", 
            "title": "The workflow"
        }, 
        {
            "location": "/supporting materials/twarc/", 
            "text": "Twitter Archiving, or TWARC\n\n\nEd Summers\n is a software developer with feet firmly planted in the digital humanities world. He's currently at the Univeristy of Maryland, in the \nMaryland Institute for Technology in the Humanities\n. Recently, he has built a \npython tool\n for searching and grabbing tweets (and all their associated metadata, which is extremely rich), called '\nTwarc\n'.\n\n\nIn this optional exercise, you will install Twarc on your machine and use it to download all of the tweets hashtagged with '#hist3907b'. Once you've seen that it's working, you will then consider Twitter as a place for performing history. Perhaps you'll search for tweets related to a current event (the first draft of history, as the journalists used to call their work), perhaps you'll see what people are saying about a historic event, person, or place. Perhaps you'll try to map out how effective/affective those 'historical' twitter accounts are (like the ones that tweet WWII in 'real-time', or that purport to be the Emperor Hadrian).\n\n\n\n\nGetting set up\n\n\nThe first thing you'll need for this exercise is Python. Python is a programming language heavily used by historians and humanists. You will have noticed that many of the \nProgramming Historian\n tutorials are written in Python. We won't be writing any code; we just need Python on your machine so that your computer understands how to interpret Twarc commands.\n\n\n\n\nDownload and install \nPython\n version 2.7.9. (Twarc will use with Python 2 or 3, but I know it works with 2.7.9 because that's what I have installed on my own machine). Follow all prompts given to you by the installer.\n\n\nMake sure you have \nPip\n installed. \nNB\n Pip \nshould\n be installed when you installed Python - it's included by default in version 2.7.9 or above, so you should be ok (this note is aimed at those of you who already have Python). You can check, on the command line, if pip is installed with this command: \npip list\n. This will list every module you've got installed for Python - if Pip is installed, it'll be in the list.\n\n\nNext thing to do is to get the Twarc files from Summers' repository. Go to \nhttps://github.com/DocNow/twarc\n and download as zip. Unzip somewhere easy-to-get-to on your machine. (As it happens, you can also type \npip install twarc\n from the command line to install it, but it won't give you the utilities folder you'll need later).\n\n\n\n\nTelling Twitter Who You Are\n\n\nTwarc works by interacting with Twitter's API. You need to set up authentication with Twitter, so that Twitter knows who you are when you start pinging them, asking for data. I assume you have a Twitter account? If not, you'll need to sign up for Twitter before you can proceed. Make sure you're logged into Twitter. Then,\n\n\n\n\n\n\nGo to their \ndeveloper page and hit the button marked 'create a new app'\n.\n\n\n\n\n\n\nYou'll be presented with a screen looking rather like this: \n . In 'name' you have to create a unique name for your Twarc - I've used \ntwarc-SMG\n for mine. In 'description' call it, 'twarc for my hist3907b class'. In 'website' give it the URL to your open notebook. Finally, you can leave 'call-back url' blank.\n\n\n\n\n\n\nAgree to terms and conditions, prove you're a human, and click 'create application'.\n\n\n\n\n\n\nYou'll then see a screen that looks like this; here I've already clicked on the 'keys and access tokens' tab: \n\n\n\n\n\n\n\n\n\n\nCopy CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET to a file on your computer; never share this file, don't upload it anywhere, don't put it in your open notebook.\n\n\n\n\nUsing Twarc\n\n\nTwarc will need that information you wrote down in the step above whenever it calls on Twitter. On a Mac, you can create a new file in the same folder where you unzipped Twarc and store your variables as a shell script. Its contents will look like this:\n\n\nexport CONSUMER_KEY=\nblah\n\nexport CONSUMER_SECRET=\nblah\n\nexport ACCESS_TOKEN=\nblah\n\nexport ACCESS_TOKEN_SECRET=\nblah\n\n\n\n\n\n....where \"blah\" is the relevant information (\nwith\n the quotation marks!), and you save this file as \nenviro.sh\n\n\nThen, open a terminal and navigate to your Twarc folder. Type:\n\n\nsource enviro.sh\n\n\nIf all goes well, nothing should appear to happen but for a new command prompt appearing.\n\n\nNon Mac users\n I'm testing solutions for how to do this in other environments. However, Twarc is smart enough that you can also just give it your credentials whenever you run a query, like so:\n\n\ntwarc.py --consumer_key foo --consumer_secret bar --access_token baz --access_token_secret bez --query ferguson\n\n\nso if you are on a mac, and can set the source as above, you don't need to tell twarc your credentials when you run a search. if you are NOT on a mac, you WILL have to do this.\n\n\nA Simple Search\n\n\nThe following would look for tweets concerning Ferguson (that is, tweets with the word or hashtag 'Ferguson', referencing the troubled town in Missouri):\n\n\ntwarc.py --query ferguson \n tweets.json\n\n\n(Keep in mind that PC \n Linux folks: pass your consumer key and secret etc as per the previous section, when you search).\nThis command tells your machine to run the code in the Twarc.py file, which asks Twitter to search and return all tweets with 'ferguson' in them, and then to write them to a file called 'tweets.json'. There might be a lot of tweets; this could take some time. (You might try to collect tweets related to Hist3907b instead, as that will be a \nmuch\n smaller dataset, just to see what's going on.)\n\n\nExtracting Geolocated tweets\n\n\nThere are a number of utilities for Twarc - see \nSummer's documentation here\n. One that you might be interested is the one for grabbing geolocated tweets.\n\n\nYou can output GeoJSON from tweets where geo coordinates are available:\n\n\nutils/geojson.py tweets.json \n tweets.geojson\n\n\nThis command tells your machine to look in the utils subfolder for a file called geojson.py and to run the code therein on your previously created json file with tweets in it. It filters that file and writes just the ones with location data into the geojson format. \n\n\n\n\nInstant Map!\n Github can recognize geojson and represent it on a map. In a new browser window, and assuming that you are logged into Github, go to \ngithub gist\n. Drag and drop your tweets.geojson file onto that browser window. Github will upload it. In the 'gist description' write 'geotagged tweets re ferguson'. Save. Instant, clickable, map!\n\n\n\n\nYour turn\n\n\nUse Twarc to collect a body of tweets that are historically interesting. Extract the geolocated ones and map them. Use some of the utilities to see if there is a difference between 'male' and 'female' tweeters (and how problematic might that be?). Convert your .json file to .csv and upload it to something like \nVoyant\n. Are there trends over time?\n\n\n(Incidentally, if you get an error message when trying to use one of the utilities, read the error message carefully. Is there something missing that the machine is looking for? \nI had such a problem just the other day\n Follow the link to see how to solve).",
            "title": "Twarc"
        }, 
        {
            "location": "/supporting materials/twarc/#twitter-archiving-or-twarc", 
            "text": "Ed Summers  is a software developer with feet firmly planted in the digital humanities world. He's currently at the Univeristy of Maryland, in the  Maryland Institute for Technology in the Humanities . Recently, he has built a  python tool  for searching and grabbing tweets (and all their associated metadata, which is extremely rich), called ' Twarc '.  In this optional exercise, you will install Twarc on your machine and use it to download all of the tweets hashtagged with '#hist3907b'. Once you've seen that it's working, you will then consider Twitter as a place for performing history. Perhaps you'll search for tweets related to a current event (the first draft of history, as the journalists used to call their work), perhaps you'll see what people are saying about a historic event, person, or place. Perhaps you'll try to map out how effective/affective those 'historical' twitter accounts are (like the ones that tweet WWII in 'real-time', or that purport to be the Emperor Hadrian).", 
            "title": "Twitter Archiving, or TWARC"
        }, 
        {
            "location": "/supporting materials/twarc/#getting-set-up", 
            "text": "The first thing you'll need for this exercise is Python. Python is a programming language heavily used by historians and humanists. You will have noticed that many of the  Programming Historian  tutorials are written in Python. We won't be writing any code; we just need Python on your machine so that your computer understands how to interpret Twarc commands.   Download and install  Python  version 2.7.9. (Twarc will use with Python 2 or 3, but I know it works with 2.7.9 because that's what I have installed on my own machine). Follow all prompts given to you by the installer.  Make sure you have  Pip  installed.  NB  Pip  should  be installed when you installed Python - it's included by default in version 2.7.9 or above, so you should be ok (this note is aimed at those of you who already have Python). You can check, on the command line, if pip is installed with this command:  pip list . This will list every module you've got installed for Python - if Pip is installed, it'll be in the list.  Next thing to do is to get the Twarc files from Summers' repository. Go to  https://github.com/DocNow/twarc  and download as zip. Unzip somewhere easy-to-get-to on your machine. (As it happens, you can also type  pip install twarc  from the command line to install it, but it won't give you the utilities folder you'll need later).",
            "title": "Getting set up"
        }, 
        {
            "location": "/supporting materials/twarc/#telling-twitter-who-you-are", 
            "text": "Twarc works by interacting with Twitter's API. You need to set up authentication with Twitter, so that Twitter knows who you are when you start pinging them, asking for data. I assume you have a Twitter account? If not, you'll need to sign up for Twitter before you can proceed. Make sure you're logged into Twitter. Then,    Go to their  developer page and hit the button marked 'create a new app' .    You'll be presented with a screen looking rather like this:   . In 'name' you have to create a unique name for your Twarc - I've used  twarc-SMG  for mine. In 'description' call it, 'twarc for my hist3907b class'. In 'website' give it the URL to your open notebook. Finally, you can leave 'call-back url' blank.    Agree to terms and conditions, prove you're a human, and click 'create application'.    You'll then see a screen that looks like this; here I've already clicked on the 'keys and access tokens' tab:       Copy CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET to a file on your computer; never share this file, don't upload it anywhere, don't put it in your open notebook.", 
            "title": "Telling Twitter Who You Are"
        }, 
        {
            "location": "/supporting materials/twarc/#using-twarc", 
            "text": "Twarc will need that information you wrote down in the step above whenever it calls on Twitter. On a Mac, you can create a new file in the same folder where you unzipped Twarc and store your variables as a shell script. Its contents will look like this:  export CONSUMER_KEY= blah \nexport CONSUMER_SECRET= blah \nexport ACCESS_TOKEN= blah \nexport ACCESS_TOKEN_SECRET= blah   ....where \"blah\" is the relevant information ( with  the quotation marks!), and you save this file as  enviro.sh  Then, open a terminal and navigate to your Twarc folder. Type:  source enviro.sh  If all goes well, nothing should appear to happen but for a new command prompt appearing.  Non Mac users  I'm testing solutions for how to do this in other environments. However, Twarc is smart enough that you can also just give it your credentials whenever you run a query, like so:  twarc.py --consumer_key foo --consumer_secret bar --access_token baz --access_token_secret bez --query ferguson  so if you are on a mac, and can set the source as above, you don't need to tell twarc your credentials when you run a search. if you are NOT on a mac, you WILL have to do this.  A Simple Search  The following would look for tweets concerning Ferguson (that is, tweets with the word or hashtag 'Ferguson', referencing the troubled town in Missouri):  twarc.py --query ferguson   tweets.json  (Keep in mind that PC   Linux folks: pass your consumer key and secret etc as per the previous section, when you search).\nThis command tells your machine to run the code in the Twarc.py file, which asks Twitter to search and return all tweets with 'ferguson' in them, and then to write them to a file called 'tweets.json'. There might be a lot of tweets; this could take some time. (You might try to collect tweets related to Hist3907b instead, as that will be a  much  smaller dataset, just to see what's going on.)  Extracting Geolocated tweets  There are a number of utilities for Twarc - see  Summer's documentation here . One that you might be interested is the one for grabbing geolocated tweets.  You can output GeoJSON from tweets where geo coordinates are available:  utils/geojson.py tweets.json   tweets.geojson  This command tells your machine to look in the utils subfolder for a file called geojson.py and to run the code therein on your previously created json file with tweets in it. It filters that file and writes just the ones with location data into the geojson format.    Instant Map!  Github can recognize geojson and represent it on a map. In a new browser window, and assuming that you are logged into Github, go to  github gist . Drag and drop your tweets.geojson file onto that browser window. Github will upload it. In the 'gist description' write 'geotagged tweets re ferguson'. Save. Instant, clickable, map!   Your turn  Use Twarc to collect a body of tweets that are historically interesting. Extract the geolocated ones and map them. Use some of the utilities to see if there is a difference between 'male' and 'female' tweeters (and how problematic might that be?). Convert your .json file to .csv and upload it to something like  Voyant . Are there trends over time?  (Incidentally, if you get an error message when trying to use one of the utilities, read the error message carefully. Is there something missing that the machine is looking for?  I had such a problem just the other day  Follow the link to see how to solve).", 
            "title": "Using Twarc"
        }, 
        {
            "location": "/supporting materials/cyoa.txt/", 
            "text": "Choose your own adventure!\n\n\nIn this exercise, I want \nyou\n to pick a text or network analysis tool from \nthis list\n or \nthis one\n and figure out how to use it. You can use our version of the \nColonial Newspaper Database\n as your source text. Write up your own basic tutorial explaining what the tool does, how you got it to work on your data, and what you think it might be telling us or be useful for. Fork and add the link to your tutorial (which you'll have as a .md file in your repository) to this document.", 
            "title": "CYOA"
        }, 
        {
            "location": "/supporting materials/cyoa.txt/#choose-your-own-adventure", 
            "text": "In this exercise, I want  you  to pick a text or network analysis tool from  this list  or  this one  and figure out how to use it. You can use our version of the  Colonial Newspaper Database  as your source text. Write up your own basic tutorial explaining what the tool does, how you got it to work on your data, and what you think it might be telling us or be useful for. Fork and add the link to your tutorial (which you'll have as a .md file in your repository) to this document.", 
            "title": "Choose your own adventure!"
        }, 
        {
            "location": "/supporting materials/geoparsing-w-python.txt/", 
            "text": "Geoparsing with Python\n\n\nThis exercise draws from the work of \nFred Gibbs\n\n\n\n\nextract, transform, and save as csv\n\n\nextract geocoded placenames from a text file\n\n\ncreate a kml file with python\n\n\n\n\nIn this exercise, you will need to \nhave Python installed on your machine; download here\n. I have version 2.7.9 on this machine, and know that what follows works with that version.\n\n\nYou should also read and understand Fred Gibbs' tutorial on \ninstalling python modules\n because you will need to install some helper modules.\n\n\nIn module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like \nthis\n.\n\n\n\n\nUse Openrefine to open that csv file. In the same way you tidied up in the \nopenrefine tutorial in module 3\n, clean up this CSV so that you merge together place names appropriately (ie, so that '4ustin' gets merged with 'Austin'). Do this for all the columns.\n\n\nExport the table as a new csv - call it 'cleaned-places.csv'.\n\n\nOpen that csv in your spreadsheet program. Copy and paste all of the columns so that they become a single list. (ie, one column of place names).\n\n\nUsing your spreadsheet's filtering options, see if you can remove any more duplicates. (It might be useful to keep track of how many duplicates you delete, in a new file, eg -\n Texas,200 \n- that kind of information might be handy, as in \nthe mapping texts project\n).\n\n\nSave the file you were removing the duplicates from (which has just a single column of unique place names) as 'placelist.txt'\n\n\n\n\nNow, at this point, we're going to open up our text editor and create a new python program, following Gibbs' \ntutorial\n. His complete script is at the bottom of his post, but make sure you understand everything that is going on. Do you see the places where he has to import new python modules to make his script work? Make sure you've installed those modules. Let's call your completed script 'geoparse.py'.\n\n\nDone that? Good. Open your terminal, navigate to the folder you're working in, and run your script:\n\n\npython geoparse.py\n\n\nDid it work? Did you get an error message? It's entirely possible that you got this message:\n\n\nTraceback (most recent call last):\n  File \ngeolocate.py\n, line 14, in \nmodule\n\n    lat = json['results'][0]['geometry']['location']['lat']\nIndexError: list index out of range\n\n\n\n\n...but check your folder. Do you have a \ngeocoded-places.txt\n file? If so, it worked! Or at least, it got most of your places from the Google maps api. (For the rest, you can try uploading your places list to \nhttp://scargill.inf.ed.ac.uk/geoparser.html\n and then copying and pasting the output to an excel file. This parser will give you several columns of results, where the first column represents its best guess and the other columns other possibilities).\n\n\nYou can now import your geocoded places into many other software packages. Gibbs also shows us how to convert our list into KML, the format that Google Earth and Google Maps can read. \nTry it now\n. You can double-click on the resulting KML file, and if you have Google Earth installed, it will open up there. Within google earth, you can start adding more information, other annotations... pretty soon, you'll have a complete map!\n\n\nRemember to upload your scripts \n data \n obersvations to your open notebook.\n\n\n(Incidentally, if you wanted to load this material into \nPalladio\n you'd need a file that looked like this:\n\n\nPlace   Coordinates\n\nMEXICO  23.634501,-102.552784\n\nCalifornia  36.778261,-119.4179324\n\nBrazos  32.661389,-98.121667\n\n\netc: that is, a tab between 'place' and 'coordinates' in the first line, a tab between 'mexico' and the latitude, and a comma between latitude and logitude. Best way to effect this transformation? Probably using Regex. It's unfortunate that Palladio doesn't accept straightforward place,latitude,longitude comma separated data).", 
            "title": "Geoparsing with Python"
        }, 
        {
            "location": "/supporting materials/geoparsing-w-python.txt/#geoparsing-with-python", 
            "text": "This exercise draws from the work of  Fred Gibbs   extract, transform, and save as csv  extract geocoded placenames from a text file  create a kml file with python   In this exercise, you will need to  have Python installed on your machine; download here . I have version 2.7.9 on this machine, and know that what follows works with that version.  You should also read and understand Fred Gibbs' tutorial on  installing python modules  because you will need to install some helper modules.  In module 3, you used the NER to extract place names from a text. After some further munging with regex, you might have ended up with a CSV that looks like  this .   Use Openrefine to open that csv file. In the same way you tidied up in the  openrefine tutorial in module 3 , clean up this CSV so that you merge together place names appropriately (ie, so that '4ustin' gets merged with 'Austin'). Do this for all the columns.  Export the table as a new csv - call it 'cleaned-places.csv'.  Open that csv in your spreadsheet program. Copy and paste all of the columns so that they become a single list. (ie, one column of place names).  Using your spreadsheet's filtering options, see if you can remove any more duplicates. (It might be useful to keep track of how many duplicates you delete, in a new file, eg -  Texas,200  - that kind of information might be handy, as in  the mapping texts project ).  Save the file you were removing the duplicates from (which has just a single column of unique place names) as 'placelist.txt'   Now, at this point, we're going to open up our text editor and create a new python program, following Gibbs'  tutorial . His complete script is at the bottom of his post, but make sure you understand everything that is going on. Do you see the places where he has to import new python modules to make his script work? Make sure you've installed those modules. Let's call your completed script 'geoparse.py'.  Done that? Good. Open your terminal, navigate to the folder you're working in, and run your script:  python geoparse.py  Did it work? Did you get an error message? It's entirely possible that you got this message:  Traceback (most recent call last):\n  File  geolocate.py , line 14, in  module \n    lat = json['results'][0]['geometry']['location']['lat']\nIndexError: list index out of range  ...but check your folder. Do you have a  geocoded-places.txt  file? If so, it worked! Or at least, it got most of your places from the Google maps api. (For the rest, you can try uploading your places list to  http://scargill.inf.ed.ac.uk/geoparser.html  and then copying and pasting the output to an excel file. This parser will give you several columns of results, where the first column represents its best guess and the other columns other possibilities).  You can now import your geocoded places into many other software packages. Gibbs also shows us how to convert our list into KML, the format that Google Earth and Google Maps can read.  Try it now . You can double-click on the resulting KML file, and if you have Google Earth installed, it will open up there. Within google earth, you can start adding more information, other annotations... pretty soon, you'll have a complete map!  Remember to upload your scripts   data   obersvations to your open notebook.  (Incidentally, if you wanted to load this material into  Palladio  you'd need a file that looked like this:  Place   Coordinates \nMEXICO  23.634501,-102.552784 \nCalifornia  36.778261,-119.4179324 \nBrazos  32.661389,-98.121667  etc: that is, a tab between 'place' and 'coordinates' in the first line, a tab between 'mexico' and the latitude, and a comma between latitude and logitude. Best way to effect this transformation? Probably using Regex. It's unfortunate that Palladio doesn't accept straightforward place,latitude,longitude comma separated data).", 
            "title": "Geoparsing with Python"
        }, 
        {
            "location": "/supporting materials/gephi.txt/", 
            "text": "Working with Gephi\n\n\nBefore we go much further, I would recommend that you look at the work of Clement Levallois, who has a suite of excellent tutorials on working with Gephi at \nhttp://clementlevallois.net/gephi.html\n. The tutorial below is adapted from our open draft of \nThe Macroscope\n.\n\n\nIntroduction\n\n\nGephi is quickly becoming the tool of choice for network analysts who do not need the full suite of algorithms offered by \nPajek\n or \nUCINET\n. It is relatively easy to use (eclipsed in this only by \nNodeXL\n), it is usable on all platforms, it can analyze fairly large networks, and it creates beautiful visualizations. The development community is also extremely active, with improvements being added constantly. We recommend Gephi for the majority of historians undertaking serious network analysis research. Gephi is available at \nhttp://gephi.github.io\n. Download and install it on your machine.\n\n\nWhen Not To Use Networks\n\n\nNetworks analysis can be a powerful method for engaging with a historical dataset but it is often not the most appropriate. Any historical database may be represented as a network with enough contriving but few should be. One could take the Atlantic trade network, including cities, ships, relevant governments and corporations, and connect them all together in a giant multipartite network. It would be extremely difficult to derive any meaning from this network, especially using traditional network analysis methodologies. Clustering coefficients (a useful network metric), for example, would be meaningless in this situation, as it would be impossible for the neighbors of any one node to be neighbors with one another. Network scientists have developed algorithms for \nmultimodal networks\n, but they are often created for fairly specific purposes, and one should be very careful before applying them to a different dataset and using that analysis to explore a historiographical question.\n\n\nNetworks, as they are commonly encoded, also suffer from a profound lack of nuance. It is all well to say that, because Henry Oldenburg corresponded with both Gottfried Leibniz and John Milton, he was the short connection between the two men. However, the encoded network holds no information of whether Oldenburg actually transferred information between the two or whether that connection was at all meaningful. In a flight network, Las Vegas and Atlanta might both have very high betweenness centrality because people from many other cities fly to or from those airports, and yet one is much more of a hub than the other. This is because, largely, people tend to fly directly back and forth from Las Vegas, rather than through it, whereas people tend to fly through Atlanta from one city to another. This is the type of information that network analysis, as it is currently used, is not well equipped to handle. Whether this shortcoming affects a historical inquiry depends primarily on the inquiry itself.\n\n\nWhen deciding whether to use networks to explore a historiographical question, the first question should be: to what extent is connectivity specifically important to this research project? The next should be: can any of the network analyses my collaborators or I know how to employ be specifically useful in the research project? If either answer is negative, another approach is probably warranted.\n\n\nTexan Correspondence\n\n\nYou will need the information you created in Module 3, after cleaning the correspondence data using Open Refine: \n\n\n\n\nIn order to get these data into Gephi, we will have to rename the \u201cSender\u201d column to \u201csource\u201d and the \u201cRecipient\u201d column to \u201ctarget\u201d. In the arrow to the left of Sender in the main OpenRefine window, select Edit column-\nRename this column, and rename the column \u201csource\u201d. Now, in the top right of the window, select \u201cExport-\nCustom tabular exporter.\u201d Notice that \u201csource,\u201d \u201ctarget,\u201d and \u201cDate\u201d are checked in the content tab; uncheck \u201cDate,\u201d as it will not be used in Gephi. Go to the download tab and change the download option from \u201cTab-separated values (TSV)\u201d to \u201cComma-separated values (CSV)\u201d and press download.   The file will likely download to your automatic download directory.\n\n\n\n\n(but see below)\n\n\nQuick instructions for getting the data into Gephi:\n\n\nOpen Gephi by double-clicking its icon. Click \u201cnew project.\u201d The middle pane of the interface window is the \u201cData Laboratory,\u201d where you can interact with your network data in spreadsheet format. This is where we can import the data cleaned up in OpenRefine. \n\n\nIn the Data Laboratory, select \u201cImport Spreadsheet.\u201d Press the ellipsis \u201c...\u201d and locate the CSV you created. Make sure that the Separator is listed as \u201cComma\u201d and the \u201cAs table\u201d is listed as \u201cEdges table.\u201d Press \u201cNext,\u201d then \u201cFinish.\u201d\n\n\nYour data should load up. Click on the \u201coverview\u201d tab and you will be presented with a tangled network graph. Skip down to 'Navigating Gephi'.\n\n\nUmm, I never did manage that openrefine stuff...\n\n\nIn module 3, you used Notepad++ or Textwrangler, regular expressions, and OpenRefine to create a comma-separated value file (*.csv) of the diplomatic correspondence of the Republic of Texas. The final version of the file you created has a row for each letter listed in the volume, and in each row the name of the sender and the recipient. The file should look like this:\n\n\nsource,target\nSam Houston,J. Pinckney Henderson\nJames Webb,Alc6e La Branche\nDavid G. Burnet,Richard G. Dunlap\n\n...\n\n\nThis file is called 'an edge list' - it's a list of connections, or edges, between the individuals. If you no longer have the file, you can find it online \nhere\n. \n\n\nInstalling Gephi on OS 10 Mavericks\n\n\nMac users might have some trouble installing Gephi 0.8 (as I write this, the release of Gephi 0.9 \nshould happen any day\n, and this newer Gephi should solve these problems). We have found that, on Mac OS X Mavericks, Gephi does not load properly after installation. This is a Java-related issue, so you\u2019ll need to install an earlier version of Java than the one provided. To fix this, control click (or right-click) on the Gephi package, and select \u201cshow package contents.\u201d Click on \u201ccontents \n resources \n gephi \n etc.\u201d Control-click (or right-click) on \u201cgephi.conf\u201d and open with your text editor. Find the line reading:\n\n\n#jdkhome=\"/path/to/jdk\"\n\n\nand paste the following underneath:\n\n\njdkhome=\"/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home\n\n\nSave that file. Then, go to \nhttp://support.apple.com/kb/DL1572\n and install the older version of Java (Java 6). Once that is installed, Gephi should run normally.\n\n\nRun Gephi once it is installed. You will be presented with a welcome prompting you to open a recent file, create a new project, or load a sample file. Click \u201cNew Project\u201d and then click the \u201cData Laboratory\u201d tab on the horizontal bar at the top of the Gephi window (Fig. 7.3).\n\n\nNavigating Gephi\n\n\nGephi is broken up into three panes: Overview, Data Laboratory, and Preview. The Overview pane is used to manipulate the visual properties of the network: change colors of nodes or edges, lay them out in different ways, and so forth. The Overview pane is also where you can apply algorithms to the network, like those you learned about in the previous chapter. The Data Laboratory is for adding, manipulating, and removing data. Once your network is as you want it to be, use the Preview pane to do some final tweaks on the look and feel of the network and to export an image for publication.\n\n\nThere is one tweak that needs to done in the Data Table before the dataset is fully ready to be explored in Gephi. Click on the \u201cNodes\u201d tab in the Data Table and notice that, of the three columns, \u201cLabel\u201d (the furthermost field on the right) is blank in every row. This will be a problem when viewing the network visualization, as those labels are essential for the network to be meaningful. \n\n\nIn the \u201cNodes\u201d tab, click \u201cCopy data to other column\u201d at the bottom, select \u201cID\u201d, and press \u201cOk\u201d (Fig. 7.5). Upon doing so, the \u201cLabel\u201d column will be filled with the appropriate labels for each correspondent. While you\u2019re still in the Data Laboratory, look in the \u201cEdges\u201d tab and notice there is a \u201cWeight\u201d column. Gephi automatically counted every time a letter was sent from correspondent A to correspondent B and summed up all the occurrences, resulting in the \u201cWeight.\u201d This means that J. Pinckney Henderson sent three letters to James Webb, because Henderson is in the \u201cSource\u201d column, Webb in the \u201cTarget,\u201d, and the \u201cWeight\u201d is three.\n\n\nClicking on the Overview pane will take you to a visual representation of the network you just imported. In the middle of the screen, you will see your network in the \u201cGraph\u201d tab. The \u201cContext\u201d tab, at the top right, will show that you imported 234 nodes and 394 edges. At first, all the nodes will be randomly strewn across the screen and make little visual sense. Fix this by selecting a layout in the \u201cLayout\u201d tab \u2013 the best one for beginners is \u201cForce Atlas 2.\u201d Press the \u201cRun\u201d button and watch the nodes and edges reorganize on the screen into something slightly more manageable. After the layout runs for a few minutes, re-press the button (now labeled \u201cStop\u201d) to settle the nodes in their place. \n\n\nYou just ran a force-directed layout. Each dot is a correspondent in the network, and lines between dots represent letters sent between individuals. Thicker lines represent more letters, and arrows represent the direction the letters were sent, such that there may be up to two lines connecting any two correspondents (one for each direction).\n\n\nAbout two-dozen smaller components of the network will appear to shoot off into the distance, unconnected from the large, connected component in the middle. For the purpose of this exercise, we are not interested in those disconnected components, so the next step will be to filter them out of the network. The first step is to calculate which components of the network are connected to which others; do this by clicking \u201cRun\u201d next to the text that says \u201cConnected Components\u201d in the \u201cStatistics\u201d tab on the right-hand side.  Once there, select \u201cUnDirected\u201d and press \u201cOK.\u201d Press \u201cClose\u201d when the report pops up indicating that the algorithm has finished running.\n\n\nNow that this is done, Gephi knows which is the giant connected component and has labeled that component \u201c0\u201d. To filter out everything but the giant component, click on the \u201cFilters\u201d tab on the right-hand side and browse to \"Component ID Integer (Node)\" in the folder directory (you\u2019ll find it under \"Attributes,\" then \"Equal\"). Double-click \"Component ID Integer (Node)\" and click the \"Filter\" button at the bottom. Doing this removes the disconnected bundles of nodes.\n\n\nThere are many possible algorithms you could use for the analysis step, but in this case you will use the PageRank of  each node in the network . This measurement calculates the prestige of a correspondent according to how often others write to him or her. The process is circular, such that correspondents with high prestige will confer their prestige on those they write to, who in turn pass their prestige along to their own correspondents. For the moment let us take its results to equate with a correspondent\u2019s importance in the Republic of Texas letter network.\n\n\nCalculate the PageRank by clicking on the \"Run\" button next to \"PageRank\" in the \"Statistics\" tab. You will be presented with a prompt asking for a few parameters; make sure \"Directed\" network is selected and that the algorithm is taking edge weight into account (by selecting \"Use edge weight\"). Leave all other parameters at their default. Press \"OK\".\n\n\nOnce PageRank is calculated, if you click back into the \"Data Laboratory\" and select the \"Nodes\" list in the Data Table, you can see that a new \"PageRank\" column has been added, with values for every node. The higher the PageRank, the more central a correspondent is in the network. Going back to the Overview pane, you can visualize this centrality by changing the size of each correspondent\u2019s node based on its PageRank. Do this in the \"Ranking\" tab on the left side of the Overview pane.\n\n\nMake sure \"Nodes\" is selected, press the icon of a little red diamond, and select PageRank from the drop-down menu. In the parameter options just below, enter the \"Min size\" as 1 and the \"Max size\" as 10. Press \"Apply,\" and watch the nodes resize based on their PageRank. To be on the safe side and decrease clutter, re-run the \"Force Atlas 2\" layout as described above, making sure to keep the \"Prevent Overlap\" box checked.\n\n\nAt this point, the network is processed enough to visualize in the Preview pane, to finally begin making sense of the data. In Preview, on the left-hand side, select \"Show Labels,\" \"Proportional Size,\" \"Rescale Weight,\" and deselect \"Curved\" edges. Press \"Refresh.\" \n\n\nSo what have we got?\n\n\nThe visualization immediately reveals apparent structure: central figures on the top (Ashbel Smith and Anson Jones) and bottom (Mirabeau B. Lamar, James Webb, and James Treat), and two central figures who connect the two split communities (James Hamilton and James S. Mayfield). A quick search online shows the top network to be associated with the last president of the Republic of Texas, Anson Jones; whereas the bottom network largely revolves around the second president, Mirabeau Lamar. Experts on this period in history could use this analysis to understand the structure of communities building to the annexation of Texas or they could ask meta-questions about the nature of the data themselves. For example, why is Sam Houston, the first and third president of the Republic of Texas, barely visible in this network?\n\n\nWrite up your own observations on this process in your open notebook, and export your gephi file as a .graphml file (because Gephi's \n.gephi\n format is a bit unstable, always save as or export your work in a variety of formats). Upload that to your repository.", 
            "title": "SNA with Gephi"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#working-with-gephi", 
            "text": "Before we go much further, I would recommend that you look at the work of Clement Levallois, who has a suite of excellent tutorials on working with Gephi at  http://clementlevallois.net/gephi.html . The tutorial below is adapted from our open draft of  The Macroscope .", 
            "title": "Working with Gephi"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#introduction", 
            "text": "Gephi is quickly becoming the tool of choice for network analysts who do not need the full suite of algorithms offered by  Pajek  or  UCINET . It is relatively easy to use (eclipsed in this only by  NodeXL ), it is usable on all platforms, it can analyze fairly large networks, and it creates beautiful visualizations. The development community is also extremely active, with improvements being added constantly. We recommend Gephi for the majority of historians undertaking serious network analysis research. Gephi is available at  http://gephi.github.io . Download and install it on your machine.", 
            "title": "Introduction"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#when-not-to-use-networks", 
            "text": "Networks analysis can be a powerful method for engaging with a historical dataset but it is often not the most appropriate. Any historical database may be represented as a network with enough contriving but few should be. One could take the Atlantic trade network, including cities, ships, relevant governments and corporations, and connect them all together in a giant multipartite network. It would be extremely difficult to derive any meaning from this network, especially using traditional network analysis methodologies. Clustering coefficients (a useful network metric), for example, would be meaningless in this situation, as it would be impossible for the neighbors of any one node to be neighbors with one another. Network scientists have developed algorithms for  multimodal networks , but they are often created for fairly specific purposes, and one should be very careful before applying them to a different dataset and using that analysis to explore a historiographical question.  Networks, as they are commonly encoded, also suffer from a profound lack of nuance. It is all well to say that, because Henry Oldenburg corresponded with both Gottfried Leibniz and John Milton, he was the short connection between the two men. However, the encoded network holds no information of whether Oldenburg actually transferred information between the two or whether that connection was at all meaningful. In a flight network, Las Vegas and Atlanta might both have very high betweenness centrality because people from many other cities fly to or from those airports, and yet one is much more of a hub than the other. This is because, largely, people tend to fly directly back and forth from Las Vegas, rather than through it, whereas people tend to fly through Atlanta from one city to another. This is the type of information that network analysis, as it is currently used, is not well equipped to handle. Whether this shortcoming affects a historical inquiry depends primarily on the inquiry itself.  When deciding whether to use networks to explore a historiographical question, the first question should be: to what extent is connectivity specifically important to this research project? The next should be: can any of the network analyses my collaborators or I know how to employ be specifically useful in the research project? If either answer is negative, another approach is probably warranted.", 
            "title": "When Not To Use Networks"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#texan-correspondence", 
            "text": "You will need the information you created in Module 3, after cleaning the correspondence data using Open Refine:    In order to get these data into Gephi, we will have to rename the \u201cSender\u201d column to \u201csource\u201d and the \u201cRecipient\u201d column to \u201ctarget\u201d. In the arrow to the left of Sender in the main OpenRefine window, select Edit column- Rename this column, and rename the column \u201csource\u201d. Now, in the top right of the window, select \u201cExport- Custom tabular exporter.\u201d Notice that \u201csource,\u201d \u201ctarget,\u201d and \u201cDate\u201d are checked in the content tab; uncheck \u201cDate,\u201d as it will not be used in Gephi. Go to the download tab and change the download option from \u201cTab-separated values (TSV)\u201d to \u201cComma-separated values (CSV)\u201d and press download.   The file will likely download to your automatic download directory.   (but see below)", 
            "title": "Texan Correspondence"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#quick-instructions-for-getting-the-data-into-gephi", 
            "text": "Open Gephi by double-clicking its icon. Click \u201cnew project.\u201d The middle pane of the interface window is the \u201cData Laboratory,\u201d where you can interact with your network data in spreadsheet format. This is where we can import the data cleaned up in OpenRefine.   In the Data Laboratory, select \u201cImport Spreadsheet.\u201d Press the ellipsis \u201c...\u201d and locate the CSV you created. Make sure that the Separator is listed as \u201cComma\u201d and the \u201cAs table\u201d is listed as \u201cEdges table.\u201d Press \u201cNext,\u201d then \u201cFinish.\u201d  Your data should load up. Click on the \u201coverview\u201d tab and you will be presented with a tangled network graph. Skip down to 'Navigating Gephi'.  Umm, I never did manage that openrefine stuff...  In module 3, you used Notepad++ or Textwrangler, regular expressions, and OpenRefine to create a comma-separated value file (*.csv) of the diplomatic correspondence of the Republic of Texas. The final version of the file you created has a row for each letter listed in the volume, and in each row the name of the sender and the recipient. The file should look like this:  source,target\nSam Houston,J. Pinckney Henderson\nJames Webb,Alc6e La Branche\nDavid G. Burnet,Richard G. Dunlap ...  This file is called 'an edge list' - it's a list of connections, or edges, between the individuals. If you no longer have the file, you can find it online  here .   Installing Gephi on OS 10 Mavericks  Mac users might have some trouble installing Gephi 0.8 (as I write this, the release of Gephi 0.9  should happen any day , and this newer Gephi should solve these problems). We have found that, on Mac OS X Mavericks, Gephi does not load properly after installation. This is a Java-related issue, so you\u2019ll need to install an earlier version of Java than the one provided. To fix this, control click (or right-click) on the Gephi package, and select \u201cshow package contents.\u201d Click on \u201ccontents   resources   gephi   etc.\u201d Control-click (or right-click) on \u201cgephi.conf\u201d and open with your text editor. Find the line reading:  #jdkhome=\"/path/to/jdk\"  and paste the following underneath:  jdkhome=\"/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home  Save that file. Then, go to  http://support.apple.com/kb/DL1572  and install the older version of Java (Java 6). Once that is installed, Gephi should run normally.  Run Gephi once it is installed. You will be presented with a welcome prompting you to open a recent file, create a new project, or load a sample file. Click \u201cNew Project\u201d and then click the \u201cData Laboratory\u201d tab on the horizontal bar at the top of the Gephi window (Fig. 7.3).", 
            "title": "Quick instructions for getting the data into Gephi:"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#navigating-gephi", 
            "text": "Gephi is broken up into three panes: Overview, Data Laboratory, and Preview. The Overview pane is used to manipulate the visual properties of the network: change colors of nodes or edges, lay them out in different ways, and so forth. The Overview pane is also where you can apply algorithms to the network, like those you learned about in the previous chapter. The Data Laboratory is for adding, manipulating, and removing data. Once your network is as you want it to be, use the Preview pane to do some final tweaks on the look and feel of the network and to export an image for publication.  There is one tweak that needs to done in the Data Table before the dataset is fully ready to be explored in Gephi. Click on the \u201cNodes\u201d tab in the Data Table and notice that, of the three columns, \u201cLabel\u201d (the furthermost field on the right) is blank in every row. This will be a problem when viewing the network visualization, as those labels are essential for the network to be meaningful.   In the \u201cNodes\u201d tab, click \u201cCopy data to other column\u201d at the bottom, select \u201cID\u201d, and press \u201cOk\u201d (Fig. 7.5). Upon doing so, the \u201cLabel\u201d column will be filled with the appropriate labels for each correspondent. While you\u2019re still in the Data Laboratory, look in the \u201cEdges\u201d tab and notice there is a \u201cWeight\u201d column. Gephi automatically counted every time a letter was sent from correspondent A to correspondent B and summed up all the occurrences, resulting in the \u201cWeight.\u201d This means that J. Pinckney Henderson sent three letters to James Webb, because Henderson is in the \u201cSource\u201d column, Webb in the \u201cTarget,\u201d, and the \u201cWeight\u201d is three.  Clicking on the Overview pane will take you to a visual representation of the network you just imported. In the middle of the screen, you will see your network in the \u201cGraph\u201d tab. The \u201cContext\u201d tab, at the top right, will show that you imported 234 nodes and 394 edges. At first, all the nodes will be randomly strewn across the screen and make little visual sense. Fix this by selecting a layout in the \u201cLayout\u201d tab \u2013 the best one for beginners is \u201cForce Atlas 2.\u201d Press the \u201cRun\u201d button and watch the nodes and edges reorganize on the screen into something slightly more manageable. After the layout runs for a few minutes, re-press the button (now labeled \u201cStop\u201d) to settle the nodes in their place.   You just ran a force-directed layout. Each dot is a correspondent in the network, and lines between dots represent letters sent between individuals. Thicker lines represent more letters, and arrows represent the direction the letters were sent, such that there may be up to two lines connecting any two correspondents (one for each direction).  About two-dozen smaller components of the network will appear to shoot off into the distance, unconnected from the large, connected component in the middle. For the purpose of this exercise, we are not interested in those disconnected components, so the next step will be to filter them out of the network. The first step is to calculate which components of the network are connected to which others; do this by clicking \u201cRun\u201d next to the text that says \u201cConnected Components\u201d in the \u201cStatistics\u201d tab on the right-hand side.  Once there, select \u201cUnDirected\u201d and press \u201cOK.\u201d Press \u201cClose\u201d when the report pops up indicating that the algorithm has finished running.  Now that this is done, Gephi knows which is the giant connected component and has labeled that component \u201c0\u201d. To filter out everything but the giant component, click on the \u201cFilters\u201d tab on the right-hand side and browse to \"Component ID Integer (Node)\" in the folder directory (you\u2019ll find it under \"Attributes,\" then \"Equal\"). Double-click \"Component ID Integer (Node)\" and click the \"Filter\" button at the bottom. Doing this removes the disconnected bundles of nodes.  There are many possible algorithms you could use for the analysis step, but in this case you will use the PageRank of  each node in the network . This measurement calculates the prestige of a correspondent according to how often others write to him or her. The process is circular, such that correspondents with high prestige will confer their prestige on those they write to, who in turn pass their prestige along to their own correspondents. For the moment let us take its results to equate with a correspondent\u2019s importance in the Republic of Texas letter network.  Calculate the PageRank by clicking on the \"Run\" button next to \"PageRank\" in the \"Statistics\" tab. You will be presented with a prompt asking for a few parameters; make sure \"Directed\" network is selected and that the algorithm is taking edge weight into account (by selecting \"Use edge weight\"). Leave all other parameters at their default. Press \"OK\".  Once PageRank is calculated, if you click back into the \"Data Laboratory\" and select the \"Nodes\" list in the Data Table, you can see that a new \"PageRank\" column has been added, with values for every node. The higher the PageRank, the more central a correspondent is in the network. Going back to the Overview pane, you can visualize this centrality by changing the size of each correspondent\u2019s node based on its PageRank. Do this in the \"Ranking\" tab on the left side of the Overview pane.  Make sure \"Nodes\" is selected, press the icon of a little red diamond, and select PageRank from the drop-down menu. In the parameter options just below, enter the \"Min size\" as 1 and the \"Max size\" as 10. Press \"Apply,\" and watch the nodes resize based on their PageRank. To be on the safe side and decrease clutter, re-run the \"Force Atlas 2\" layout as described above, making sure to keep the \"Prevent Overlap\" box checked.  At this point, the network is processed enough to visualize in the Preview pane, to finally begin making sense of the data. In Preview, on the left-hand side, select \"Show Labels,\" \"Proportional Size,\" \"Rescale Weight,\" and deselect \"Curved\" edges. Press \"Refresh.\"", 
            "title": "Navigating Gephi"
        }, 
        {
            "location": "/supporting materials/gephi.txt/#so-what-have-we-got", 
            "text": "The visualization immediately reveals apparent structure: central figures on the top (Ashbel Smith and Anson Jones) and bottom (Mirabeau B. Lamar, James Webb, and James Treat), and two central figures who connect the two split communities (James Hamilton and James S. Mayfield). A quick search online shows the top network to be associated with the last president of the Republic of Texas, Anson Jones; whereas the bottom network largely revolves around the second president, Mirabeau Lamar. Experts on this period in history could use this analysis to understand the structure of communities building to the annexation of Texas or they could ask meta-questions about the nature of the data themselves. For example, why is Sam Houston, the first and third president of the Republic of Texas, barely visible in this network?  Write up your own observations on this process in your open notebook, and export your gephi file as a .graphml file (because Gephi's  .gephi  format is a bit unstable, always save as or export your work in a variety of formats). Upload that to your repository.", 
            "title": "So what have we got?"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/", 
            "text": "Graphing the Net\n\n\nIt may be that you are interested in the structure of links on the internet. Perhaps you'd like to see how a particular topic plays out across Wikiepedia. What you'd have to do is a \nweb-crawl\n. Here are quick instructions for setting up a webcrawl, which will follow every link on a page to a particular depth, and echo the results through your browser into Gephi for visualization and analysis.\n\n\nYou will need:\n\n\n\n\nThe Chrome browser with \nSite Spider Mk II\n installed\n\n\nGephi \n\n\nHTTP Graph Generator Plugin for Gephi installed\n\n\n\n\nNB\n You can install the graph generator plugin from \nwithin\n Gephi by selecting 'Tools' on the main menu ribbon at the top of the screen (and \nnot\n the 'plugins' item). Then, within 'Tools', select 'plugins' and then 'available plugins'. Search for 'HTTPGraph'. Tick off the box and install it. When finished, close and restart Gephi with a new project.\n\n\nGetting set up to scrape\n\n\nIn Chrome, go to the settings page (the 'hamburg' icon at the extreme right of the address bar, or by typing \nchrome://settings/\n in the address bar. Click on 'show advanced settings' at the bottom of the page. Then, scroll down to 'Network' and click on 'change proxy settings'. \n\n\nIn the popup that opens, click the 'connections' tab, and then the 'LAN Settings' button. \nAnother\n popup will open. Select the 'Use a proxy server for your LAN'. Enter:\n\n\n127.0.0.1\n for the address, and \n8088\n for the port. Now, whenever you go to a website, the info will be echoed through that port. We need to set Gephi up to hear what's being passed.\n\n\nOpen Gephi. Start a new project. Go to 'file' then 'generate' then 'http graph'. A pop up will open, asking you to specify a port. Input \n8088\n. Accept the defaults, and press OK. On the overview panel, nodes will begin to appear when you go to a URL in Chrome. \n\n\nBegin the scrape\n\n\nGo back to Chrome. Put in the URL that you want to start your scrape on, eg \nhttp://en.wikipedia.org/wiki/Archaeology\n. Then click the SiteSpider II button in your toolbar. Site spider will open a popup asking you how far and how deep you want to scrape; set the parameters accordingly. Hit 'go' and then flip over to your gephi window. You'll see the network begin to populate! Let it run as long as you want. When you're finished, save your network by 'exporting' it (on the file menu) as any other format than .gephi. I say this because I find sometimes the .gephi format is unstable. You can then filter your network for resources or html pages or what have you. I suggest deleting the node labelled 127.0.0.1 because that's your computer and it will throw off any metrics you choose to calculate. \n\n\nWhat does it all mean?\n\n\nWell, that depends.\n\n\nFor an example of why you might want to do all this, and what you might find, see \n'Shouting Into the Void?'\n a piece where I tried to understand the shape of the archaeological web.", 
            "title": "Graphing the Net"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#graphing-the-net", 
            "text": "It may be that you are interested in the structure of links on the internet. Perhaps you'd like to see how a particular topic plays out across Wikiepedia. What you'd have to do is a  web-crawl . Here are quick instructions for setting up a webcrawl, which will follow every link on a page to a particular depth, and echo the results through your browser into Gephi for visualization and analysis.", 
            "title": "Graphing the Net"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#you-will-need", 
            "text": "The Chrome browser with  Site Spider Mk II  installed  Gephi   HTTP Graph Generator Plugin for Gephi installed   NB  You can install the graph generator plugin from  within  Gephi by selecting 'Tools' on the main menu ribbon at the top of the screen (and  not  the 'plugins' item). Then, within 'Tools', select 'plugins' and then 'available plugins'. Search for 'HTTPGraph'. Tick off the box and install it. When finished, close and restart Gephi with a new project.", 
            "title": "You will need:"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#getting-set-up-to-scrape", 
            "text": "In Chrome, go to the settings page (the 'hamburg' icon at the extreme right of the address bar, or by typing  chrome://settings/  in the address bar. Click on 'show advanced settings' at the bottom of the page. Then, scroll down to 'Network' and click on 'change proxy settings'.   In the popup that opens, click the 'connections' tab, and then the 'LAN Settings' button.  Another  popup will open. Select the 'Use a proxy server for your LAN'. Enter:  127.0.0.1  for the address, and  8088  for the port. Now, whenever you go to a website, the info will be echoed through that port. We need to set Gephi up to hear what's being passed.  Open Gephi. Start a new project. Go to 'file' then 'generate' then 'http graph'. A pop up will open, asking you to specify a port. Input  8088 . Accept the defaults, and press OK. On the overview panel, nodes will begin to appear when you go to a URL in Chrome.", 
            "title": "Getting set up to scrape"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#begin-the-scrape", 
            "text": "Go back to Chrome. Put in the URL that you want to start your scrape on, eg  http://en.wikipedia.org/wiki/Archaeology . Then click the SiteSpider II button in your toolbar. Site spider will open a popup asking you how far and how deep you want to scrape; set the parameters accordingly. Hit 'go' and then flip over to your gephi window. You'll see the network begin to populate! Let it run as long as you want. When you're finished, save your network by 'exporting' it (on the file menu) as any other format than .gephi. I say this because I find sometimes the .gephi format is unstable. You can then filter your network for resources or html pages or what have you. I suggest deleting the node labelled 127.0.0.1 because that's your computer and it will throw off any metrics you choose to calculate.", 
            "title": "Begin the scrape"
        }, 
        {
            "location": "/supporting materials/graphing-the-net.txt/#what-does-it-all-mean", 
            "text": "Well, that depends.  For an example of why you might want to do all this, and what you might find, see  'Shouting Into the Void?'  a piece where I tried to understand the shape of the archaeological web.", 
            "title": "What does it all mean?"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/", 
            "text": "Transforming 2-mode network data to 1-mode\n\n\nNetworks can be composed of all sorts of things. Trains, busses, Uber, metro - all of these can be combined into a 'transportation' network. Books, authors, editors, funders, censors, sponsors - a publishing network. Students, profs, classes, universities - an education network. Anytime you have more than one kind of \nthing\n (however defined) in your network, you formally have a bipartite (or tripartite, and so on, depending on the number of things) network. Visualizing such a network as nodes and edges can be a useful heuristic exercise. \n\n\nBut.\n\n\nIf you are doing this in Gephi, and you run some of Gephi's metrics (statistics) on a bipartite network, your results may not mean what you think they mean. The metrics, the algorithms assume 1-mode networks. Thus, running them on 2-mode (or more!) networks will result in ....issues.\n\n\n\n\nNow in truth, it's not quite so clear-cut to simply say, 'convert every bimodal network to unimodal before running any stats' (see \nScott's discussion here\n), but as a rule of thumb for when you're getting started, you'll be on much firmer methodological grounds if you transform your mutlimodal networks into a series of 1-mode networks. So convert every bimodal network to unimodal before running any statistics on your network(s).\n\n\nThus, this network: \n\nProfA -\n student1   (where -\n is a directed relationship, 'teaches')\n\nProfA -\n student2 \n\nProfB -\n student3 \n\nProbB -\n student1 \n\n\n...can be transformed into \ntwo\n networks, one where profA is connected to profB by virtue of a shared student (student1). That is, profs connected by students; and the inverse: students connected by profs. You could then run your metrics, and get two different perspectives on the educational dynamics of this particular university.\n\n\nIn this exercise, you'll transform a network of women and social organizations into two 1-mode networks.\n\n\nThe data\n\n\nThe data for this exercise comes from a former Carleton public history MA student, Peter Holdsworth. Peter lodged \na copy of his MA research on Figshare\n. Peter was interested in the social networks surrounding ideas of commemoration of the centenerary of the War of 1812, in 1912. He studied the membership rolls for women\u2019s service organization in Ontario both before and after that centenerary. By making his data public, Peter enables others to build upon his own research in a way not commonly done in history. (Peter can be followed on Twitter at \nhttps://twitter.com/P_W_Holdsworth\n).\n\n\nRight-click and 'save link' to get \nthe data files you'll need for this exercise\n\n\nConfiguring Gephi\n\n\nThere is a plugin for Gephi that we will use to transform our network. Open Gephi. Across the top of Gephi in the menu ribbon you\u2019ll see \nFile Workspace View Tools Window Plugins Help\n. \n\n\nTo get and install the plugin, select \nTools \n Plugins\n (The top level menu item 'Plugins' is empty and not used - a useful reminder that Gephi is still in \nbeta\n).\n\n\nIn the popup, under \u2018available plugins\u2019 look for \u2018MultimodeNetworksTransformation\u2019. Tick this box, then click on Install. Follow the instructions, ignore any warnings, click on \u2018finish\u2019. You may or may not need to restart Gephi to get the plugin running. If you suddenly see on the far right of ht Gephi window a new tab besid \u2018statistics\u2019, \u2018filters\u2019, called \u2018Multimode Network\u2019, then you\u2019re ok.\n\n\n\n\nImporting the data\n\n\n\n\nUnder \u2018file\u2019, select -\n New project.\n\n\nOn the data  laboratory tab, select Import-spreadsheet, and in the pop-up, make sure to select under \u2018as table: EDGES table. Select women-orgs.csv.  Click \u2018next\u2019, click finish.\n\n\n\n\nOn the data table, have \u2018edges\u2019 selected. This is showing you the source and the target for each link (aka \u2018edge\u2019). This implies a directionality to the relationship that we just don\u2019t know \u2013 so down below, when we get to statistics, we will always have to make sure to tell Gephi that we want the network treated as \u2018undirected\u2019. More on that below.\n\n\n\n\nLoading your csv file, step 1.\n\n\n\n\nLoading your CSV file, step 2\n\n\n\n\n\n\nClick on \u2018copy data to other column\u2019. Select \u2018Id\u2019. In the pop-up, select \u2018Label\u2019. You now have your edges labelled.\n\n\n\n\n\n\nJust as you did above, now import NODES (Women-names.csv)\n\n\n\n\nmaking sure you're on the Nodes page in the data laboratory, copy ID to Label to that your nodes are labelled.\n\n\n\n\n(nb. You can always add more attribute data to your network this way, as long as you always use a column called Id so that Gephi knows where to slot the new information. Make sure to never tick off the box labeled \u2018force nodes to be created as new ones\u2019.)\n\n\nPrepping your data\n\n\nWe're now going to manipulate the data a bit in order to get it ready for the transformation. In this data, we have women, and we have organizations. We need to tell Gephi which rows are the organizations. In Peter's original data input phase, he decided to use a unique number for each woman so that he would minimize data entry errors (misspellings and so on), as well as control for the use of maiden and married names. Miss Eliza Smith might become, at a later date, Mrs. George Doe. In Peter's scheme, this is the same person (remember in module 3 in the TEI exercise how we dealt with such issues?).\n\n\n\n\nOn your NODES page in the data laboratory, add new column, make it boolean. Call it \u2018organization\u2019\n\n\n\n\n\n\n\n\nIn the Filter box, type [a-z], and select Id \u2013 this filters out all the women. (What would you have to do to filter out the organizations? Remember, this is a regex search!)\n\n\nTick off the check boxes in the \u2018organization\u2019 columns.\n\n\n\n\n\n\n(I note a typo in the image above, 'a-b'. that should be, 'a-z').\n\n\nSave this as \nwomen-organizations-2-mode.gephi\n.\n\n\nPro tip\n: always export your data from gephi (file \n export) in .net or .graphml or .gefx format as the .gephi format (which is your only option under file \n save as) is unstable. That is, sometimes gephi won't read .gephi files! I did say this was \nbeta\n software).\n\n\nTransforming the network\n\n\nAt this point, you have a two mode network in gephi. You could click on the 'overview' panel and play with some of the layouts and begin to form impressions about the nature of your data. Remember though any metrics calculated at this point would be largely spurious. Let's transform this two-mode network so that we can explore how women are connected to other women via shared membership.\n\n\n\n\nOn the multimode networks projection tab,\n+ click load attributes.\n+ in \u2018attribute type\u2019, select organization\n+ in left matrix, select \u2018false \u2013 true\u2019 (or \u2018null \u2013 true\u2019)\n+ in right matrix, select \u2018true \u2013 false\u2019. (or \u2018true \u2013 null\u2019) (do you see why this is the case? what would selecting the inverse accomplish?)\n+ select \u2018remove edges\u2019 and \u2018remove nodes\u2019.\n+ Once you hit \u2018run\u2019, organizations will be removed from your bipartite network, leaving you with a single-mode network. hit \u2018run\u2019.\n+ save as \nwomen-to-women-network.gephi\n \nand\n export as \nwomen-to-women.net\n\n\nNB\n if your nodes data table is blank, your filter might still be active. make sure the filter box is clear. You should be left with a list of women (ie, a list of nodes where the identiers are numbers, per Peter's schema).\n\n\nAt this point, you could re-start Gephi and reload your \u2018women-organizations-2-mode.gephi\u2019 file and re-run the multimode networks projection so that you are left with an organization to organization network. Do this, and save and export with appropriate file names.\n\n\nExploring this network\n\n\nPeter's data has a number of \nattributes\n describing it, including the membership year. So let's see what this network of women looks like in 1902.\n\n\n\n\nUnder the filters tab at the right hand side of the Gephi interface, select \u2018attributes \u2013 equal\u2019 and then drag \u20181902\u2019 to the queries box.\n\n\nIn \u2018pattern\u2019 enter [0-9] and tick the \u2018use regex\u2019 box.\n\n\nclick ok, click \u2018filter\u2019.\n\n\n\n\nYou should now have a network with 188 nodes and 8728 edges, showing the women who were active in 1902.\n\n\nLet\u2019s learn something about this network. Under the statistics tab at the right hand side of the Gephi interface,\n\n\n\n\n\n\nRun \u2018avg. path length\u2019 by clicking on \u2018run\u2019\n\n\n\n\n\n\nIn the pop up that opens, select \u2018undirected\u2019 (as we know nothing about directionality in this network; we simply know that two women were members of the same organization at the same time. Note also that if the same pair were members of the more than one organziation, the weight of their connection will be corresponding stronger).\n\n\n\n\n\n\nclick ok.\n\n\n\n\n\n\nrun \u2018modularity\u2019 to look for subgroups. make sure \u2018randomize\u2019 and \u2018use weights\u2019 are selected. Leave \u2018resolution\u2019 at 1.0\n\n\n\n\n\n\nWe selected 'average path length' because one of the byproducts of this routine is 'betweeness centrality'. We're making an assumption here that a woman who has a high betweeness centrality score was in a position to affect information flow in 1902 society. Modularity looks at similar patterns of connections to cluster women who have more-or-less similar connections into groups.\n\n\nLet\u2019s visualize what we\u2019ve just learned.\n\n\n\n\nOn the \u2018partition\u2019 tab, over on the left hand side of the \u2018overview\u2019 screen, click on nodes, then click the green arrows beside \u2018choose a partition parameter\u2019.\n\n\nClick on \u2018choose a partition parameter\u2019. Scroll down to modularity class. The different groups will be listed, with their colours and their % composition of the network.\n\n\nHit \u2018apply\u2019 to recolour your network graph.\n\n\n\n\nLet\u2019s resize the nodes to show off betweeness-centrality (to figure out which woman was in the greatest position to influence flows of information in this network.) \n\n\n\n\nClick \u2018ranking\u2019. (It's on the left hand side of the interface, beside 'partition' and just below 'overview'.\n\n\nClick \u2018nodes\u2019.\n\n\nClick the down arrow on \u2018choose a rank parameter\u2019. Select \u2018betweeness centrality\u2019.\n\n\nClick the red diamond. This will resize the nodes according to their \u2018betweeness centrality\u2019.\n\n\nClick \u2018apply\u2019.\n\n\n\n\nNow, down at the bottom of the middle panel, you can click the large black \u2018T\u2019 to display labels. Do so. Click the black letter \u2018A\u2019 and select \u2018node size\u2019.\n\n\nMrs. Mary Elliot-Murray-Kynynmound and Mrs. John Henry Wilson should now dominate your network (if you go back to the original data zip, you'll be able to find Peter's key to figure out who's who). Who were these ladies? What organizations were they members of? Who were they connected to? To the archives!\n\n\nCongratulations! You\u2019ve imported historical network data into Gephi, transformed it, manipulated it, and run some analyses. Play with the settings on \u2018preview\u2019 in order to share your visualization as svg, pdf, or png.\n\n\nNow go back to your original gephi file, and recast it as organizations to organizations via shared members, to figure out which organizations were key in early 20th century Ontario\u2026 make appropriate notes in your open notebook.", 
            "title": "Multimode Networks"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#transforming-2-mode-network-data-to-1-mode", 
            "text": "Networks can be composed of all sorts of things. Trains, busses, Uber, metro - all of these can be combined into a 'transportation' network. Books, authors, editors, funders, censors, sponsors - a publishing network. Students, profs, classes, universities - an education network. Anytime you have more than one kind of  thing  (however defined) in your network, you formally have a bipartite (or tripartite, and so on, depending on the number of things) network. Visualizing such a network as nodes and edges can be a useful heuristic exercise.   But.  If you are doing this in Gephi, and you run some of Gephi's metrics (statistics) on a bipartite network, your results may not mean what you think they mean. The metrics, the algorithms assume 1-mode networks. Thus, running them on 2-mode (or more!) networks will result in ....issues.   Now in truth, it's not quite so clear-cut to simply say, 'convert every bimodal network to unimodal before running any stats' (see  Scott's discussion here ), but as a rule of thumb for when you're getting started, you'll be on much firmer methodological grounds if you transform your mutlimodal networks into a series of 1-mode networks. So convert every bimodal network to unimodal before running any statistics on your network(s).  Thus, this network:  \nProfA -  student1   (where -  is a directed relationship, 'teaches') \nProfA -  student2  \nProfB -  student3  \nProbB -  student1   ...can be transformed into  two  networks, one where profA is connected to profB by virtue of a shared student (student1). That is, profs connected by students; and the inverse: students connected by profs. You could then run your metrics, and get two different perspectives on the educational dynamics of this particular university.  In this exercise, you'll transform a network of women and social organizations into two 1-mode networks.", 
            "title": "Transforming 2-mode network data to 1-mode"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#the-data", 
            "text": "The data for this exercise comes from a former Carleton public history MA student, Peter Holdsworth. Peter lodged  a copy of his MA research on Figshare . Peter was interested in the social networks surrounding ideas of commemoration of the centenerary of the War of 1812, in 1912. He studied the membership rolls for women\u2019s service organization in Ontario both before and after that centenerary. By making his data public, Peter enables others to build upon his own research in a way not commonly done in history. (Peter can be followed on Twitter at  https://twitter.com/P_W_Holdsworth ).  Right-click and 'save link' to get  the data files you'll need for this exercise", 
            "title": "The data"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#configuring-gephi", 
            "text": "There is a plugin for Gephi that we will use to transform our network. Open Gephi. Across the top of Gephi in the menu ribbon you\u2019ll see  File Workspace View Tools Window Plugins Help .   To get and install the plugin, select  Tools   Plugins  (The top level menu item 'Plugins' is empty and not used - a useful reminder that Gephi is still in  beta ).  In the popup, under \u2018available plugins\u2019 look for \u2018MultimodeNetworksTransformation\u2019. Tick this box, then click on Install. Follow the instructions, ignore any warnings, click on \u2018finish\u2019. You may or may not need to restart Gephi to get the plugin running. If you suddenly see on the far right of ht Gephi window a new tab besid \u2018statistics\u2019, \u2018filters\u2019, called \u2018Multimode Network\u2019, then you\u2019re ok.", 
            "title": "Configuring Gephi"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#importing-the-data", 
            "text": "Under \u2018file\u2019, select -  New project.  On the data  laboratory tab, select Import-spreadsheet, and in the pop-up, make sure to select under \u2018as table: EDGES table. Select women-orgs.csv.  Click \u2018next\u2019, click finish.   On the data table, have \u2018edges\u2019 selected. This is showing you the source and the target for each link (aka \u2018edge\u2019). This implies a directionality to the relationship that we just don\u2019t know \u2013 so down below, when we get to statistics, we will always have to make sure to tell Gephi that we want the network treated as \u2018undirected\u2019. More on that below.   Loading your csv file, step 1.   Loading your CSV file, step 2    Click on \u2018copy data to other column\u2019. Select \u2018Id\u2019. In the pop-up, select \u2018Label\u2019. You now have your edges labelled.    Just as you did above, now import NODES (Women-names.csv)   making sure you're on the Nodes page in the data laboratory, copy ID to Label to that your nodes are labelled.   (nb. You can always add more attribute data to your network this way, as long as you always use a column called Id so that Gephi knows where to slot the new information. Make sure to never tick off the box labeled \u2018force nodes to be created as new ones\u2019.)", 
            "title": "Importing the data"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#prepping-your-data", 
            "text": "We're now going to manipulate the data a bit in order to get it ready for the transformation. In this data, we have women, and we have organizations. We need to tell Gephi which rows are the organizations. In Peter's original data input phase, he decided to use a unique number for each woman so that he would minimize data entry errors (misspellings and so on), as well as control for the use of maiden and married names. Miss Eliza Smith might become, at a later date, Mrs. George Doe. In Peter's scheme, this is the same person (remember in module 3 in the TEI exercise how we dealt with such issues?).   On your NODES page in the data laboratory, add new column, make it boolean. Call it \u2018organization\u2019     In the Filter box, type [a-z], and select Id \u2013 this filters out all the women. (What would you have to do to filter out the organizations? Remember, this is a regex search!)  Tick off the check boxes in the \u2018organization\u2019 columns.    (I note a typo in the image above, 'a-b'. that should be, 'a-z').  Save this as  women-organizations-2-mode.gephi .  Pro tip : always export your data from gephi (file   export) in .net or .graphml or .gefx format as the .gephi format (which is your only option under file   save as) is unstable. That is, sometimes gephi won't read .gephi files! I did say this was  beta  software).", 
            "title": "Prepping your data"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#transforming-the-network", 
            "text": "At this point, you have a two mode network in gephi. You could click on the 'overview' panel and play with some of the layouts and begin to form impressions about the nature of your data. Remember though any metrics calculated at this point would be largely spurious. Let's transform this two-mode network so that we can explore how women are connected to other women via shared membership.   On the multimode networks projection tab,\n+ click load attributes.\n+ in \u2018attribute type\u2019, select organization\n+ in left matrix, select \u2018false \u2013 true\u2019 (or \u2018null \u2013 true\u2019)\n+ in right matrix, select \u2018true \u2013 false\u2019. (or \u2018true \u2013 null\u2019) (do you see why this is the case? what would selecting the inverse accomplish?)\n+ select \u2018remove edges\u2019 and \u2018remove nodes\u2019.\n+ Once you hit \u2018run\u2019, organizations will be removed from your bipartite network, leaving you with a single-mode network. hit \u2018run\u2019.\n+ save as  women-to-women-network.gephi   and  export as  women-to-women.net  NB  if your nodes data table is blank, your filter might still be active. make sure the filter box is clear. You should be left with a list of women (ie, a list of nodes where the identiers are numbers, per Peter's schema).  At this point, you could re-start Gephi and reload your \u2018women-organizations-2-mode.gephi\u2019 file and re-run the multimode networks projection so that you are left with an organization to organization network. Do this, and save and export with appropriate file names.", 
            "title": "Transforming the network"
        }, 
        {
            "location": "/supporting materials/multimode-networks.txt/#exploring-this-network", 
            "text": "Peter's data has a number of  attributes  describing it, including the membership year. So let's see what this network of women looks like in 1902.   Under the filters tab at the right hand side of the Gephi interface, select \u2018attributes \u2013 equal\u2019 and then drag \u20181902\u2019 to the queries box.  In \u2018pattern\u2019 enter [0-9] and tick the \u2018use regex\u2019 box.  click ok, click \u2018filter\u2019.   You should now have a network with 188 nodes and 8728 edges, showing the women who were active in 1902.  Let\u2019s learn something about this network. Under the statistics tab at the right hand side of the Gephi interface,    Run \u2018avg. path length\u2019 by clicking on \u2018run\u2019    In the pop up that opens, select \u2018undirected\u2019 (as we know nothing about directionality in this network; we simply know that two women were members of the same organization at the same time. Note also that if the same pair were members of the more than one organziation, the weight of their connection will be corresponding stronger).    click ok.    run \u2018modularity\u2019 to look for subgroups. make sure \u2018randomize\u2019 and \u2018use weights\u2019 are selected. Leave \u2018resolution\u2019 at 1.0    We selected 'average path length' because one of the byproducts of this routine is 'betweeness centrality'. We're making an assumption here that a woman who has a high betweeness centrality score was in a position to affect information flow in 1902 society. Modularity looks at similar patterns of connections to cluster women who have more-or-less similar connections into groups.  Let\u2019s visualize what we\u2019ve just learned.   On the \u2018partition\u2019 tab, over on the left hand side of the \u2018overview\u2019 screen, click on nodes, then click the green arrows beside \u2018choose a partition parameter\u2019.  Click on \u2018choose a partition parameter\u2019. Scroll down to modularity class. The different groups will be listed, with their colours and their % composition of the network.  Hit \u2018apply\u2019 to recolour your network graph.   Let\u2019s resize the nodes to show off betweeness-centrality (to figure out which woman was in the greatest position to influence flows of information in this network.)    Click \u2018ranking\u2019. (It's on the left hand side of the interface, beside 'partition' and just below 'overview'.  Click \u2018nodes\u2019.  Click the down arrow on \u2018choose a rank parameter\u2019. Select \u2018betweeness centrality\u2019.  Click the red diamond. This will resize the nodes according to their \u2018betweeness centrality\u2019.  Click \u2018apply\u2019.   Now, down at the bottom of the middle panel, you can click the large black \u2018T\u2019 to display labels. Do so. Click the black letter \u2018A\u2019 and select \u2018node size\u2019.  Mrs. Mary Elliot-Murray-Kynynmound and Mrs. John Henry Wilson should now dominate your network (if you go back to the original data zip, you'll be able to find Peter's key to figure out who's who). Who were these ladies? What organizations were they members of? Who were they connected to? To the archives!  Congratulations! You\u2019ve imported historical network data into Gephi, transformed it, manipulated it, and run some analyses. Play with the settings on \u2018preview\u2019 in order to share your visualization as svg, pdf, or png.  Now go back to your original gephi file, and recast it as organizations to organizations via shared members, to figure out which organizations were key in early 20th century Ontario\u2026 make appropriate notes in your open notebook.", 
            "title": "Exploring this network"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/", 
            "text": "Making a map website with Leaflet.js\n\n\nThe \nleaflet.js\n library allows you to create quite nice interactive maps in a web-browser, that are also mobile friendly. Here, we'll build a map that uses our georectified map as the base layer (rather than as an image overlay). I won't go into the details, but rather will provide you enough guidance to get going. The documentation for Leaflet is quite extensive, and many other tutorials abound.\n\n\nSetup\n\n\n\n\ncreate a new github repository for this exercise. Create a new branch called \ngh-pages\n. We will be putting our html on the gh-pages branch, so that \nyour username\n/github.io/\nrepo\nmap.html\n can serve us up the webpage when we're done.\n\n\nLeaflet comes with a number of \nexcellent tutorials\n. We're going to look at the \nfirst one\n. Go to the \nleaflet quick-start tutorial\n and read through it carefully. In essence, you create a webpage that draws its instructions on how to handle geographic data and how to style that data from the leaflet.js source. That way, the browser knows how to render all the geographic information you're about to give it. Do you see where leaflet is calling on geographic information? This bit: \n\n\n\n\nL.tileLayer('http://{s}.tiles.mapbox.com/v3/MapID/{z}/{x}/{y}.png', {\n    attribution: 'Map data \ncopy; \na href=\nhttp://openstreetmap.org\nOpenStreetMap\n/a\n contributors, \na href=\nhttp://creativecommons.org/licenses/by-sa/2.0/\nCC-BY-SA\n/a\n, Imagery \u00a9 \na href=\nhttp://mapbox.com\nMapbox\n/a\n',\n    maxZoom: 18\n}).addTo(map); \n\n\n\n\nis calling on a background layer from the mapbox service. Instead of using mapbox, We can slot the url to the georectified map we made in module 4 in there! That is, swap out the url from the line beginning \nL.tileLayer\n. You'd change the copyright notice, etc, too, obviously. \n\n\nThe other bits of code that create callouts, polygons, and so on, are all using decimal degrees to locate the drawn elements on top of that map. \n\n\nThis bit:\n\n\nL.marker([51.5, -0.09]).addTo(map)\n            .bindPopup(\nb\nHello world!\n/b\nbr /\nI am a popup.\n).openPopup();\n\n\n\n\ncan be copied and repeated in the document, with new coordinates in decimal degrees for each new point. In the second line, between the quotation marks, you can use regular html to style your text, include pictures, and so on.\n\n\nIn a new browser window, open the \nexample map for the quickstart guide\n, and then right-click \n view source. \n\n\nSo let's get started.\n\n\n\n\nCreate a new html document in your gh-pages branch of your repo. \n\n\nCopy the html from the quickstart map (right-click and select 'view source' on this page: \nleafletjs.com/examples/quick-start-example.html\n \n\n\nPaste the code in your new html document in your gh-pages branch of your repo. Call it 'map.html' and commit your changes.\n\n\nChange the source map to point to a georectified map you made in module 4. Using the Ottawa Fire Insurance map I used as an example in module 4, I created \nthis map\n. Right click and view my page source to see what I changed up.  \nNB\n You could keep the basic mapbox service base map, and render the Ottawa Fire Insurance map as an overlay \nreference documentation\n. Or you could do a series of overlays, showing the change in the city over time. (My favourite example of a leaflet-powered historical map visualization is the \nSlave Revolt in Jamaica project\n by Vincent Brown). But you don't necessarily have to do this.\n\n\nAdd a series of markers with historical information by duplicating and then changing up the \nL.marker\n settings to your own data. Commit your changes!\n\n\n\n\nThis all just makes the map. The rest of the webpage would have to be styled as you would normally for a webpage. That is, you'd probably want to add an explanation about what the map shows, how it was created, and how the user ought to interact with it, links to your source data, and so on. The easiest place to add all that kind of information would be between these two tags in the page source:\n\n\n/script\n\n\n/body\n\n\n\n\n\nGoing further\n\n\nLet's say you have a whole bunch of information that you want to represent on the map. Perhaps it's in a well organized csv file, with a latitude and a longitude column in decimal degrees. Adding points one at a time to the map as described above would take ages. Instead, let's convert that csv to geojson, and then use \nbootleaf\n to make a map.\n\n\nBootleaf is a template that uses a common html template package, '\nBootstrap\n' as a wrapper for a leaflet powered map that draws its points of interest from a geojson file. Here's how you'd get this up and running.\n\n\n\n\nGo to the \ngithub repo for bootleaf\n.\n\n\nFork a copy to a new repo (you have to be logged into github.com) by hitting the 'fork' button. \n\n\nIn your copy of bootleaf, you now have a gh-pages version of the site. If you go to \nyourusername\n.github.io/bootleaf\n you should see an active version of the map.\n\n\nNow, the map is grabbing its data from a series of geojson files. You can use \nthis service\n to convert your csv to geojson. There are other services.\n\n\nClone your repository in your desktop (by pressing the clone your repo in desktop).\n\n\nOpen your desktop client, and make sure you're in the gh-pages branch\n\n\nUsing your windows explorer or mac finder, put your newly created geojson file in the data folder.\n\n\ncommit and sync your changes.\n\n\n\n\nTo add your data to the dropdown menu, you need to change the code in the index.html file:\n\n\n  \nli class=\ndropdown\n\n                \na class=\ndropdown-toggle\n id=\ndownloadDrop\n href=\n#\n role=\nbutton\n data-toggle=\ndropdown\ni class=\nfa fa-cloud-download white\n/i\nnbsp;\nnbsp;Download \nb class=\ncaret\n/b\n/a\n\n                \nul class=\ndropdown-menu\n\n                  \nli\na href=\ndata/boroughs.geojson\n download=\nboroughs.geojson\n target=\n_blank\n data-toggle=\ncollapse\n data-target=\n.navbar-collapse.in\ni class=\nfa fa-download\n/i\nnbsp;\nnbsp;Boroughs\n/a\n/li\n\n                  \nli\na href=\ndata/subways.geojson\n download=\nsubways.geojson\n target=\n_blank\n data-toggle=\ncollapse\n data-target=\n.navbar-collapse.in\ni class=\nfa fa-download\n/i\nnbsp;\nnbsp;Subway Lines\n/a\n/li\n\n                  \nli\na href=\ndata/DOITT_THEATER_01_13SEPT2010.geojson\n download=\ntheaters.geojson\n target=\n_blank\n data-toggle=\ncollapse\n data-target=\n.navbar-collapse.in\ni class=\nfa fa-download\n/i\nnbsp;\nnbsp;Theaters\n/a\n/li\n\n                  \nli\na href=\ndata/DOITT_MUSEUM_01_13SEPT2010.geojson\n download=\nmuseums.geojson\n target=\n_blank\n data-toggle=\ncollapse\n data-target=\n.navbar-collapse.in\ni class=\nfa fa-download\n/i\nnbsp;\nnbsp;Museums\n/a\n/li\n\n                \n/ul\n\n\n\n\n\nand since you probably don't want that other stuff, you could delete it.\n\n\nYou could change the 'about' pop-up here:\n\n\ndiv class=\ntab-pane fade active in\n id=\nabout\n\n                \np\nA simple, responsive template for building web mapping applications with \na href=\nhttp://getbootstrap.com/\nBootstrap 3\n/a\n, \na href=\nhttp://leafletjs.com/\n target=\n_blank\nLeaflet\n/a\n, and \na href=\nhttp://twitter.github.io/typeahead.js/\n target=\n_blank\ntypeahead.js\n/a\n. Open source, MIT licensed, and available on \na href=\nhttps://github.com/bmcbride/bootleaf\n target=\n_blank\nGitHub\n/a\n.\n/p\n\n                \ndiv class=\npanel panel-primary\n\n                  \ndiv class=\npanel-heading\nFeatures\n/div\n\n                  \nul class=\nlist-group\n\n                    \nli class=\nlist-group-item\nFullscreen mobile-friendly map template with responsive navbar and modal placeholders\n/li\n\n                    \nli class=\nlist-group-item\njQuery loading of external GeoJSON files\n/li\n\n                    \nli class=\nlist-group-item\nLogical multiple layer marker clustering via the \na href=\nhttps://github.com/Leaflet/Leaflet.markercluster\n target=\n_blank\nleaflet marker cluster plugin\n/a\n/li\n\n                    \nli class=\nlist-group-item\nElegant client-side multi-layer feature search with autocomplete using \na href=\nhttp://twitter.github.io/typeahead.js/\n target=\n_blank\ntypeahead.js\n/a\n/li\n\n                    \nli class=\nlist-group-item\nResponsive sidebar feature list synced with map bounds, which includes sorting and filtering via \na href=\nhttp://listjs.com/\n target=\n_blank\nlist.js\n/a\n/li\n\n                    \nli class=\nlist-group-item\nMarker icons included in grouped layer control via the \na href=\nhttps://github.com/ismyrnow/Leaflet.groupedlayercontrol\n target=\n_blank\ngrouped layer control plugin\n/a\n/li\n\n                  \n/ul\n\n                \n/div\n\n\n\n\n\nAnd you have to remove this:\n\n\n    \n!-- Remove this maptiks analytics code from your BootLeaf implementation --\n\n    \nscript src=\n//cdn.maptiks.com/maptiks-leaflet.min.js\n/script\n\n    \nscript\nmaptiks.trackcode='c7ca251e-9c17-47ef-ac33-e0fb24e05976';\n/script\n\n    \n!-- End maptiks analytics code --\n\n\n\n\n\nNow the really hard part: putting your own base maps in. To do this, you have to find, and modify, a file called app.js. You should be able to find it by following this path: bootleaf/assets/js/app.js\n\n\nYou need to change these lines to point to your maps\n\n\n/* Basemap Layers */\nvar mapquestOSM = L.tileLayer(\nhttp://{s}.mqcdn.com/tiles/1.0.0/osm/{z}/{x}/{y}.png\n, {\n  maxZoom: 19,\n  subdomains: [\notile1\n, \notile2\n, \notile3\n, \notile4\n],\n  attribution: 'Tiles courtesy of \na href=\nhttp://www.mapquest.com/\n target=\n_blank\nMapQuest\n/a\n \nimg src=\nhttp://developer.mapquest.com/content/osm/mq_logo.png\n. Map data (c) \na href=\nhttp://www.openstreetmap.org/\n target=\n_blank\nOpenStreetMap\n/a\n contributors, CC-BY-SA.'\n});\nvar mapquestOAM = L.tileLayer(\nhttp://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\n, {\n  maxZoom: 18,\n  subdomains: [\noatile1\n, \noatile2\n, \noatile3\n, \noatile4\n],\n  attribution: 'Tiles courtesy of \na href=\nhttp://www.mapquest.com/\n target=\n_blank\nMapQuest\n/a\n. Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n});\nvar mapquestHYB = L.layerGroup([L.tileLayer(\nhttp://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg\n, {\n  maxZoom: 18,\n  subdomains: [\noatile1\n, \noatile2\n, \noatile3\n, \noatile4\n]\n}), L.tileLayer(\nhttp://{s}.mqcdn.com/tiles/1.0.0/hyb/{z}/{x}/{y}.png\n, {\n  maxZoom: 19,\n  subdomains: [\noatile1\n, \noatile2\n, \noatile3\n, \noatile4\n],\n  attribution: 'Labels courtesy of \na href=\nhttp://www.mapquest.com/\n target=\n_blank\nMapQuest\n/a\n \nimg src=\nhttp://developer.mapquest.com/content/osm/mq_logo.png\n. Map data (c) \na href=\nhttp://www.openstreetmap.org/\n target=\n_blank\nOpenStreetMap\n/a\n contributors, CC-BY-SA. Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n})]);\n\n\n\n\n...and then you'd have to go through the rest of that file and change up the .geojson pointers to point to your own data.\n\n\nGood luck!\n\n\n(\nfurther reading\n Here's a nice piece on using \nTilemill\n by Arian Katsimbras to make beautiful maps.)", 
            "title": "Leaflet"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/#making-a-map-website-with-leafletjs", 
            "text": "The  leaflet.js  library allows you to create quite nice interactive maps in a web-browser, that are also mobile friendly. Here, we'll build a map that uses our georectified map as the base layer (rather than as an image overlay). I won't go into the details, but rather will provide you enough guidance to get going. The documentation for Leaflet is quite extensive, and many other tutorials abound.", 
            "title": "Making a map website with Leaflet.js"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/#setup", 
            "text": "create a new github repository for this exercise. Create a new branch called  gh-pages . We will be putting our html on the gh-pages branch, so that  your username /github.io/ repo map.html  can serve us up the webpage when we're done.  Leaflet comes with a number of  excellent tutorials . We're going to look at the  first one . Go to the  leaflet quick-start tutorial  and read through it carefully. In essence, you create a webpage that draws its instructions on how to handle geographic data and how to style that data from the leaflet.js source. That way, the browser knows how to render all the geographic information you're about to give it. Do you see where leaflet is calling on geographic information? This bit:    L.tileLayer('http://{s}.tiles.mapbox.com/v3/MapID/{z}/{x}/{y}.png', {\n    attribution: 'Map data  copy;  a href= http://openstreetmap.org OpenStreetMap /a  contributors,  a href= http://creativecommons.org/licenses/by-sa/2.0/ CC-BY-SA /a , Imagery \u00a9  a href= http://mapbox.com Mapbox /a ',\n    maxZoom: 18\n}).addTo(map);   is calling on a background layer from the mapbox service. Instead of using mapbox, We can slot the url to the georectified map we made in module 4 in there! That is, swap out the url from the line beginning  L.tileLayer . You'd change the copyright notice, etc, too, obviously.   The other bits of code that create callouts, polygons, and so on, are all using decimal degrees to locate the drawn elements on top of that map.   This bit:  L.marker([51.5, -0.09]).addTo(map)\n            .bindPopup( b Hello world! /b br / I am a popup. ).openPopup();  can be copied and repeated in the document, with new coordinates in decimal degrees for each new point. In the second line, between the quotation marks, you can use regular html to style your text, include pictures, and so on.  In a new browser window, open the  example map for the quickstart guide , and then right-click   view source.", 
            "title": "Setup"
        }, 
        {
            "location": "/supporting materials/leaflet.txt/#so-lets-get-started", 
            "text": "Create a new html document in your gh-pages branch of your repo.   Copy the html from the quickstart map (right-click and select 'view source' on this page:  leafletjs.com/examples/quick-start-example.html    Paste the code in your new html document in your gh-pages branch of your repo. Call it 'map.html' and commit your changes.  Change the source map to point to a georectified map you made in module 4. Using the Ottawa Fire Insurance map I used as an example in module 4, I created  this map . Right click and view my page source to see what I changed up.   NB  You could keep the basic mapbox service base map, and render the Ottawa Fire Insurance map as an overlay  reference documentation . Or you could do a series of overlays, showing the change in the city over time. (My favourite example of a leaflet-powered historical map visualization is the  Slave Revolt in Jamaica project  by Vincent Brown). But you don't necessarily have to do this.  Add a series of markers with historical information by duplicating and then changing up the  L.marker  settings to your own data. Commit your changes!   This all just makes the map. The rest of the webpage would have to be styled as you would normally for a webpage. That is, you'd probably want to add an explanation about what the map shows, how it was created, and how the user ought to interact with it, links to your source data, and so on. The easiest place to add all that kind of information would be between these two tags in the page source:  /script  /body   Going further  Let's say you have a whole bunch of information that you want to represent on the map. Perhaps it's in a well organized csv file, with a latitude and a longitude column in decimal degrees. Adding points one at a time to the map as described above would take ages. Instead, let's convert that csv to geojson, and then use  bootleaf  to make a map.  Bootleaf is a template that uses a common html template package, ' Bootstrap ' as a wrapper for a leaflet powered map that draws its points of interest from a geojson file. Here's how you'd get this up and running.   Go to the  github repo for bootleaf .  Fork a copy to a new repo (you have to be logged into github.com) by hitting the 'fork' button.   In your copy of bootleaf, you now have a gh-pages version of the site. If you go to  yourusername .github.io/bootleaf  you should see an active version of the map.  Now, the map is grabbing its data from a series of geojson files. You can use  this service  to convert your csv to geojson. There are other services.  Clone your repository in your desktop (by pressing the clone your repo in desktop).  Open your desktop client, and make sure you're in the gh-pages branch  Using your windows explorer or mac finder, put your newly created geojson file in the data folder.  commit and sync your changes.   To add your data to the dropdown menu, you need to change the code in the index.html file:     li class= dropdown \n                 a class= dropdown-toggle  id= downloadDrop  href= #  role= button  data-toggle= dropdown i class= fa fa-cloud-download white /i nbsp; nbsp;Download  b class= caret /b /a \n                 ul class= dropdown-menu \n                   li a href= data/boroughs.geojson  download= boroughs.geojson  target= _blank  data-toggle= collapse  data-target= .navbar-collapse.in i class= fa fa-download /i nbsp; nbsp;Boroughs /a /li \n                   li a href= data/subways.geojson  download= subways.geojson  target= _blank  data-toggle= collapse  data-target= .navbar-collapse.in i class= fa fa-download /i nbsp; nbsp;Subway Lines /a /li \n                   li a href= data/DOITT_THEATER_01_13SEPT2010.geojson  download= theaters.geojson  target= _blank  data-toggle= collapse  data-target= .navbar-collapse.in i class= fa fa-download /i nbsp; nbsp;Theaters /a /li \n                   li a href= data/DOITT_MUSEUM_01_13SEPT2010.geojson  download= museums.geojson  target= _blank  data-toggle= collapse  data-target= .navbar-collapse.in i class= fa fa-download /i nbsp; nbsp;Museums /a /li \n                 /ul   and since you probably don't want that other stuff, you could delete it.  You could change the 'about' pop-up here:  div class= tab-pane fade active in  id= about \n                 p A simple, responsive template for building web mapping applications with  a href= http://getbootstrap.com/ Bootstrap 3 /a ,  a href= http://leafletjs.com/  target= _blank Leaflet /a , and  a href= http://twitter.github.io/typeahead.js/  target= _blank typeahead.js /a . Open source, MIT licensed, and available on  a href= https://github.com/bmcbride/bootleaf  target= _blank GitHub /a . /p \n                 div class= panel panel-primary \n                   div class= panel-heading Features /div \n                   ul class= list-group \n                     li class= list-group-item Fullscreen mobile-friendly map template with responsive navbar and modal placeholders /li \n                     li class= list-group-item jQuery loading of external GeoJSON files /li \n                     li class= list-group-item Logical multiple layer marker clustering via the  a href= https://github.com/Leaflet/Leaflet.markercluster  target= _blank leaflet marker cluster plugin /a /li \n                     li class= list-group-item Elegant client-side multi-layer feature search with autocomplete using  a href= http://twitter.github.io/typeahead.js/  target= _blank typeahead.js /a /li \n                     li class= list-group-item Responsive sidebar feature list synced with map bounds, which includes sorting and filtering via  a href= http://listjs.com/  target= _blank list.js /a /li \n                     li class= list-group-item Marker icons included in grouped layer control via the  a href= https://github.com/ismyrnow/Leaflet.groupedlayercontrol  target= _blank grouped layer control plugin /a /li \n                   /ul \n                 /div   And you have to remove this:       !-- Remove this maptiks analytics code from your BootLeaf implementation -- \n     script src= //cdn.maptiks.com/maptiks-leaflet.min.js /script \n     script maptiks.trackcode='c7ca251e-9c17-47ef-ac33-e0fb24e05976'; /script \n     !-- End maptiks analytics code --   Now the really hard part: putting your own base maps in. To do this, you have to find, and modify, a file called app.js. You should be able to find it by following this path: bootleaf/assets/js/app.js  You need to change these lines to point to your maps  /* Basemap Layers */\nvar mapquestOSM = L.tileLayer( http://{s}.mqcdn.com/tiles/1.0.0/osm/{z}/{x}/{y}.png , {\n  maxZoom: 19,\n  subdomains: [ otile1 ,  otile2 ,  otile3 ,  otile4 ],\n  attribution: 'Tiles courtesy of  a href= http://www.mapquest.com/  target= _blank MapQuest /a   img src= http://developer.mapquest.com/content/osm/mq_logo.png . Map data (c)  a href= http://www.openstreetmap.org/  target= _blank OpenStreetMap /a  contributors, CC-BY-SA.'\n});\nvar mapquestOAM = L.tileLayer( http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg , {\n  maxZoom: 18,\n  subdomains: [ oatile1 ,  oatile2 ,  oatile3 ,  oatile4 ],\n  attribution: 'Tiles courtesy of  a href= http://www.mapquest.com/  target= _blank MapQuest /a . Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n});\nvar mapquestHYB = L.layerGroup([L.tileLayer( http://{s}.mqcdn.com/tiles/1.0.0/sat/{z}/{x}/{y}.jpg , {\n  maxZoom: 18,\n  subdomains: [ oatile1 ,  oatile2 ,  oatile3 ,  oatile4 ]\n}), L.tileLayer( http://{s}.mqcdn.com/tiles/1.0.0/hyb/{z}/{x}/{y}.png , {\n  maxZoom: 19,\n  subdomains: [ oatile1 ,  oatile2 ,  oatile3 ,  oatile4 ],\n  attribution: 'Labels courtesy of  a href= http://www.mapquest.com/  target= _blank MapQuest /a   img src= http://developer.mapquest.com/content/osm/mq_logo.png . Map data (c)  a href= http://www.openstreetmap.org/  target= _blank OpenStreetMap /a  contributors, CC-BY-SA. Portions Courtesy NASA/JPL-Caltech and U.S. Depart. of Agriculture, Farm Service Agency'\n})]);  ...and then you'd have to go through the rest of that file and change up the .geojson pointers to point to your own data.  Good luck!  ( further reading  Here's a nice piece on using  Tilemill  by Arian Katsimbras to make beautiful maps.)", 
            "title": "So let's get started."
        }
    ]
}